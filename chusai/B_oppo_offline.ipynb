{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import gc\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import chi2, SelectPercentile\n",
    "import math\n",
    "from sklearn.metrics import f1_score\n",
    "import jieba\n",
    "import jieba.posseg as psg\n",
    "from collections import Counter\n",
    "import functools\n",
    "from gensim.models import word2vec\n",
    "import Levenshtein\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_table('../data/oppo_round1_train_20180929.txt', names=['prefix', 'query_prediction', 'title', 'tag', 'label'], header=None, quoting=3)\n",
    "train_df['index'] = train_df.index\n",
    "valid_df = pd.read_table('../data/oppo_round1_vali_20180929.txt', names=['prefix', 'query_prediction', 'title', 'tag', 'label'], header=None, quoting=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  prefix                                   query_prediction            title  \\\n",
      "0     小品  {\"小品大全宋小宝\": \"0.009\", \"小品相亲\": \"0.012\", \"小品剧本\": ...               小品   \n",
      "1   1368  {\"13688cc赛马会\": \"0.059\", \"13685367892\": \"0.124\"...  HCG大于1368%2C正常吗   \n",
      "2   1368  {\"13688cc赛马会\": \"0.059\", \"13685367892\": \"0.124\"...            1368年   \n",
      "3     银耳  {\"银耳汤的功效\": \"0.012\", \"银耳为什么不能天天吃\": \"0.009\", \"银耳...         银耳红枣汤的做法   \n",
      "4   月经量少  {\"月经量少喝红糖水好吗\": \"0.010\", \"月经量少该怎么调理\": \"0.016\", ...         月经量少怎么调理   \n",
      "\n",
      "  tag  label  index                              query_prediction_dict  \\\n",
      "0  阅读      0      0  {'小品大全宋小宝': '0.009', '小品相亲': '0.012', '小品剧本': ...   \n",
      "1  健康      0      1  {'13688cc赛马会': '0.059', '13685367892': '0.124'...   \n",
      "2  百科      1      2  {'13688cc赛马会': '0.059', '13685367892': '0.124'...   \n",
      "3  菜谱      1      3  {'银耳汤的功效': '0.012', '银耳为什么不能天天吃': '0.009', '银耳...   \n",
      "4  百科      0      4  {'月经量少喝红糖水好吗': '0.010', '月经量少该怎么调理': '0.016', ...   \n",
      "\n",
      "                               query_prediction_keys  \\\n",
      "0  [小品大全宋小宝, 小品相亲, 小品剧本, 小品搞笑大全, 小品不差钱, 小品搞笑大全剧本,...   \n",
      "1  [13688cc赛马会, 13685367892, 13688cc, 1368个单词就够了,...   \n",
      "2  [13688cc赛马会, 13685367892, 13688cc, 1368个单词就够了,...   \n",
      "3  [银耳汤的功效, 银耳为什么不能天天吃, 银耳莲子羹, 银耳的做法, 银耳的功效, 银耳莲子...   \n",
      "4  [月经量少喝红糖水好吗, 月经量少该怎么调理, 月经量少怎么, 月经量少发黑, 月经量少是什...   \n",
      "\n",
      "                             query_prediction_values  query_prediction_number  \\\n",
      "0  [0.009, 0.012, 0.02, 0.066, 0.007, 0.01, 0.198...                       10   \n",
      "1  [0.059, 0.124, 0.029, 0.07, 0.022, 0.042, 0.08...                        9   \n",
      "2  [0.059, 0.124, 0.029, 0.07, 0.022, 0.042, 0.08...                        9   \n",
      "3  [0.012, 0.009, 0.05, 0.045, 0.053, 0.014, 0.05...                       10   \n",
      "4  [0.01, 0.016, 0.009, 0.009, 0.569, 0.016, 0.02...                       10   \n",
      "\n",
      "   query_prediction_max  query_prediction_min  query_prediction_mean  \\\n",
      "0                 0.198                 0.007               0.037300   \n",
      "1                 0.124                 0.022               0.057778   \n",
      "2                 0.124                 0.022               0.057778   \n",
      "3                 0.114                 0.009               0.040400   \n",
      "4                 0.569                 0.009               0.074700   \n",
      "\n",
      "   query_prediction_std  \n",
      "0              0.056023  \n",
      "1              0.031538  \n",
      "2              0.031538  \n",
      "3              0.030660  \n",
      "4              0.165089  \n"
     ]
    }
   ],
   "source": [
    "def get_float_list(x):\n",
    "    return_list = []\n",
    "    for temp in x:\n",
    "        return_list.append(float(temp))\n",
    "    return return_list\n",
    "\n",
    "# 处理跟query_prediction相关的统计特征\n",
    "def get_query_prediction_feature(df):\n",
    "    df['query_prediction_dict'] = df['query_prediction'].map(lambda x : eval(x))\n",
    "    df['query_prediction_keys'] = df['query_prediction_dict'].map(lambda x : list(x.keys()))\n",
    "    df['query_prediction_values'] = df['query_prediction_dict'].map(lambda x : get_float_list(list(x.values())))\n",
    "    df['query_prediction_number'] = df['query_prediction_keys'].map(lambda x : len(x))\n",
    "    df['query_prediction_max'] = df['query_prediction_values'].map(lambda x : np.nan if len(x) == 0 else np.max(x))\n",
    "    df['query_prediction_min'] = df['query_prediction_values'].map(lambda x : np.nan if len(x) == 0 else np.min(x))\n",
    "    df['query_prediction_mean'] = df['query_prediction_values'].map(lambda x : np.nan if len(x) == 0 else np.mean(x))\n",
    "    df['query_prediction_std'] = df['query_prediction_values'].map(lambda x : np.nan if len(x) == 0 else np.std(x))\n",
    "    return df\n",
    "\n",
    "train_df = get_query_prediction_feature(train_df)\n",
    "valid_df = get_query_prediction_feature(valid_df)\n",
    "print(train_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefix : finish!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:40: FutureWarning: by argument to sort_index is deprecated, pls use .sort_values(by=...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title : finish!!!\n",
      "tag : finish!!!\n",
      "query_prediction : finish!!!\n",
      "  prefix                                   query_prediction            title  \\\n",
      "0     小品  {\"小品大全宋小宝\": \"0.009\", \"小品相亲\": \"0.012\", \"小品剧本\": ...               小品   \n",
      "0   1368  {\"13688cc赛马会\": \"0.059\", \"13685367892\": \"0.124\"...  HCG大于1368%2C正常吗   \n",
      "1   1368  {\"13688cc赛马会\": \"0.059\", \"13685367892\": \"0.124\"...            1368年   \n",
      "0     银耳  {\"银耳汤的功效\": \"0.012\", \"银耳为什么不能天天吃\": \"0.009\", \"银耳...         银耳红枣汤的做法   \n",
      "2   月经量少  {\"月经量少喝红糖水好吗\": \"0.010\", \"月经量少该怎么调理\": \"0.016\", ...         月经量少怎么调理   \n",
      "\n",
      "  tag  label  index                              query_prediction_dict  \\\n",
      "0  阅读      0      0  {'小品大全宋小宝': '0.009', '小品相亲': '0.012', '小品剧本': ...   \n",
      "0  健康      0      1  {'13688cc赛马会': '0.059', '13685367892': '0.124'...   \n",
      "1  百科      1      2  {'13688cc赛马会': '0.059', '13685367892': '0.124'...   \n",
      "0  菜谱      1      3  {'银耳汤的功效': '0.012', '银耳为什么不能天天吃': '0.009', '银耳...   \n",
      "2  百科      0      4  {'月经量少喝红糖水好吗': '0.010', '月经量少该怎么调理': '0.016', ...   \n",
      "\n",
      "                               query_prediction_keys  \\\n",
      "0  [小品大全宋小宝, 小品相亲, 小品剧本, 小品搞笑大全, 小品不差钱, 小品搞笑大全剧本,...   \n",
      "0  [13688cc赛马会, 13685367892, 13688cc, 1368个单词就够了,...   \n",
      "1  [13688cc赛马会, 13685367892, 13688cc, 1368个单词就够了,...   \n",
      "0  [银耳汤的功效, 银耳为什么不能天天吃, 银耳莲子羹, 银耳的做法, 银耳的功效, 银耳莲子...   \n",
      "2  [月经量少喝红糖水好吗, 月经量少该怎么调理, 月经量少怎么, 月经量少发黑, 月经量少是什...   \n",
      "\n",
      "                             query_prediction_values  query_prediction_number  \\\n",
      "0  [0.009, 0.012, 0.02, 0.066, 0.007, 0.01, 0.198...                       10   \n",
      "0  [0.059, 0.124, 0.029, 0.07, 0.022, 0.042, 0.08...                        9   \n",
      "1  [0.059, 0.124, 0.029, 0.07, 0.022, 0.042, 0.08...                        9   \n",
      "0  [0.012, 0.009, 0.05, 0.045, 0.053, 0.014, 0.05...                       10   \n",
      "2  [0.01, 0.016, 0.009, 0.009, 0.569, 0.016, 0.02...                       10   \n",
      "\n",
      "               ...                prefix_click_number  title_count  \\\n",
      "0              ...                               47.0         78.0   \n",
      "0              ...                                1.0          NaN   \n",
      "1              ...                                0.0          NaN   \n",
      "0              ...                              167.0        156.0   \n",
      "2              ...                               56.0         61.0   \n",
      "\n",
      "   title_rate  title_click_number  tag_count  tag_rate  tag_click_number  \\\n",
      "0    0.397363                31.0      62649  0.263601             16514   \n",
      "0         NaN                 NaN     111875  0.296672             33190   \n",
      "1         NaN                 NaN     577482  0.396063            228720   \n",
      "0    0.640572               100.0      76314  0.416087             31754   \n",
      "2    0.507625                31.0     577482  0.396063            228720   \n",
      "\n",
      "   query_prediction_count  query_prediction_rate  \\\n",
      "0                   118.0               0.398307   \n",
      "0                     1.0               0.673008   \n",
      "1                     1.0               0.216097   \n",
      "0                   442.0               0.377882   \n",
      "2                   148.0               0.378538   \n",
      "\n",
      "   query_prediction_click_number  \n",
      "0                           47.0  \n",
      "0                            1.0  \n",
      "1                            0.0  \n",
      "0                          167.0  \n",
      "2                           56.0  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "def getBayesSmoothParam(origion_rate):\n",
    "    origion_rate_mean = origion_rate.mean()\n",
    "    origion_rate_var = origion_rate.var()\n",
    "    alpha = origion_rate_mean / origion_rate_var * (origion_rate_mean * (1 - origion_rate_mean) - origion_rate_var)\n",
    "    beta = (1 - origion_rate_mean) / origion_rate_var * (origion_rate_mean * (1 - origion_rate_mean) - origion_rate_var)\n",
    "#     print('origion_rate_mean : ', origion_rate_mean)\n",
    "#     print('origion_rate_var : ', origion_rate_var)\n",
    "#     print('alpha : ', alpha)\n",
    "#     print('beta : ', beta)\n",
    "    return alpha, beta\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, random_state=2018, shuffle=True)\n",
    "\n",
    "# 统计单维度的转化率特征\n",
    "def get_single_dimension_rate_feature(train_df, valid_df, fea_set):\n",
    "    for fea in fea_set:\n",
    "        train_temp_df = pd.DataFrame()\n",
    "        for index, (train_index, test_index) in enumerate(skf.split(train_df, train_df['label'])):\n",
    "            temp_df = train_df[[fea, 'label']].iloc[train_index].copy()\n",
    "            temp_pivot_table = pd.pivot_table(temp_df, index=fea, values='label', aggfunc={len, np.mean, np.sum})\n",
    "            temp_pivot_table.reset_index(inplace=True)\n",
    "            temp_pivot_table.rename(columns={'len':fea + '_count', 'mean':fea + '_rate', 'sum':fea + '_click_number'}, inplace=True)\n",
    "            alpha, beta = getBayesSmoothParam(temp_pivot_table[fea + '_rate'])\n",
    "            temp_pivot_table[fea + '_rate'] = (temp_pivot_table[fea + '_click_number'] + alpha) / (temp_pivot_table[fea + '_count'] + alpha + beta)\n",
    "#             del temp_pivot_table[fea + '_click_number']\n",
    "            fea_df = train_df.iloc[test_index].copy()\n",
    "            fea_df = pd.merge(fea_df, temp_pivot_table, on=fea, how='left')\n",
    "#             print(fea_df.head())\n",
    "            train_temp_df = pd.concat([train_temp_df, fea_df])\n",
    "        temp_df = train_df[[fea, 'label']].copy()\n",
    "        temp_pivot_table = pd.pivot_table(temp_df, index=fea, values='label', aggfunc={len, np.mean, np.sum})\n",
    "        temp_pivot_table.reset_index(inplace=True)\n",
    "        temp_pivot_table.rename(columns={'len':fea + '_count', 'mean':fea + '_rate', 'sum':fea + '_click_number'}, inplace=True)\n",
    "        alpha, beta = getBayesSmoothParam(temp_pivot_table[fea + '_rate'])\n",
    "        temp_pivot_table[fea + '_rate'] = (temp_pivot_table[fea + '_click_number'] + alpha) / (temp_pivot_table[fea + '_count'] + alpha + beta)\n",
    "#             del temp_pivot_table[fea + '_click_number']\n",
    "        valid_df = pd.merge(valid_df, temp_pivot_table, on=fea, how='left')\n",
    "        print(fea + ' : finish!!!')\n",
    "        train_df = train_temp_df\n",
    "        train_df.sort_index(by='index', ascending=True, inplace=True)\n",
    "    return train_df, valid_df\n",
    "    \n",
    "fea_set = ['prefix', 'title', 'tag', 'query_prediction']\n",
    "train_df, valid_df = get_single_dimension_rate_feature(train_df, valid_df, fea_set)\n",
    "print(train_df.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefix_title : finish!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:29: FutureWarning: by argument to sort_index is deprecated, pls use .sort_values(by=...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefix_tag : finish!!!\n",
      "title_tag : finish!!!\n",
      "  prefix                                   query_prediction            title  \\\n",
      "0     小品  {\"小品大全宋小宝\": \"0.009\", \"小品相亲\": \"0.012\", \"小品剧本\": ...               小品   \n",
      "0   1368  {\"13688cc赛马会\": \"0.059\", \"13685367892\": \"0.124\"...  HCG大于1368%2C正常吗   \n",
      "1   1368  {\"13688cc赛马会\": \"0.059\", \"13685367892\": \"0.124\"...            1368年   \n",
      "0     银耳  {\"银耳汤的功效\": \"0.012\", \"银耳为什么不能天天吃\": \"0.009\", \"银耳...         银耳红枣汤的做法   \n",
      "2   月经量少  {\"月经量少喝红糖水好吗\": \"0.010\", \"月经量少该怎么调理\": \"0.016\", ...         月经量少怎么调理   \n",
      "\n",
      "  tag  label  index                              query_prediction_dict  \\\n",
      "0  阅读      0      0  {'小品大全宋小宝': '0.009', '小品相亲': '0.012', '小品剧本': ...   \n",
      "0  健康      0      1  {'13688cc赛马会': '0.059', '13685367892': '0.124'...   \n",
      "1  百科      1      2  {'13688cc赛马会': '0.059', '13685367892': '0.124'...   \n",
      "0  菜谱      1      3  {'银耳汤的功效': '0.012', '银耳为什么不能天天吃': '0.009', '银耳...   \n",
      "2  百科      0      4  {'月经量少喝红糖水好吗': '0.010', '月经量少该怎么调理': '0.016', ...   \n",
      "\n",
      "                               query_prediction_keys  \\\n",
      "0  [小品大全宋小宝, 小品相亲, 小品剧本, 小品搞笑大全, 小品不差钱, 小品搞笑大全剧本,...   \n",
      "0  [13688cc赛马会, 13685367892, 13688cc, 1368个单词就够了,...   \n",
      "1  [13688cc赛马会, 13685367892, 13688cc, 1368个单词就够了,...   \n",
      "0  [银耳汤的功效, 银耳为什么不能天天吃, 银耳莲子羹, 银耳的做法, 银耳的功效, 银耳莲子...   \n",
      "2  [月经量少喝红糖水好吗, 月经量少该怎么调理, 月经量少怎么, 月经量少发黑, 月经量少是什...   \n",
      "\n",
      "                             query_prediction_values  query_prediction_number  \\\n",
      "0  [0.009, 0.012, 0.02, 0.066, 0.007, 0.01, 0.198...                       10   \n",
      "0  [0.059, 0.124, 0.029, 0.07, 0.022, 0.042, 0.08...                        9   \n",
      "1  [0.059, 0.124, 0.029, 0.07, 0.022, 0.042, 0.08...                        9   \n",
      "0  [0.012, 0.009, 0.05, 0.045, 0.053, 0.014, 0.05...                       10   \n",
      "2  [0.01, 0.016, 0.009, 0.009, 0.569, 0.016, 0.02...                       10   \n",
      "\n",
      "            ...            query_prediction_click_number  prefix_title_count  \\\n",
      "0           ...                                     47.0                78.0   \n",
      "0           ...                                      1.0                 NaN   \n",
      "1           ...                                      0.0                 NaN   \n",
      "0           ...                                    167.0               149.0   \n",
      "2           ...                                     56.0                54.0   \n",
      "\n",
      "   prefix_title_rate  prefix_title_click_number  prefix_tag_count  \\\n",
      "0           0.397366                       31.0              38.0   \n",
      "0                NaN                        NaN               NaN   \n",
      "1                NaN                        NaN               NaN   \n",
      "0           0.657273                       98.0             149.0   \n",
      "2           0.481020                       26.0              54.0   \n",
      "\n",
      "   prefix_tag_rate  prefix_tag_click_number  title_tag_count  title_tag_rate  \\\n",
      "0         0.030108                      1.0             38.0        0.028439   \n",
      "0              NaN                      NaN              NaN             NaN   \n",
      "1              NaN                      NaN              NaN             NaN   \n",
      "0         0.656955                     98.0            156.0        0.640630   \n",
      "2         0.480718                     26.0             59.0        0.524844   \n",
      "\n",
      "   title_tag_click_number  \n",
      "0                     1.0  \n",
      "0                     NaN  \n",
      "1                     NaN  \n",
      "0                   100.0  \n",
      "2                    31.0  \n",
      "\n",
      "[5 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "# 统计双维度交叉转化率\n",
    "def get_jiaocha_dimension_rate_feature(train_df, valid_df, fea_set):\n",
    "    for i in range(len(fea_set)):\n",
    "        for j in range((i+1), len(fea_set)):\n",
    "            fea1 = fea_set[i]\n",
    "            fea2 = fea_set[j]\n",
    "            train_temp_df = pd.DataFrame()\n",
    "            for index, (train_index, test_index) in enumerate(skf.split(train_df, train_df['label'])):\n",
    "                temp_df = train_df[[fea1, fea2, 'label']].iloc[train_index].copy()\n",
    "                temp_pivot_table = pd.pivot_table(temp_df, index=[fea1, fea2], values='label', aggfunc={len, np.mean, np.sum})\n",
    "                temp_pivot_table.reset_index(inplace=True)\n",
    "                temp_pivot_table.rename(columns={'len':fea1 + '_' + fea2 + '_count', 'mean':fea1 + '_' + fea2 + '_rate', 'sum':fea1 + '_' + fea2 + '_click_number'}, inplace=True)\n",
    "                alpha, beta = getBayesSmoothParam(temp_pivot_table[fea1 + '_' + fea2 + '_rate'])\n",
    "                temp_pivot_table[fea1 + '_' + fea2 + '_rate'] = (temp_pivot_table[fea1 + '_' + fea2 + '_click_number'] + alpha) / (temp_pivot_table[fea1 + '_' + fea2 + '_count'] + alpha + beta)\n",
    "#                 del temp_pivot_table[fea1 + '_' + fea2 + '_click_number']\n",
    "                fea_df = train_df.iloc[test_index].copy()\n",
    "                fea_df = pd.merge(fea_df, temp_pivot_table, on=[fea1, fea2], how='left')\n",
    "                train_temp_df = pd.concat([train_temp_df, fea_df])\n",
    "            temp_df = train_df[[fea1, fea2, 'label']].copy()\n",
    "            temp_pivot_table = pd.pivot_table(temp_df, index=[fea1, fea2], values='label', aggfunc={len, np.mean, np.sum})\n",
    "            temp_pivot_table.reset_index(inplace=True)\n",
    "            temp_pivot_table.rename(columns={'len':fea1 + '_' + fea2 + '_count', 'mean':fea1 + '_' + fea2 + '_rate', 'sum':fea1 + '_' + fea2 + '_click_number'}, inplace=True)\n",
    "            alpha, beta = getBayesSmoothParam(temp_pivot_table[fea1 + '_' + fea2 + '_rate'])\n",
    "            temp_pivot_table[fea1 + '_' + fea2 + '_rate'] = (temp_pivot_table[fea1 + '_' + fea2 + '_click_number'] + alpha) / (temp_pivot_table[fea1 + '_' + fea2 + '_count'] + alpha + beta)\n",
    "#             del temp_pivot_table[fea1 + '_' + fea2 + '_click_number']\n",
    "            print(fea1 + '_' + fea2 + ' : finish!!!')\n",
    "            valid_df = pd.merge(valid_df, temp_pivot_table, on=[fea1, fea2], how='left')\n",
    "            train_df = train_temp_df\n",
    "            train_df.sort_index(by='index', ascending=True, inplace=True)\n",
    "    return train_df, valid_df\n",
    "\n",
    "jiaocha_fea_set = ['prefix', 'title', 'tag']\n",
    "train_df, valid_df = get_jiaocha_dimension_rate_feature(train_df, valid_df, jiaocha_fea_set)\n",
    "print(train_df.head())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 统计一些是否交叉的特征\n",
    "def get_is_title_in_query_feature(df):\n",
    "    x = df['title']\n",
    "    y = df['query_prediction_keys']\n",
    "    is_title_in_query = np.nan\n",
    "    if len(y) > 0:\n",
    "        if x in y:\n",
    "            is_title_in_query = 1\n",
    "        else:\n",
    "            is_title_in_query = 0\n",
    "    return is_title_in_query\n",
    "\n",
    "def get_is_prefix_in_title_feature(df):\n",
    "    x = df['prefix']\n",
    "    y = df['title']\n",
    "    is_prefix_in_title = np.nan\n",
    "    if x in y:\n",
    "        is_prefix_in_title = 1\n",
    "    else:\n",
    "        is_prefix_in_title = 0\n",
    "    return is_prefix_in_title\n",
    "\n",
    "train_df['is_title_in_query'] = train_df[['title', 'query_prediction_keys']].apply(get_is_title_in_query_feature, axis = 1)\n",
    "valid_df['is_title_in_query'] = valid_df[['title', 'query_prediction_keys']].apply(get_is_title_in_query_feature, axis = 1)\n",
    "\n",
    "train_df['is_prefix_in_title'] = train_df[['prefix', 'title']].apply(get_is_prefix_in_title_feature, axis = 1)\n",
    "valid_df['is_prefix_in_title'] = valid_df[['prefix', 'title']].apply(get_is_prefix_in_title_feature, axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  prefix                                   query_prediction            title  \\\n",
      "0     小品  {\"小品大全宋小宝\": \"0.009\", \"小品相亲\": \"0.012\", \"小品剧本\": ...               小品   \n",
      "1   1368  {\"13688cc赛马会\": \"0.059\", \"13685367892\": \"0.124\"...  HCG大于1368%2C正常吗   \n",
      "2   1368  {\"13688cc赛马会\": \"0.059\", \"13685367892\": \"0.124\"...            1368年   \n",
      "3     银耳  {\"银耳汤的功效\": \"0.012\", \"银耳为什么不能天天吃\": \"0.009\", \"银耳...         银耳红枣汤的做法   \n",
      "4   月经量少  {\"月经量少喝红糖水好吗\": \"0.010\", \"月经量少该怎么调理\": \"0.016\", ...         月经量少怎么调理   \n",
      "\n",
      "  tag  label  index                              query_prediction_dict  \\\n",
      "0  阅读      0      0  {'小品大全宋小宝': '0.009', '小品相亲': '0.012', '小品剧本': ...   \n",
      "1  健康      0      1  {'13688cc赛马会': '0.059', '13685367892': '0.124'...   \n",
      "2  百科      1      2  {'13688cc赛马会': '0.059', '13685367892': '0.124'...   \n",
      "3  菜谱      1      3  {'银耳汤的功效': '0.012', '银耳为什么不能天天吃': '0.009', '银耳...   \n",
      "4  百科      0      4  {'月经量少喝红糖水好吗': '0.010', '月经量少该怎么调理': '0.016', ...   \n",
      "\n",
      "                               query_prediction_keys  \\\n",
      "0  [小品大全宋小宝, 小品相亲, 小品剧本, 小品搞笑大全, 小品不差钱, 小品搞笑大全剧本,...   \n",
      "1  [13688cc赛马会, 13685367892, 13688cc, 1368个单词就够了,...   \n",
      "2  [13688cc赛马会, 13685367892, 13688cc, 1368个单词就够了,...   \n",
      "3  [银耳汤的功效, 银耳为什么不能天天吃, 银耳莲子羹, 银耳的做法, 银耳的功效, 银耳莲子...   \n",
      "4  [月经量少喝红糖水好吗, 月经量少该怎么调理, 月经量少怎么, 月经量少发黑, 月经量少是什...   \n",
      "\n",
      "                             query_prediction_values  query_prediction_number  \\\n",
      "0  [0.009, 0.012, 0.02, 0.066, 0.007, 0.01, 0.198...                       10   \n",
      "1  [0.059, 0.124, 0.029, 0.07, 0.022, 0.042, 0.08...                        9   \n",
      "2  [0.059, 0.124, 0.029, 0.07, 0.022, 0.042, 0.08...                        9   \n",
      "3  [0.012, 0.009, 0.05, 0.045, 0.053, 0.014, 0.05...                       10   \n",
      "4  [0.01, 0.016, 0.009, 0.009, 0.569, 0.016, 0.02...                       10   \n",
      "\n",
      "               ...               is_title_in_query  is_prefix_in_title  \\\n",
      "0              ...                             0.0                   1   \n",
      "1              ...                             0.0                   1   \n",
      "2              ...                             1.0                   1   \n",
      "3              ...                             0.0                   1   \n",
      "4              ...                             1.0                   1   \n",
      "\n",
      "   title_tag_types  prefix_tag_types  tag_title_types  tag_prefix_types  \\\n",
      "0                2                 3            17838             17146   \n",
      "1                1                 2            45210             38751   \n",
      "2                1                 2           127159            107447   \n",
      "3                1                 4            12532             11213   \n",
      "4                2                 3           127159            107447   \n",
      "\n",
      "   title_prefix_types  prefix_title_types  tag_query_prediction_types  \\\n",
      "0                   1                   2                       16973   \n",
      "1                   1                   2                       36889   \n",
      "2                   1                   2                      105363   \n",
      "3                   4                   4                       11086   \n",
      "4                   4                   3                      105363   \n",
      "\n",
      "   title_query_prediction_types  \n",
      "0                             1  \n",
      "1                             1  \n",
      "2                             1  \n",
      "3                             4  \n",
      "4                             4  \n",
      "\n",
      "[5 rows x 45 columns]\n"
     ]
    }
   ],
   "source": [
    "# 统计一些交叉种类特征\n",
    "def get_jiaocha_type_feature(train_df, valid_df, jiaocha_type_list):\n",
    "    for jiaocha_type in jiaocha_type_list:\n",
    "        fea1 = jiaocha_type[0]\n",
    "        fea2 = jiaocha_type[1]\n",
    "        temp_df = pd.concat([train_df, valid_df])\n",
    "        temp_pivot_table = pd.pivot_table(temp_df[[fea1, fea2, 'label']], index=[fea1, fea2], values='label', aggfunc=len)\n",
    "        temp_pivot_table.reset_index(inplace=True)\n",
    "        final_pivot_table = pd.pivot_table(temp_pivot_table, index=fea1, values=fea2, aggfunc=len)\n",
    "        final_pivot_table.reset_index(inplace=True)\n",
    "        final_pivot_table.rename(columns={fea2 : fea1 + '_' + fea2 + '_types'}, inplace=True)\n",
    "        train_df = pd.merge(train_df, final_pivot_table[[fea1, fea1 + '_' + fea2 + '_types']], on=fea1, how='left')\n",
    "        valid_df = pd.merge(valid_df, final_pivot_table[[fea1, fea1 + '_' + fea2 + '_types']], on=fea1, how='left')\n",
    "    return train_df, valid_df\n",
    "\n",
    "jiaocha_type_list = [['title', 'tag'], ['prefix', 'tag'], ['tag', 'title'], ['tag', 'prefix'], \n",
    "                     ['title', 'prefix'], ['prefix', 'title'], ['tag', 'query_prediction'], ['title', 'query_prediction']]\n",
    "train_df, valid_df = get_jiaocha_type_feature(train_df, valid_df, jiaocha_type_list)\n",
    "print(train_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  prefix                                   query_prediction            title  \\\n",
      "0     小品  {\"小品大全宋小宝\": \"0.009\", \"小品相亲\": \"0.012\", \"小品剧本\": ...               小品   \n",
      "1   1368  {\"13688cc赛马会\": \"0.059\", \"13685367892\": \"0.124\"...  HCG大于1368%2C正常吗   \n",
      "2   1368  {\"13688cc赛马会\": \"0.059\", \"13685367892\": \"0.124\"...            1368年   \n",
      "3     银耳  {\"银耳汤的功效\": \"0.012\", \"银耳为什么不能天天吃\": \"0.009\", \"银耳...         银耳红枣汤的做法   \n",
      "4   月经量少  {\"月经量少喝红糖水好吗\": \"0.010\", \"月经量少该怎么调理\": \"0.016\", ...         月经量少怎么调理   \n",
      "\n",
      "  tag  label  index                              query_prediction_dict  \\\n",
      "0  阅读      0      0  {'小品大全宋小宝': '0.009', '小品相亲': '0.012', '小品剧本': ...   \n",
      "1  健康      0      1  {'13688cc赛马会': '0.059', '13685367892': '0.124'...   \n",
      "2  百科      1      2  {'13688cc赛马会': '0.059', '13685367892': '0.124'...   \n",
      "3  菜谱      1      3  {'银耳汤的功效': '0.012', '银耳为什么不能天天吃': '0.009', '银耳...   \n",
      "4  百科      0      4  {'月经量少喝红糖水好吗': '0.010', '月经量少该怎么调理': '0.016', ...   \n",
      "\n",
      "                               query_prediction_keys  \\\n",
      "0  [小品大全宋小宝, 小品相亲, 小品剧本, 小品搞笑大全, 小品不差钱, 小品搞笑大全剧本,...   \n",
      "1  [13688cc赛马会, 13685367892, 13688cc, 1368个单词就够了,...   \n",
      "2  [13688cc赛马会, 13685367892, 13688cc, 1368个单词就够了,...   \n",
      "3  [银耳汤的功效, 银耳为什么不能天天吃, 银耳莲子羹, 银耳的做法, 银耳的功效, 银耳莲子...   \n",
      "4  [月经量少喝红糖水好吗, 月经量少该怎么调理, 月经量少怎么, 月经量少发黑, 月经量少是什...   \n",
      "\n",
      "                             query_prediction_values  query_prediction_number  \\\n",
      "0  [0.009, 0.012, 0.02, 0.066, 0.007, 0.01, 0.198...                       10   \n",
      "1  [0.059, 0.124, 0.029, 0.07, 0.022, 0.042, 0.08...                        9   \n",
      "2  [0.059, 0.124, 0.029, 0.07, 0.022, 0.042, 0.08...                        9   \n",
      "3  [0.012, 0.009, 0.05, 0.045, 0.053, 0.014, 0.05...                       10   \n",
      "4  [0.01, 0.016, 0.009, 0.009, 0.569, 0.016, 0.02...                       10   \n",
      "\n",
      "        ...        prefix_len  title_len  query_prediction_key_len_max  \\\n",
      "0       ...                 2          2                          10.0   \n",
      "1       ...                 4         15                          15.0   \n",
      "2       ...                 4          5                          15.0   \n",
      "3       ...                 2          8                          10.0   \n",
      "4       ...                 4          8                          10.0   \n",
      "\n",
      "   query_prediction_key_len_min  query_prediction_key_len_mean  \\\n",
      "0                           4.0                       5.600000   \n",
      "1                           5.0                       9.222222   \n",
      "2                           5.0                       9.222222   \n",
      "3                           3.0                       5.700000   \n",
      "4                           6.0                       7.900000   \n",
      "\n",
      "   query_prediction_key_len_std  len_title-prefix  len_prefix/title  \\\n",
      "0                      2.009975                 0          1.000000   \n",
      "1                      3.010270                11          0.266667   \n",
      "2                      3.010270                 1          0.800000   \n",
      "3                      1.734935                 6          0.250000   \n",
      "4                      1.300000                 4          0.500000   \n",
      "\n",
      "   len_mean-title  len_mean/title  \n",
      "0        3.600000        2.800000  \n",
      "1       -5.777778        0.614815  \n",
      "2        4.222222        1.844444  \n",
      "3       -2.300000        0.712500  \n",
      "4       -0.100000        0.987500  \n",
      "\n",
      "[5 rows x 55 columns]\n"
     ]
    }
   ],
   "source": [
    "def get_key_len_list(x):\n",
    "    return_list = []\n",
    "    for temp in x:\n",
    "        return_list.append(len(temp))\n",
    "    return return_list\n",
    "\n",
    "# 统计一些跟字符串长度相关的特征\n",
    "def get_string_len_feature(df):\n",
    "    df['prefix_len'] = df['prefix'].map(lambda x : len(x))\n",
    "    df['title_len'] = df['title'].map(lambda x : len(x))\n",
    "    df['query_prediction_key_len_list'] = df['query_prediction_keys'].map(lambda x : get_key_len_list(x))\n",
    "    df['query_prediction_key_len_max'] = df['query_prediction_key_len_list'].map(lambda x : np.nan if len(x) == 0 else np.max(x))\n",
    "    df['query_prediction_key_len_min'] = df['query_prediction_key_len_list'].map(lambda x : np.nan if len(x) == 0 else np.min(x))\n",
    "    df['query_prediction_key_len_mean'] = df['query_prediction_key_len_list'].map(lambda x : np.nan if len(x) == 0 else np.mean(x))\n",
    "    df['query_prediction_key_len_std'] = df['query_prediction_key_len_list'].map(lambda x : np.nan if len(x) == 0 else np.std(x))\n",
    "    df['len_title-prefix'] = df['title_len'] - df['prefix_len']\n",
    "    df['len_prefix/title'] = df['prefix_len'] / df['title_len']\n",
    "    df['len_mean-title'] = df['query_prediction_key_len_mean'] - df['title_len']\n",
    "    df['len_mean/title'] = df['query_prediction_key_len_mean'] / df['title_len']\n",
    "    del df['query_prediction_key_len_list']\n",
    "    return df\n",
    "\n",
    "train_df = get_string_len_feature(train_df)\n",
    "valid_df = get_string_len_feature(valid_df)\n",
    "print(train_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  prefix                                   query_prediction            title  \\\n",
      "0     小品  {\"小品大全宋小宝\": \"0.009\", \"小品相亲\": \"0.012\", \"小品剧本\": ...               小品   \n",
      "1   1368  {\"13688cc赛马会\": \"0.059\", \"13685367892\": \"0.124\"...  HCG大于1368%2C正常吗   \n",
      "2   1368  {\"13688cc赛马会\": \"0.059\", \"13685367892\": \"0.124\"...            1368年   \n",
      "3     银耳  {\"银耳汤的功效\": \"0.012\", \"银耳为什么不能天天吃\": \"0.009\", \"银耳...         银耳红枣汤的做法   \n",
      "4   月经量少  {\"月经量少喝红糖水好吗\": \"0.010\", \"月经量少该怎么调理\": \"0.016\", ...         月经量少怎么调理   \n",
      "\n",
      "  tag  label  index                              query_prediction_dict  \\\n",
      "0  阅读      0      0  {'小品大全宋小宝': '0.009', '小品相亲': '0.012', '小品剧本': ...   \n",
      "1  健康      0      1  {'13688cc赛马会': '0.059', '13685367892': '0.124'...   \n",
      "2  百科      1      2  {'13688cc赛马会': '0.059', '13685367892': '0.124'...   \n",
      "3  菜谱      1      3  {'银耳汤的功效': '0.012', '银耳为什么不能天天吃': '0.009', '银耳...   \n",
      "4  百科      0      4  {'月经量少喝红糖水好吗': '0.010', '月经量少该怎么调理': '0.016', ...   \n",
      "\n",
      "                               query_prediction_keys  \\\n",
      "0  [小品大全宋小宝, 小品相亲, 小品剧本, 小品搞笑大全, 小品不差钱, 小品搞笑大全剧本,...   \n",
      "1  [13688cc赛马会, 13685367892, 13688cc, 1368个单词就够了,...   \n",
      "2  [13688cc赛马会, 13685367892, 13688cc, 1368个单词就够了,...   \n",
      "3  [银耳汤的功效, 银耳为什么不能天天吃, 银耳莲子羹, 银耳的做法, 银耳的功效, 银耳莲子...   \n",
      "4  [月经量少喝红糖水好吗, 月经量少该怎么调理, 月经量少怎么, 月经量少发黑, 月经量少是什...   \n",
      "\n",
      "                             query_prediction_values  query_prediction_number  \\\n",
      "0  [0.009, 0.012, 0.02, 0.066, 0.007, 0.01, 0.198...                       10   \n",
      "1  [0.059, 0.124, 0.029, 0.07, 0.022, 0.042, 0.08...                        9   \n",
      "2  [0.059, 0.124, 0.029, 0.07, 0.022, 0.042, 0.08...                        9   \n",
      "3  [0.012, 0.009, 0.05, 0.045, 0.053, 0.014, 0.05...                       10   \n",
      "4  [0.01, 0.016, 0.009, 0.009, 0.569, 0.016, 0.02...                       10   \n",
      "\n",
      "            ...             query_prediction_key_len_max  \\\n",
      "0           ...                                     10.0   \n",
      "1           ...                                     15.0   \n",
      "2           ...                                     15.0   \n",
      "3           ...                                     10.0   \n",
      "4           ...                                     10.0   \n",
      "\n",
      "   query_prediction_key_len_min  query_prediction_key_len_mean  \\\n",
      "0                           4.0                       5.600000   \n",
      "1                           5.0                       9.222222   \n",
      "2                           5.0                       9.222222   \n",
      "3                           3.0                       5.700000   \n",
      "4                           6.0                       7.900000   \n",
      "\n",
      "   query_prediction_key_len_std  len_title-prefix  len_prefix/title  \\\n",
      "0                      2.009975                 0          1.000000   \n",
      "1                      3.010270                11          0.266667   \n",
      "2                      3.010270                 1          0.800000   \n",
      "3                      1.734935                 6          0.250000   \n",
      "4                      1.300000                 4          0.500000   \n",
      "\n",
      "   len_mean-title  len_mean/title  title_prefix_leven  title_prefix_leven_rate  \n",
      "0        3.600000        2.800000                   0                 0.000000  \n",
      "1       -5.777778        0.614815                  11                 0.611111  \n",
      "2        4.222222        1.844444                   1                 0.125000  \n",
      "3       -2.300000        0.712500                   6                 0.545455  \n",
      "4       -0.100000        0.987500                   4                 0.363636  \n",
      "\n",
      "[5 rows x 57 columns]\n"
     ]
    }
   ],
   "source": [
    "# 统计title跟prefix的编辑距离\n",
    "def get_title_prefix_levenshtein_distance(df):\n",
    "    title = df['title']\n",
    "    prefix = df['prefix']\n",
    "    return Levenshtein.distance(title, prefix)\n",
    "\n",
    "def get_title_prefix_levenshtein_distance_rate(df):\n",
    "    title_prefix_leven = df['title_prefix_leven']\n",
    "    title = df['title']\n",
    "    return (title_prefix_leven / (len(title) + 3))\n",
    "\n",
    "train_df['title_prefix_leven'] = train_df[['title', 'prefix']].apply(get_title_prefix_levenshtein_distance, axis=1)\n",
    "valid_df['title_prefix_leven'] = valid_df[['title', 'prefix']].apply(get_title_prefix_levenshtein_distance, axis=1)\n",
    "\n",
    "train_df['title_prefix_leven_rate'] = train_df[['title', 'title_prefix_leven']].apply(get_title_prefix_levenshtein_distance_rate, axis=1)\n",
    "valid_df['title_prefix_leven_rate'] = valid_df[['title', 'title_prefix_leven']].apply(get_title_prefix_levenshtein_distance_rate, axis=1)\n",
    "\n",
    "print(train_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  prefix                                   query_prediction            title  \\\n",
      "0     小品  {\"小品大全宋小宝\": \"0.009\", \"小品相亲\": \"0.012\", \"小品剧本\": ...               小品   \n",
      "1   1368  {\"13688cc赛马会\": \"0.059\", \"13685367892\": \"0.124\"...  HCG大于1368%2C正常吗   \n",
      "2   1368  {\"13688cc赛马会\": \"0.059\", \"13685367892\": \"0.124\"...            1368年   \n",
      "3     银耳  {\"银耳汤的功效\": \"0.012\", \"银耳为什么不能天天吃\": \"0.009\", \"银耳...         银耳红枣汤的做法   \n",
      "4   月经量少  {\"月经量少喝红糖水好吗\": \"0.010\", \"月经量少该怎么调理\": \"0.016\", ...         月经量少怎么调理   \n",
      "\n",
      "  tag  label  index                              query_prediction_dict  \\\n",
      "0  阅读      0      0  {'小品大全宋小宝': '0.009', '小品相亲': '0.012', '小品剧本': ...   \n",
      "1  健康      0      1  {'13688cc赛马会': '0.059', '13685367892': '0.124'...   \n",
      "2  百科      1      2  {'13688cc赛马会': '0.059', '13685367892': '0.124'...   \n",
      "3  菜谱      1      3  {'银耳汤的功效': '0.012', '银耳为什么不能天天吃': '0.009', '银耳...   \n",
      "4  百科      0      4  {'月经量少喝红糖水好吗': '0.010', '月经量少该怎么调理': '0.016', ...   \n",
      "\n",
      "                               query_prediction_keys  \\\n",
      "0  [小品大全宋小宝, 小品相亲, 小品剧本, 小品搞笑大全, 小品不差钱, 小品搞笑大全剧本,...   \n",
      "1  [13688cc赛马会, 13685367892, 13688cc, 1368个单词就够了,...   \n",
      "2  [13688cc赛马会, 13685367892, 13688cc, 1368个单词就够了,...   \n",
      "3  [银耳汤的功效, 银耳为什么不能天天吃, 银耳莲子羹, 银耳的做法, 银耳的功效, 银耳莲子...   \n",
      "4  [月经量少喝红糖水好吗, 月经量少该怎么调理, 月经量少怎么, 月经量少发黑, 月经量少是什...   \n",
      "\n",
      "                             query_prediction_values  query_prediction_number  \\\n",
      "0  [0.009, 0.012, 0.02, 0.066, 0.007, 0.01, 0.198...                       10   \n",
      "1  [0.059, 0.124, 0.029, 0.07, 0.022, 0.042, 0.08...                        9   \n",
      "2  [0.059, 0.124, 0.029, 0.07, 0.022, 0.042, 0.08...                        9   \n",
      "3  [0.012, 0.009, 0.05, 0.045, 0.053, 0.014, 0.05...                       10   \n",
      "4  [0.01, 0.016, 0.009, 0.009, 0.569, 0.016, 0.02...                       10   \n",
      "\n",
      "           ...            len_mean-title  len_mean/title  title_prefix_leven  \\\n",
      "0          ...                  3.600000        2.800000                   0   \n",
      "1          ...                 -5.777778        0.614815                  11   \n",
      "2          ...                  4.222222        1.844444                   1   \n",
      "3          ...                 -2.300000        0.712500                   6   \n",
      "4          ...                 -0.100000        0.987500                   4   \n",
      "\n",
      "   title_prefix_leven_rate                             title_query_leven_list  \\\n",
      "0                 0.000000  [0.045, 0.024, 0.04, 0.264, 0.021, 0.06, 0.396...   \n",
      "1                 0.611111  [0.649, 1.488, 0.319, 0.77, 0.242, 0.504, 0.94...   \n",
      "2                 0.125000  [0.354, 0.868, 0.08700000000000001, 0.42000000...   \n",
      "3                 0.545455  [0.048, 0.072, 0.30000000000000004, 0.135, 0.2...   \n",
      "4                 0.363636  [0.06, 0.016, 0.018, 0.036, 2.276, 0.064, 0.08...   \n",
      "\n",
      "   title_query_leven_sum  title_query_leven_max  title_query_leven_min  \\\n",
      "0                  1.054                  0.396                  0.020   \n",
      "1                  5.978                  1.488                  0.242   \n",
      "2                  2.623                  0.868                  0.000   \n",
      "3                  1.590                  0.342                  0.048   \n",
      "4                  2.672                  2.276                  0.000   \n",
      "\n",
      "   title_query_leven_mean  title_query_leven_std  \n",
      "0                0.105400               0.120359  \n",
      "1                0.664222               0.364696  \n",
      "2                0.291444               0.248284  \n",
      "3                0.159000               0.103037  \n",
      "4                0.267200               0.670101  \n",
      "\n",
      "[5 rows x 63 columns]\n"
     ]
    }
   ],
   "source": [
    "# 统计title跟query_prediction编辑距离相关的特征\n",
    "def get_title_query_levenshtein_distance_list(df):\n",
    "    query_keys_list = df['query_prediction_keys']\n",
    "    query_values_list = df['query_prediction_values']\n",
    "    title = df['title']\n",
    "    return_list = list()\n",
    "    for i in range(len(query_keys_list)):\n",
    "        distance = Levenshtein.distance(title, query_keys_list[i])\n",
    "        return_list.append(distance * query_values_list[i])\n",
    "    return return_list\n",
    "\n",
    "def get_title_query_levenshtein_distance_feature(df):\n",
    "    df['title_query_leven_list'] = df[['query_prediction_keys', 'query_prediction_values', 'title']].apply(get_title_query_levenshtein_distance_list, axis=1)\n",
    "    df['title_query_leven_sum'] = df['title_query_leven_list'].map(lambda x : np.nan if len(x) == 0 else np.sum(x))\n",
    "    df['title_query_leven_max'] = df['title_query_leven_list'].map(lambda x : np.nan if len(x) == 0 else np.max(x))\n",
    "    df['title_query_leven_min'] = df['title_query_leven_list'].map(lambda x : np.nan if len(x) == 0 else np.min(x))\n",
    "    df['title_query_leven_mean'] = df['title_query_leven_list'].map(lambda x : np.nan if len(x) == 0 else np.mean(x))\n",
    "    df['title_query_leven_std'] = df['title_query_leven_list'].map(lambda x : np.nan if len(x) == 0 else np.std(x))\n",
    "    return df\n",
    "\n",
    "train_df = get_title_query_levenshtein_distance_feature(train_df)\n",
    "valid_df = get_title_query_levenshtein_distance_feature(valid_df)\n",
    "print(train_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Dump cache file failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/jieba/__init__.py\", line 152, in initialize\n",
      "    _replace_file(fpath, cache_file)\n",
      "PermissionError: [Errno 1] Operation not permitted: '/tmp/tmpejumvqsg' -> '/tmp/jieba.cache'\n",
      "Loading model cost 2.079 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  prefix                                   query_prediction            title  \\\n",
      "0     小品  {\"小品大全宋小宝\": \"0.009\", \"小品相亲\": \"0.012\", \"小品剧本\": ...               小品   \n",
      "1   1368  {\"13688cc赛马会\": \"0.059\", \"13685367892\": \"0.124\"...  HCG大于1368%2C正常吗   \n",
      "2   1368  {\"13688cc赛马会\": \"0.059\", \"13685367892\": \"0.124\"...            1368年   \n",
      "3     银耳  {\"银耳汤的功效\": \"0.012\", \"银耳为什么不能天天吃\": \"0.009\", \"银耳...         银耳红枣汤的做法   \n",
      "4   月经量少  {\"月经量少喝红糖水好吗\": \"0.010\", \"月经量少该怎么调理\": \"0.016\", ...         月经量少怎么调理   \n",
      "\n",
      "  tag  label  index                              query_prediction_dict  \\\n",
      "0  阅读      0      0  {'小品大全宋小宝': '0.009', '小品相亲': '0.012', '小品剧本': ...   \n",
      "1  健康      0      1  {'13688cc赛马会': '0.059', '13685367892': '0.124'...   \n",
      "2  百科      1      2  {'13688cc赛马会': '0.059', '13685367892': '0.124'...   \n",
      "3  菜谱      1      3  {'银耳汤的功效': '0.012', '银耳为什么不能天天吃': '0.009', '银耳...   \n",
      "4  百科      0      4  {'月经量少喝红糖水好吗': '0.010', '月经量少该怎么调理': '0.016', ...   \n",
      "\n",
      "                               query_prediction_keys  \\\n",
      "0  [小品大全宋小宝, 小品相亲, 小品剧本, 小品搞笑大全, 小品不差钱, 小品搞笑大全剧本,...   \n",
      "1  [13688cc赛马会, 13685367892, 13688cc, 1368个单词就够了,...   \n",
      "2  [13688cc赛马会, 13685367892, 13688cc, 1368个单词就够了,...   \n",
      "3  [银耳汤的功效, 银耳为什么不能天天吃, 银耳莲子羹, 银耳的做法, 银耳的功效, 银耳莲子...   \n",
      "4  [月经量少喝红糖水好吗, 月经量少该怎么调理, 月经量少怎么, 月经量少发黑, 月经量少是什...   \n",
      "\n",
      "                             query_prediction_values  query_prediction_number  \\\n",
      "0  [0.009, 0.012, 0.02, 0.066, 0.007, 0.01, 0.198...                       10   \n",
      "1  [0.059, 0.124, 0.029, 0.07, 0.022, 0.042, 0.08...                        9   \n",
      "2  [0.059, 0.124, 0.029, 0.07, 0.022, 0.042, 0.08...                        9   \n",
      "3  [0.012, 0.009, 0.05, 0.045, 0.053, 0.014, 0.05...                       10   \n",
      "4  [0.01, 0.016, 0.009, 0.009, 0.569, 0.016, 0.02...                       10   \n",
      "\n",
      "          ...          title_query_leven_min  title_query_leven_mean  \\\n",
      "0         ...                          0.020                0.105400   \n",
      "1         ...                          0.242                0.664222   \n",
      "2         ...                          0.000                0.291444   \n",
      "3         ...                          0.048                0.159000   \n",
      "4         ...                          0.000                0.267200   \n",
      "\n",
      "   title_query_leven_std                     query_prediction_key_sentences  \\\n",
      "0               0.120359  小品大全宋小宝小品相亲小品剧本小品搞笑大全小品不差钱小品搞笑大全剧本小品大全小品演员小品视频...   \n",
      "1               0.364696  13688cc赛马会1368536789213688cc1368个单词就够了1368e136...   \n",
      "2               0.248284  13688cc赛马会1368536789213688cc1368个单词就够了1368e136...   \n",
      "3               0.103037  银耳汤的功效银耳为什么不能天天吃银耳莲子羹银耳的做法银耳的功效银耳莲子汤银耳汤的做法银耳红枣...   \n",
      "4               0.670101  月经量少喝红糖水好吗月经量少该怎么调理月经量少怎么月经量少发黑月经量少是什么原因月经量少吃什...   \n",
      "\n",
      "   query_prediction_key_max_sentences  \\\n",
      "0                                小品大全   \n",
      "1                         13685367892   \n",
      "2                         13685367892   \n",
      "3                               银耳红枣汤   \n",
      "4                           月经量少是什么原因   \n",
      "\n",
      "                    query_prediction_key_jieba_words  \\\n",
      "0  [小品, 大全, 宋, 小宝, 小品, 相亲, 小品, 剧本, 小品, 搞笑, 大全, 小品...   \n",
      "1  [13688cc, 赛马会, 1368536789213688cc1368, 个, 单词, ...   \n",
      "2  [13688cc, 赛马会, 1368536789213688cc1368, 个, 单词, ...   \n",
      "3  [银耳汤, 的, 功效, 银耳, 为什么, 不能, 天天, 吃, 银耳, 莲子, 羹, 银耳...   \n",
      "4  [月经, 量少, 喝, 红糖, 水好, 吗, 月经, 量少, 该, 怎么, 调理, 月经, ...   \n",
      "\n",
      "   query_prediction_key_max_jieba_words  \\\n",
      "0                              [小品, 大全]   \n",
      "1                         [13685367892]   \n",
      "2                         [13685367892]   \n",
      "3                             [银耳, 红枣汤]   \n",
      "4                   [月经, 量少, 是, 什么, 原因]   \n",
      "\n",
      "                              query_prediction_words  \\\n",
      "0  [[小品, 大全, 宋, 小宝], [小品, 相亲], [小品, 剧本], [小品, 搞笑,...   \n",
      "1  [[13688cc, 赛马会], [13685367892], [13688cc], [13...   \n",
      "2  [[13688cc, 赛马会], [13685367892], [13688cc], [13...   \n",
      "3  [[银耳汤, 的, 功效], [银耳, 为什么, 不能, 天天, 吃], [银耳, 莲子, ...   \n",
      "4  [[月经, 量少, 喝, 红糖, 水好, 吗], [月经, 量少, 该, 怎么, 调理], ...   \n",
      "\n",
      "             title_jieba_words  prefix_jieba_words  \n",
      "0                         [小品]                [小品]  \n",
      "1  [HCG, 大于, 1368%, 2C, 正常, 吗]              [1368]  \n",
      "2                    [1368, 年]              [1368]  \n",
      "3             [银耳, 红枣汤, 的, 做法]                [银耳]  \n",
      "4             [月经, 量少, 怎么, 调理]            [月经, 量少]  \n",
      "\n",
      "[5 rows x 70 columns]\n"
     ]
    }
   ],
   "source": [
    "#分词方法，调用结巴接口\n",
    "def jieba_seg_to_list(sentence, pos=False):\n",
    "    if not pos:\n",
    "        #不进行词性标注的分词方法\n",
    "        seg_list = jieba.cut(sentence)\n",
    "    else:\n",
    "        #进行词性标注的分词方法\n",
    "        seg_list = psg.cut(sentence)\n",
    "    return seg_list\n",
    "\n",
    "#去除干扰词\n",
    "def jieba_word_filter(seg_list, pos=False):\n",
    "    \n",
    "    filter_list = []\n",
    "    #根据pos参数选择是否词性过滤\n",
    "    #不进行词性过滤，则将词性都标记为n，表示全部保留\n",
    "    for seg in seg_list:\n",
    "        if not pos:\n",
    "            word = seg\n",
    "            flag = 'n'\n",
    "        else:\n",
    "            word = seg.word\n",
    "            flag = seg.flag\n",
    "        if not flag.startswith('n'):\n",
    "            continue\n",
    "        filter_list.append(word)\n",
    "    return filter_list\n",
    "\n",
    "def jieba_word_deal(sentence, pos=False):\n",
    "    #调用上面方式对数据集进行处理，处理后的每条数据仅保留非干扰词\n",
    "    seg_list = jieba_seg_to_list(sentence, pos)\n",
    "    filter_list = jieba_word_filter(seg_list, pos)\n",
    "    return filter_list\n",
    "\n",
    "def get_prefix_prediction_key_sentences(x):\n",
    "    prefix_prediction_key_sentences = \"\"\n",
    "    for temp in x:\n",
    "        if len(prefix_prediction_key_sentences) > 0:\n",
    "            prefix_prediction_key_sentences = prefix_prediction_key_sentences + temp\n",
    "        else:\n",
    "            prefix_prediction_key_sentences = temp\n",
    "    return prefix_prediction_key_sentences\n",
    "\n",
    "def get_max_query_key_sentences(x):\n",
    "    if len(x) == 0:\n",
    "        return \"\"\n",
    "    else:\n",
    "        return max(x, key=x.get)\n",
    "\n",
    "def get_jieba_word(df):\n",
    "    df['query_prediction_key_sentences'] = df['query_prediction_keys'].map(lambda x : get_prefix_prediction_key_sentences(x))\n",
    "    df['query_prediction_key_max_sentences'] = df['query_prediction_dict'].map(lambda x : get_max_query_key_sentences(x))\n",
    "    df['query_prediction_key_jieba_words'] = df['query_prediction_key_sentences'].map(lambda x : jieba_word_deal(x, False))\n",
    "    df['query_prediction_key_max_jieba_words'] = df['query_prediction_key_max_sentences'].map(lambda x : jieba_word_deal(x, False))\n",
    "    df['query_prediction_words'] = df['query_prediction_keys'].map(lambda x : [jieba_word_deal(j, False) for j in x] if len(x) > 0 else np.nan)\n",
    "    df['title_jieba_words'] = df['title'].map(lambda x : jieba_word_deal(x, False))\n",
    "    df['prefix_jieba_words'] = df['prefix'].map(lambda x : jieba_word_deal(x, False))\n",
    "#     del df['query_prediction_key_sentences']\n",
    "    return df\n",
    "\n",
    "train_df = get_jieba_word(train_df)\n",
    "valid_df = get_jieba_word(valid_df)\n",
    "print(train_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_t_word_match finish!!!\n",
      "q_t_common_words finish!!!\n",
      "q_t_total_unique_words finish!!!\n",
      "q_t_wc_diff finish!!!\n",
      "q_t_wc_ratio finish!!!\n",
      "q_t_wc_diff_unique finish!!!\n",
      "q_t_wc_ratio_unique finish!!!\n",
      "p_t_word_match finish!!!\n",
      "p_t_common_words finish!!!\n",
      "p_t_total_unique_words finish!!!\n",
      "p_t_wc_diff finish!!!\n",
      "p_t_wc_ratio finish!!!\n",
      "p_t_wc_diff_unique finish!!!\n",
      "p_t_wc_ratio_unique finish!!!\n",
      "p_q_word_match finish!!!\n",
      "p_q_common_words finish!!!\n",
      "p_q_total_unique_words finish!!!\n",
      "p_q_wc_diff finish!!!\n",
      "p_q_wc_ratio finish!!!\n",
      "p_q_wc_diff_unique finish!!!\n",
      "p_q_wc_ratio_unique finish!!!\n",
      "  prefix                                   query_prediction            title  \\\n",
      "0     小品  {\"小品大全宋小宝\": \"0.009\", \"小品相亲\": \"0.012\", \"小品剧本\": ...               小品   \n",
      "1   1368  {\"13688cc赛马会\": \"0.059\", \"13685367892\": \"0.124\"...  HCG大于1368%2C正常吗   \n",
      "2   1368  {\"13688cc赛马会\": \"0.059\", \"13685367892\": \"0.124\"...            1368年   \n",
      "3     银耳  {\"银耳汤的功效\": \"0.012\", \"银耳为什么不能天天吃\": \"0.009\", \"银耳...         银耳红枣汤的做法   \n",
      "4   月经量少  {\"月经量少喝红糖水好吗\": \"0.010\", \"月经量少该怎么调理\": \"0.016\", ...         月经量少怎么调理   \n",
      "\n",
      "  tag  label  index                              query_prediction_dict  \\\n",
      "0  阅读      0      0  {'小品大全宋小宝': '0.009', '小品相亲': '0.012', '小品剧本': ...   \n",
      "1  健康      0      1  {'13688cc赛马会': '0.059', '13685367892': '0.124'...   \n",
      "2  百科      1      2  {'13688cc赛马会': '0.059', '13685367892': '0.124'...   \n",
      "3  菜谱      1      3  {'银耳汤的功效': '0.012', '银耳为什么不能天天吃': '0.009', '银耳...   \n",
      "4  百科      0      4  {'月经量少喝红糖水好吗': '0.010', '月经量少该怎么调理': '0.016', ...   \n",
      "\n",
      "                               query_prediction_keys  \\\n",
      "0  [小品大全宋小宝, 小品相亲, 小品剧本, 小品搞笑大全, 小品不差钱, 小品搞笑大全剧本,...   \n",
      "1  [13688cc赛马会, 13685367892, 13688cc, 1368个单词就够了,...   \n",
      "2  [13688cc赛马会, 13685367892, 13688cc, 1368个单词就够了,...   \n",
      "3  [银耳汤的功效, 银耳为什么不能天天吃, 银耳莲子羹, 银耳的做法, 银耳的功效, 银耳莲子...   \n",
      "4  [月经量少喝红糖水好吗, 月经量少该怎么调理, 月经量少怎么, 月经量少发黑, 月经量少是什...   \n",
      "\n",
      "                             query_prediction_values  query_prediction_number  \\\n",
      "0  [0.009, 0.012, 0.02, 0.066, 0.007, 0.01, 0.198...                       10   \n",
      "1  [0.059, 0.124, 0.029, 0.07, 0.022, 0.042, 0.08...                        9   \n",
      "2  [0.059, 0.124, 0.029, 0.07, 0.022, 0.042, 0.08...                        9   \n",
      "3  [0.012, 0.009, 0.05, 0.045, 0.053, 0.014, 0.05...                       10   \n",
      "4  [0.01, 0.016, 0.009, 0.009, 0.569, 0.016, 0.02...                       10   \n",
      "\n",
      "          ...           p_t_wc_ratio  p_t_wc_diff_unique  p_t_wc_ratio_unique  \\\n",
      "0         ...                    1.0                   0                  1.0   \n",
      "1         ...                    6.0                   5                  6.0   \n",
      "2         ...                    2.0                   1                  2.0   \n",
      "3         ...                    4.0                   3                  4.0   \n",
      "4         ...                    2.0                   2                  2.0   \n",
      "\n",
      "   p_q_word_match  p_q_common_words  p_q_total_unique_words  p_q_wc_diff  \\\n",
      "0        0.153846                 1                      12           28   \n",
      "1        0.000000                 0                      15           13   \n",
      "2        0.000000                 0                      15           13   \n",
      "3        0.133333                 1                      14           28   \n",
      "4        0.222222                 2                      16           41   \n",
      "\n",
      "   p_q_wc_ratio  p_q_wc_diff_unique  p_q_wc_ratio_unique  \n",
      "0          29.0                  11                 12.0  \n",
      "1          14.0                  13                 14.0  \n",
      "2          14.0                  13                 14.0  \n",
      "3          29.0                  13                 14.0  \n",
      "4          21.5                  14                  8.0  \n",
      "\n",
      "[5 rows x 91 columns]\n"
     ]
    }
   ],
   "source": [
    "def word_match_share(df):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in df[0]:\n",
    "        q1words[word] = 1\n",
    "    for word in df[1]:\n",
    "        q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
    "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
    "    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n",
    "    return R\n",
    "\n",
    "def jaccard(df):\n",
    "    wic = set(df[0]).intersection(set(df[1]))\n",
    "    uw = set(df[0]).union(df[1])\n",
    "    if len(uw) == 0:\n",
    "        uw = [1]\n",
    "    return (len(wic) / len(uw))\n",
    "\n",
    "def common_words(df):\n",
    "    return len(set(df[0]).intersection(set(df[1])))\n",
    "\n",
    "def total_unique_words(df):\n",
    "    return len(set(df[0]).union(df[1]))\n",
    "\n",
    "def wc_diff(df):\n",
    "    return abs(len(df[0]) - len(df[1]))\n",
    "\n",
    "def wc_ratio(df):\n",
    "    l1 = len(df[0])*1.0 \n",
    "    l2 = len(df[1])\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "\n",
    "def wc_diff_unique(df):\n",
    "    return abs(len(set(df[0])) - len(set(df[1])))\n",
    "    \n",
    "def wc_ratio_unique(df):\n",
    "    l1 = len(set(df[0])) * 1.0\n",
    "    l2 = len(set(df[1]))\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "    \n",
    "def tfidf_word_match_share(df, weights=None):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in df[0]:\n",
    "        q1words[word] = 1\n",
    "    for word in df[1]:\n",
    "        q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
    "    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "    R = np.sum(shared_weights) / np.sum(total_weights)\n",
    "    return R\n",
    "\n",
    "def deal_word_for_all(train_df, valid_df, fea1, fea2, func, colName):\n",
    "    train_df[colName] = train_df[[fea1, fea2]].apply(func, axis=1)\n",
    "    valid_df[colName] = valid_df[[fea1, fea2]].apply(func, axis=1)\n",
    "    print(colName + ' finish!!!')\n",
    "    return train_df, valid_df\n",
    "                   \n",
    "def get_weight(count, eps=10000, min_count=2):\n",
    "    if count < min_count:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 / (count + eps)\n",
    "\n",
    "def get_word_statistic_feature(train_df, valid_df, col_list):\n",
    "    df = pd.concat([train_df[['query_prediction_key_jieba_words', 'title_jieba_words', 'prefix_jieba_words']], valid_df[['query_prediction_key_jieba_words', 'title_jieba_words', 'prefix_jieba_words']]])\n",
    "    train_qs = pd.Series(df['query_prediction_key_jieba_words'].tolist() + df['title_jieba_words'].tolist() + df['prefix_jieba_words'].tolist())\n",
    "    words = [x for y in train_qs for x in y]\n",
    "    counts = Counter(words)\n",
    "    weights = {word: get_weight(count) for word, count in counts.items()}\n",
    "    for col in col_list:\n",
    "        fea1 = col[0]\n",
    "        fea2 = col[1]\n",
    "        train_df, valid_df = deal_word_for_all(train_df, valid_df, fea1, fea2, word_match_share, fea1[0] + '_' + fea2[0] + '_word_match')\n",
    "#         train_df, valid_df = deal_word_for_all(train_df, valid_df, fea1, fea2, jaccard, fea1[0] + '_' + fea2[0] + '_jaccard')\n",
    "        train_df, valid_df = deal_word_for_all(train_df, valid_df, fea1, fea2, common_words, fea1[0] + '_' + fea2[0] + '_common_words')\n",
    "        train_df, valid_df = deal_word_for_all(train_df, valid_df, fea1, fea2, total_unique_words, fea1[0] + '_' + fea2[0] + '_total_unique_words')\n",
    "        train_df, valid_df = deal_word_for_all(train_df, valid_df, fea1, fea2, wc_diff, fea1[0] + '_' + fea2[0] + '_wc_diff')\n",
    "        train_df, valid_df = deal_word_for_all(train_df, valid_df, fea1, fea2, wc_ratio, fea1[0] + '_' + fea2[0] + '_wc_ratio')\n",
    "        train_df, valid_df = deal_word_for_all(train_df, valid_df, fea1, fea2, wc_diff_unique, fea1[0] + '_' + fea2[0] + '_wc_diff_unique')\n",
    "        train_df, valid_df = deal_word_for_all(train_df, valid_df, fea1, fea2, wc_ratio_unique, fea1[0] + '_' + fea2[0] + '_wc_ratio_unique')\n",
    "#         f = functools.partial(tfidf_word_match_share, weights=weights)\n",
    "#         train_df, valid_df = deal_word_for_all(train_df, valid_df, fea1, fea2, f, fea1[0] + '_' + fea2[0] + '_tfidf_word_match_share')\n",
    "    return train_df, valid_df\n",
    "\n",
    "col_list = [['query_prediction_key_jieba_words', 'title_jieba_words'], ['prefix_jieba_words', 'title_jieba_words'], ['prefix_jieba_words', 'query_prediction_key_jieba_words']]\n",
    "train_df, valid_df = get_word_statistic_feature(train_df, valid_df, col_list)\n",
    "print(train_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set values for various parameters\n",
    "dir_num_features = 30  # Word vector dimensionality                      \n",
    "dir_min_word_count = 1  # Minimum word count                        \n",
    "dir_num_workers = 20       # Number of threads to run in parallel\n",
    "dir_context = 5          # Context window size                                                                                    \n",
    "dir_downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "word2vec_df = train_df[['query_prediction_words', 'title_jieba_words', 'prefix_jieba_words', 'query_prediction_number']]\n",
    "word2vec_df.reset_index(inplace=True)\n",
    "word2vec_list = word2vec_df['title_jieba_words'].tolist() + word2vec_df['prefix_jieba_words'].tolist() + [y for x in word2vec_df['query_prediction_words'][word2vec_df.query_prediction_number > 0] for y in x]\n",
    "dir_model = word2vec.Word2Vec(word2vec_list, workers=dir_num_workers, \\\n",
    "            size=dir_num_features, min_count = dir_min_word_count, \\\n",
    "            window = dir_context, sample = dir_downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "dir_model.init_sims(replace=True)\n",
    "\n",
    "dir_word_wv = dir_model.wv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:73: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    }
   ],
   "source": [
    "def get_dir_w2v_array(word_list, word_wv, num_features):\n",
    "    word_vectors = np.zeros((len(word_list), num_features))\n",
    "    for i in range(len(word_list)):\n",
    "        if str(word_list[i]) in word_wv.vocab.keys():\n",
    "            word_vectors[i][:] = word_wv[str(word_list[i])]\n",
    "    mean_array = np.mean(word_vectors, axis=0)\n",
    "    return mean_array\n",
    "\n",
    "train_df['dir_title_jieba_array'] = train_df['title_jieba_words'].map(lambda x : get_dir_w2v_array(x, dir_word_wv, dir_num_features))\n",
    "valid_df['dir_title_jieba_array'] = valid_df['title_jieba_words'].map(lambda x : get_dir_w2v_array(x, dir_word_wv, dir_num_features))\n",
    "\n",
    "train_df['dir_prefix_jieba_array'] = train_df['prefix_jieba_words'].map(lambda x : get_dir_w2v_array(x, dir_word_wv, dir_num_features))\n",
    "valid_df['dir_prefix_jieba_array'] = valid_df['prefix_jieba_words'].map(lambda x : get_dir_w2v_array(x, dir_word_wv, dir_num_features))\n",
    "\n",
    "train_df['dir_query_max_jieba_array'] = train_df['query_prediction_key_max_jieba_words'].map(lambda x : get_dir_w2v_array(x, dir_word_wv, dir_num_features))\n",
    "valid_df['dir_query_max_jieba_array'] = valid_df['query_prediction_key_max_jieba_words'].map(lambda x : get_dir_w2v_array(x, dir_word_wv, dir_num_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dir_t_p_w2v_array(df):\n",
    "    dir_title_jieba_array = df['dir_title_jieba_array']\n",
    "    dir_prefix_jieba_array = df['dir_prefix_jieba_array']\n",
    "    return_list = list()\n",
    "    if (len(dir_title_jieba_array) == 30) & (len(dir_prefix_jieba_array) == 30):\n",
    "        for i in range(30):\n",
    "            return_list.append(dir_title_jieba_array[i] - dir_prefix_jieba_array[i])\n",
    "    return return_list\n",
    "\n",
    "def get_dir_t_maxQ_w2v_array(df):\n",
    "    dir_title_jieba_array = df['dir_title_jieba_array']\n",
    "    dir_query_max_jieba_array = df['dir_query_max_jieba_array']\n",
    "    return_list = list()\n",
    "    if (len(dir_title_jieba_array) == 30) & (len(dir_query_max_jieba_array) == 30):\n",
    "        for i in range(30):\n",
    "            return_list.append(dir_title_jieba_array[i] - dir_query_max_jieba_array[i])\n",
    "    return return_list\n",
    "\n",
    "train_df['dir_t_p_array'] = train_df.apply(get_dir_t_p_w2v_array, axis=1)\n",
    "valid_df['dir_t_p_array'] = valid_df.apply(get_dir_t_p_w2v_array, axis=1)\n",
    "\n",
    "train_df['dir_t_maxQ_array'] = train_df.apply(get_dir_t_maxQ_w2v_array, axis=1)\n",
    "valid_df['dir_t_maxQ_array'] = valid_df.apply(get_dir_t_maxQ_w2v_array, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dir_similarity_feature(df, dir_num_features):\n",
    "    for i in range(dir_num_features):\n",
    "        df['dir_t_p_array_' + str(i) + '_fea'] = df['dir_t_p_array'].map(lambda x: np.nan if len(x) < (i+1) else x[i])\n",
    "        df['dir_t_maxQ_array' + str(i) + '_fea'] = df['dir_t_maxQ_array'].map(lambda x: np.nan if len(x) < (i+1) else x[i])\n",
    "    return df\n",
    "\n",
    "train_df = get_dir_similarity_feature(train_df, dir_num_features)\n",
    "valid_df = get_dir_similarity_feature(valid_df, dir_num_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dir_w2v_feature(df, dir_num_features):\n",
    "    for i in range(dir_num_features):\n",
    "        df['dir_t_array_' + str(i) + '_fea'] = df['dir_title_jieba_array'].map(lambda x: np.nan if len(x) < (i+1) else x[i])\n",
    "        df['dir_p_array' + str(i) + '_fea'] = df['dir_prefix_jieba_array'].map(lambda x: np.nan if len(x) < (i+1) else x[i])\n",
    "        df['dir_maxQ_array' + str(i) + '_fea'] = df['dir_query_max_jieba_array'].map(lambda x: np.nan if len(x) < (i+1) else x[i])\n",
    "    return df\n",
    "\n",
    "train_df = get_dir_w2v_feature(train_df, dir_num_features)\n",
    "valid_df = get_dir_w2v_feature(valid_df, dir_num_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set values for various parameters\n",
    "num_features = 500  # Word vector dimensionality                      \n",
    "min_word_count = 1  # Minimum word count                        \n",
    "num_workers = 20       # Number of threads to run in parallel\n",
    "context = 5          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "word2vec_df = train_df[['query_prediction_words', 'title_jieba_words', 'prefix_jieba_words', 'query_prediction_number']]\n",
    "word2vec_df.reset_index(inplace=True)\n",
    "word2vec_list = word2vec_df['title_jieba_words'].tolist() + word2vec_df['prefix_jieba_words'].tolist() + [y for x in word2vec_df['query_prediction_words'][word2vec_df.query_prediction_number > 0] for y in x]\n",
    "model = word2vec.Word2Vec(word2vec_list, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "word_wv = model.wv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_w2v_array(word_list, word_wv, num_features):\n",
    "    word_vectors = np.zeros((len(word_list), num_features))\n",
    "    for i in range(len(word_list)):\n",
    "        if str(word_list[i]) in word_wv.vocab.keys():\n",
    "            word_vectors[i][:] = word_wv[str(word_list[i])]\n",
    "    mean_array = np.mean(word_vectors, axis=0)\n",
    "    return mean_array\n",
    "\n",
    "train_df['title_jieba_array'] = train_df['title_jieba_words'].map(lambda x : get_w2v_array(x, word_wv, num_features))\n",
    "valid_df['title_jieba_array'] = valid_df['title_jieba_words'].map(lambda x : get_w2v_array(x, word_wv, num_features))\n",
    "\n",
    "train_df['prefix_jieba_array'] = train_df['prefix_jieba_words'].map(lambda x : get_w2v_array(x, word_wv, num_features))\n",
    "valid_df['prefix_jieba_array'] = valid_df['prefix_jieba_words'].map(lambda x : get_w2v_array(x, word_wv, num_features))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot : finish!!!\n",
      "norm : finish!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:67: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:26: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_maximum(a, axis, None, out, keepdims)\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:29: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_minimum(a, axis, None, out, keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine : finish!!!\n"
     ]
    }
   ],
   "source": [
    "def get_title_prefix_similarity(df, f_similarity):\n",
    "    title_array = df['title_jieba_array']\n",
    "    prefix_array = df['prefix_jieba_array']\n",
    "    similarity = 0\n",
    "    if f_similarity == 'dot':\n",
    "        similarity = np.dot(title_array, prefix_array)\n",
    "    elif f_similarity == 'norm':\n",
    "        similarity = np.linalg.norm(title_array - prefix_array)\n",
    "    else:\n",
    "        similarity = np.dot(title_array,prefix_array) / (np.linalg.norm(title_array) * np.linalg.norm(prefix_array))\n",
    "    return similarity\n",
    "\n",
    "# def get_title_query_similarity(df, f_similarity, word_wv, num_features):\n",
    "#     title_array = df['title_jieba_array']\n",
    "#     query_prediction_words = df['query_prediction_words']\n",
    "#     query_prediction_keys = df['query_prediction_keys']\n",
    "#     query_prediction_dict = df['query_prediction_dict']\n",
    "#     if len(query_prediction_keys) <= 0:\n",
    "#         return np.nan\n",
    "#     similarity = 0\n",
    "#     if f_similarity == 'dot':\n",
    "#         i = 0\n",
    "#         for key in query_prediction_keys:\n",
    "#             key_array = get_w2v_array(query_prediction_words[i], word_wv, num_features)\n",
    "#             similarity = similarity + np.dot(title_array, key_array) * float(query_prediction_dict[key])\n",
    "#             i = i + 1\n",
    "#     elif f_similarity == 'norm':\n",
    "#         i = 0\n",
    "#         for key in query_prediction_keys:\n",
    "#             key_array = get_w2v_array(query_prediction_words[i], word_wv, num_features)\n",
    "#             similarity = similarity + np.linalg.norm(title_array - key_array) * float(query_prediction_dict[key])\n",
    "#             i = i + 1\n",
    "#     else:\n",
    "#         i = 0\n",
    "#         for key in query_prediction_keys:\n",
    "#             key_array = get_w2v_array(query_prediction_words[i], word_wv, num_features)\n",
    "#             similarity = similarity + (np.dot(title_array, key_array) / (np.linalg.norm(title_array) * np.linalg.norm(key_array))) * float(query_prediction_dict[key])\n",
    "#             i = i + 1\n",
    "#     return similarity\n",
    "\n",
    "def get_title_query_similarity_list(df, f_similarity, word_wv, num_features):\n",
    "    title_array = df['title_jieba_array']\n",
    "    query_prediction_words = df['query_prediction_words']\n",
    "    query_prediction_keys = df['query_prediction_keys']\n",
    "    query_prediction_dict = df['query_prediction_dict']\n",
    "    similarity_list = list()\n",
    "    if len(query_prediction_keys) <= 0:\n",
    "        return similarity_list\n",
    "    if f_similarity == 'dot':\n",
    "        i = 0\n",
    "        for key in query_prediction_keys:\n",
    "            key_array = get_w2v_array(query_prediction_words[i], word_wv, num_features)\n",
    "            similarity = np.dot(title_array, key_array) * float(query_prediction_dict[key])\n",
    "            similarity_list.append(similarity)\n",
    "            i = i + 1\n",
    "    elif f_similarity == 'norm':\n",
    "        i = 0\n",
    "        for key in query_prediction_keys:\n",
    "            key_array = get_w2v_array(query_prediction_words[i], word_wv, num_features)\n",
    "            similarity = np.linalg.norm(title_array - key_array) * float(query_prediction_dict[key])\n",
    "            similarity_list.append(similarity)\n",
    "            i = i + 1\n",
    "    else:\n",
    "        i = 0\n",
    "        for key in query_prediction_keys:\n",
    "            key_array = get_w2v_array(query_prediction_words[i], word_wv, num_features)\n",
    "            similarity = (np.dot(title_array, key_array) / (np.linalg.norm(title_array) * np.linalg.norm(key_array))) * float(query_prediction_dict[key])\n",
    "            similarity_list.append(similarity)\n",
    "            i = i + 1\n",
    "    return similarity_list\n",
    "\n",
    "def get_similarity_feature(train_df, valid_df):\n",
    "    f_list = ['dot', 'norm', 'cosine']\n",
    "    for fun in f_list:\n",
    "        f_prefix_similarity = functools.partial(get_title_prefix_similarity, f_similarity=fun)\n",
    "        train_df['title_prefix_' + fun + '_similarity'] = train_df[['title_jieba_array', 'prefix_jieba_array']].apply(f_prefix_similarity, axis=1)\n",
    "        valid_df['title_prefix_' + fun + '_similarity'] = valid_df[['title_jieba_array', 'prefix_jieba_array']].apply(f_prefix_similarity, axis=1)\n",
    "#         f_query_similarity = functools.partial(get_title_query_similarity, f_similarity=fun, word_wv=word_wv, num_features=num_features)\n",
    "#         train_df['title_query_' + fun + '_similarity'] = train_df[['title_jieba_array', 'query_prediction_words', 'query_prediction_keys', 'query_prediction_dict']].apply(f_query_similarity, axis=1)\n",
    "#         valid_df['title_query_' + fun + '_similarity'] = valid_df[['title_jieba_array', 'query_prediction_words', 'query_prediction_keys', 'query_prediction_dict']].apply(f_query_similarity, axis=1)\n",
    "        f_query_similarity_list = functools.partial(get_title_query_similarity_list, f_similarity=fun, word_wv=word_wv, num_features=num_features)\n",
    "        train_df['title_query_' + fun + '_similarity_list'] = train_df[['title_jieba_array', 'query_prediction_words', 'query_prediction_keys', 'query_prediction_dict']].apply(f_query_similarity_list, axis=1)\n",
    "        valid_df['title_query_' + fun + '_similarity_list'] = valid_df[['title_jieba_array', 'query_prediction_words', 'query_prediction_keys', 'query_prediction_dict']].apply(f_query_similarity_list, axis=1)\n",
    "        train_df['title_query_' + fun + '_similarity'] = train_df['title_query_' + fun + '_similarity_list'].map(lambda x : np.nan if len(x)==0 else np.sum(x))\n",
    "        train_df['title_query_' + fun + '_similarity_max'] = train_df['title_query_' + fun + '_similarity_list'].map(lambda x : np.nan if len(x)==0 else np.max(x))\n",
    "        train_df['title_query_' + fun + '_similarity_min'] = train_df['title_query_' + fun + '_similarity_list'].map(lambda x : np.nan if len(x)==0 else np.min(x))\n",
    "        train_df['title_query_' + fun + '_similarity_mean'] = train_df['title_query_' + fun + '_similarity_list'].map(lambda x : np.nan if len(x)==0 else np.mean(x))\n",
    "        train_df['title_query_' + fun + '_similarity_std'] = train_df['title_query_' + fun + '_similarity_list'].map(lambda x : np.nan if len(x)==0 else np.std(x))\n",
    "        valid_df['title_query_' + fun + '_similarity'] = valid_df['title_query_' + fun + '_similarity_list'].map(lambda x : np.nan if len(x)==0 else np.sum(x))\n",
    "        valid_df['title_query_' + fun + '_similarity_max'] = valid_df['title_query_' + fun + '_similarity_list'].map(lambda x : np.nan if len(x)==0 else np.max(x))\n",
    "        valid_df['title_query_' + fun + '_similarity_min'] = valid_df['title_query_' + fun + '_similarity_list'].map(lambda x : np.nan if len(x)==0 else np.min(x))\n",
    "        valid_df['title_query_' + fun + '_similarity_mean'] = valid_df['title_query_' + fun + '_similarity_list'].map(lambda x : np.nan if len(x)==0 else np.mean(x))\n",
    "        valid_df['title_query_' + fun + '_similarity_std'] = valid_df['title_query_' + fun + '_similarity_list'].map(lambda x : np.nan if len(x)==0 else np.std(x))\n",
    "        print(fun + ' : finish!!!')\n",
    "    return train_df, valid_df\n",
    "\n",
    "train_df, valid_df = get_similarity_feature(train_df, valid_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['prefix' 'query_prediction' 'title' 'tag' 'label' 'index'\n",
      " 'query_prediction_dict' 'query_prediction_keys' 'query_prediction_values'\n",
      " 'query_prediction_number' 'query_prediction_max' 'query_prediction_min'\n",
      " 'query_prediction_mean' 'query_prediction_std' 'prefix_count'\n",
      " 'prefix_rate' 'prefix_click_number' 'title_count' 'title_rate'\n",
      " 'title_click_number' 'tag_count' 'tag_rate' 'tag_click_number'\n",
      " 'query_prediction_count' 'query_prediction_rate'\n",
      " 'query_prediction_click_number' 'prefix_title_count' 'prefix_title_rate'\n",
      " 'prefix_title_click_number' 'prefix_tag_count' 'prefix_tag_rate'\n",
      " 'prefix_tag_click_number' 'title_tag_count' 'title_tag_rate'\n",
      " 'title_tag_click_number' 'is_title_in_query' 'is_prefix_in_title'\n",
      " 'title_tag_types' 'prefix_tag_types' 'tag_title_types' 'tag_prefix_types'\n",
      " 'title_prefix_types' 'prefix_title_types' 'tag_query_prediction_types'\n",
      " 'title_query_prediction_types' 'prefix_len' 'title_len'\n",
      " 'query_prediction_key_len_max' 'query_prediction_key_len_min'\n",
      " 'query_prediction_key_len_mean' 'query_prediction_key_len_std'\n",
      " 'len_title-prefix' 'len_prefix/title' 'len_mean-title' 'len_mean/title'\n",
      " 'title_prefix_leven' 'title_prefix_leven_rate' 'title_query_leven_list'\n",
      " 'title_query_leven_sum' 'title_query_leven_max' 'title_query_leven_min'\n",
      " 'title_query_leven_mean' 'title_query_leven_std'\n",
      " 'query_prediction_key_sentences' 'query_prediction_key_max_sentences'\n",
      " 'query_prediction_key_jieba_words' 'query_prediction_key_max_jieba_words'\n",
      " 'query_prediction_words' 'title_jieba_words' 'prefix_jieba_words'\n",
      " 'q_t_word_match' 'q_t_common_words' 'q_t_total_unique_words'\n",
      " 'q_t_wc_diff' 'q_t_wc_ratio' 'q_t_wc_diff_unique' 'q_t_wc_ratio_unique'\n",
      " 'p_t_word_match' 'p_t_common_words' 'p_t_total_unique_words'\n",
      " 'p_t_wc_diff' 'p_t_wc_ratio' 'p_t_wc_diff_unique' 'p_t_wc_ratio_unique'\n",
      " 'p_q_word_match' 'p_q_common_words' 'p_q_total_unique_words'\n",
      " 'p_q_wc_diff' 'p_q_wc_ratio' 'p_q_wc_diff_unique' 'p_q_wc_ratio_unique'\n",
      " 'dir_title_jieba_array' 'dir_prefix_jieba_array'\n",
      " 'dir_query_max_jieba_array' 'dir_t_p_array' 'dir_t_maxQ_array'\n",
      " 'dir_t_p_array_0_fea' 'dir_t_maxQ_array0_fea' 'dir_t_p_array_1_fea'\n",
      " 'dir_t_maxQ_array1_fea' 'dir_t_p_array_2_fea' 'dir_t_maxQ_array2_fea'\n",
      " 'dir_t_p_array_3_fea' 'dir_t_maxQ_array3_fea' 'dir_t_p_array_4_fea'\n",
      " 'dir_t_maxQ_array4_fea' 'dir_t_p_array_5_fea' 'dir_t_maxQ_array5_fea'\n",
      " 'dir_t_p_array_6_fea' 'dir_t_maxQ_array6_fea' 'dir_t_p_array_7_fea'\n",
      " 'dir_t_maxQ_array7_fea' 'dir_t_p_array_8_fea' 'dir_t_maxQ_array8_fea'\n",
      " 'dir_t_p_array_9_fea' 'dir_t_maxQ_array9_fea' 'dir_t_p_array_10_fea'\n",
      " 'dir_t_maxQ_array10_fea' 'dir_t_p_array_11_fea' 'dir_t_maxQ_array11_fea'\n",
      " 'dir_t_p_array_12_fea' 'dir_t_maxQ_array12_fea' 'dir_t_p_array_13_fea'\n",
      " 'dir_t_maxQ_array13_fea' 'dir_t_p_array_14_fea' 'dir_t_maxQ_array14_fea'\n",
      " 'dir_t_p_array_15_fea' 'dir_t_maxQ_array15_fea' 'dir_t_p_array_16_fea'\n",
      " 'dir_t_maxQ_array16_fea' 'dir_t_p_array_17_fea' 'dir_t_maxQ_array17_fea'\n",
      " 'dir_t_p_array_18_fea' 'dir_t_maxQ_array18_fea' 'dir_t_p_array_19_fea'\n",
      " 'dir_t_maxQ_array19_fea' 'dir_t_p_array_20_fea' 'dir_t_maxQ_array20_fea'\n",
      " 'dir_t_p_array_21_fea' 'dir_t_maxQ_array21_fea' 'dir_t_p_array_22_fea'\n",
      " 'dir_t_maxQ_array22_fea' 'dir_t_p_array_23_fea' 'dir_t_maxQ_array23_fea'\n",
      " 'dir_t_p_array_24_fea' 'dir_t_maxQ_array24_fea' 'dir_t_p_array_25_fea'\n",
      " 'dir_t_maxQ_array25_fea' 'dir_t_p_array_26_fea' 'dir_t_maxQ_array26_fea'\n",
      " 'dir_t_p_array_27_fea' 'dir_t_maxQ_array27_fea' 'dir_t_p_array_28_fea'\n",
      " 'dir_t_maxQ_array28_fea' 'dir_t_p_array_29_fea' 'dir_t_maxQ_array29_fea'\n",
      " 'dir_t_array_0_fea' 'dir_p_array0_fea' 'dir_maxQ_array0_fea'\n",
      " 'dir_t_array_1_fea' 'dir_p_array1_fea' 'dir_maxQ_array1_fea'\n",
      " 'dir_t_array_2_fea' 'dir_p_array2_fea' 'dir_maxQ_array2_fea'\n",
      " 'dir_t_array_3_fea' 'dir_p_array3_fea' 'dir_maxQ_array3_fea'\n",
      " 'dir_t_array_4_fea' 'dir_p_array4_fea' 'dir_maxQ_array4_fea'\n",
      " 'dir_t_array_5_fea' 'dir_p_array5_fea' 'dir_maxQ_array5_fea'\n",
      " 'dir_t_array_6_fea' 'dir_p_array6_fea' 'dir_maxQ_array6_fea'\n",
      " 'dir_t_array_7_fea' 'dir_p_array7_fea' 'dir_maxQ_array7_fea'\n",
      " 'dir_t_array_8_fea' 'dir_p_array8_fea' 'dir_maxQ_array8_fea'\n",
      " 'dir_t_array_9_fea' 'dir_p_array9_fea' 'dir_maxQ_array9_fea'\n",
      " 'dir_t_array_10_fea' 'dir_p_array10_fea' 'dir_maxQ_array10_fea'\n",
      " 'dir_t_array_11_fea' 'dir_p_array11_fea' 'dir_maxQ_array11_fea'\n",
      " 'dir_t_array_12_fea' 'dir_p_array12_fea' 'dir_maxQ_array12_fea'\n",
      " 'dir_t_array_13_fea' 'dir_p_array13_fea' 'dir_maxQ_array13_fea'\n",
      " 'dir_t_array_14_fea' 'dir_p_array14_fea' 'dir_maxQ_array14_fea'\n",
      " 'dir_t_array_15_fea' 'dir_p_array15_fea' 'dir_maxQ_array15_fea'\n",
      " 'dir_t_array_16_fea' 'dir_p_array16_fea' 'dir_maxQ_array16_fea'\n",
      " 'dir_t_array_17_fea' 'dir_p_array17_fea' 'dir_maxQ_array17_fea'\n",
      " 'dir_t_array_18_fea' 'dir_p_array18_fea' 'dir_maxQ_array18_fea'\n",
      " 'dir_t_array_19_fea' 'dir_p_array19_fea' 'dir_maxQ_array19_fea'\n",
      " 'dir_t_array_20_fea' 'dir_p_array20_fea' 'dir_maxQ_array20_fea'\n",
      " 'dir_t_array_21_fea' 'dir_p_array21_fea' 'dir_maxQ_array21_fea'\n",
      " 'dir_t_array_22_fea' 'dir_p_array22_fea' 'dir_maxQ_array22_fea'\n",
      " 'dir_t_array_23_fea' 'dir_p_array23_fea' 'dir_maxQ_array23_fea'\n",
      " 'dir_t_array_24_fea' 'dir_p_array24_fea' 'dir_maxQ_array24_fea'\n",
      " 'dir_t_array_25_fea' 'dir_p_array25_fea' 'dir_maxQ_array25_fea'\n",
      " 'dir_t_array_26_fea' 'dir_p_array26_fea' 'dir_maxQ_array26_fea'\n",
      " 'dir_t_array_27_fea' 'dir_p_array27_fea' 'dir_maxQ_array27_fea'\n",
      " 'dir_t_array_28_fea' 'dir_p_array28_fea' 'dir_maxQ_array28_fea'\n",
      " 'dir_t_array_29_fea' 'dir_p_array29_fea' 'dir_maxQ_array29_fea'\n",
      " 'title_jieba_array' 'prefix_jieba_array' 'title_prefix_dot_similarity'\n",
      " 'title_query_dot_similarity_list' 'title_query_dot_similarity'\n",
      " 'title_query_dot_similarity_max' 'title_query_dot_similarity_min'\n",
      " 'title_query_dot_similarity_mean' 'title_query_dot_similarity_std'\n",
      " 'title_prefix_norm_similarity' 'title_query_norm_similarity_list'\n",
      " 'title_query_norm_similarity' 'title_query_norm_similarity_max'\n",
      " 'title_query_norm_similarity_min' 'title_query_norm_similarity_mean'\n",
      " 'title_query_norm_similarity_std' 'title_prefix_cosine_similarity'\n",
      " 'title_query_cosine_similarity_list' 'title_query_cosine_similarity'\n",
      " 'title_query_cosine_similarity_max' 'title_query_cosine_similarity_min'\n",
      " 'title_query_cosine_similarity_mean' 'title_query_cosine_similarity_std']\n"
     ]
    }
   ],
   "source": [
    "print(train_df.columns.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fea = [\n",
    "    'query_prediction_number', 'query_prediction_max', 'query_prediction_min', 'query_prediction_mean', 'query_prediction_std',\n",
    "       'prefix_count', 'prefix_rate',\n",
    " 'title_count', 'title_rate', 'tag_count', 'tag_rate',\n",
    " 'query_prediction_count', 'query_prediction_rate', 'prefix_title_count',\n",
    " 'prefix_title_rate',  'prefix_tag_count', 'prefix_tag_rate',\n",
    " 'title_tag_count', 'title_tag_rate',\n",
    "    'prefix_click_number', 'title_click_number', 'query_prediction_click_number', 'prefix_tag_click_number', \n",
    "    'prefix_title_click_number', 'title_tag_click_number',\n",
    "    'is_title_in_query', 'is_prefix_in_title', \n",
    "    'title_tag_types', 'prefix_tag_types', 'tag_title_types', 'tag_prefix_types',\n",
    " 'title_prefix_types', 'prefix_title_types', 'tag_query_prediction_types', 'title_query_prediction_types',\n",
    "      'prefix_len', 'title_len',\n",
    " 'query_prediction_key_len_max', 'query_prediction_key_len_min',\n",
    " 'query_prediction_key_len_mean', 'query_prediction_key_len_std',\n",
    " 'len_title-prefix', 'len_prefix/title', 'len_mean-title', 'len_mean/title',\n",
    "    'q_t_word_match', 'q_t_common_words',\n",
    " 'q_t_total_unique_words', 'q_t_wc_diff', 'q_t_wc_ratio',\n",
    " 'q_t_wc_diff_unique', 'q_t_wc_ratio_unique',\n",
    " 'p_t_word_match', 'p_t_common_words',\n",
    " 'p_t_total_unique_words', 'p_t_wc_diff', 'p_t_wc_ratio',\n",
    " 'p_t_wc_diff_unique', 'p_t_wc_ratio_unique',\n",
    " 'p_q_word_match', 'p_q_common_words',\n",
    " 'p_q_total_unique_words', 'p_q_wc_diff', 'p_q_wc_ratio',\n",
    " 'p_q_wc_diff_unique', 'p_q_wc_ratio_unique',\n",
    "    'title_prefix_dot_similarity',\n",
    " 'title_query_dot_similarity', 'title_prefix_norm_similarity',\n",
    " 'title_query_norm_similarity', 'title_prefix_cosine_similarity',\n",
    " 'title_query_cosine_similarity',\n",
    "    'title_query_dot_similarity_max', 'title_query_dot_similarity_min',\n",
    " 'title_query_dot_similarity_mean', 'title_query_dot_similarity_std',\n",
    "    'title_query_norm_similarity_min', 'title_query_norm_similarity_mean',\n",
    " 'title_query_norm_similarity_std',\n",
    "    'title_query_cosine_similarity_max', 'title_query_cosine_similarity_min',\n",
    " 'title_query_cosine_similarity_mean', 'title_query_cosine_similarity_std',\n",
    "    'title_prefix_leven', 'title_prefix_leven_rate',\n",
    " 'title_query_leven_sum', 'title_query_leven_max', 'title_query_leven_min',\n",
    " 'title_query_leven_mean', 'title_query_leven_std',\n",
    "    \n",
    "#      'dir_t_array_0_fea', 'dir_p_array0_fea', 'dir_maxQ_array0_fea',\n",
    "#  'dir_t_array_1_fea', 'dir_p_array1_fea', 'dir_maxQ_array1_fea',\n",
    "#  'dir_t_array_2_fea', 'dir_p_array2_fea', 'dir_maxQ_array2_fea',\n",
    "#  'dir_t_array_3_fea', 'dir_p_array3_fea', 'dir_maxQ_array3_fea',\n",
    "#  'dir_t_array_4_fea', 'dir_p_array4_fea', 'dir_maxQ_array4_fea',\n",
    "#  'dir_t_array_5_fea', 'dir_p_array5_fea', 'dir_maxQ_array5_fea',\n",
    "#  'dir_t_array_6_fea', 'dir_p_array6_fea', 'dir_maxQ_array6_fea',\n",
    "#  'dir_t_array_7_fea', 'dir_p_array7_fea', 'dir_maxQ_array7_fea',\n",
    "#  'dir_t_array_8_fea', 'dir_p_array8_fea', 'dir_maxQ_array8_fea',\n",
    "#  'dir_t_array_9_fea', 'dir_p_array9_fea', 'dir_maxQ_array9_fea',\n",
    "#  'dir_t_array_10_fea', 'dir_p_array10_fea', 'dir_maxQ_array10_fea',\n",
    "#  'dir_t_array_11_fea', 'dir_p_array11_fea', 'dir_maxQ_array11_fea',\n",
    "#  'dir_t_array_12_fea', 'dir_p_array12_fea', 'dir_maxQ_array12_fea',\n",
    "#  'dir_t_array_13_fea', 'dir_p_array13_fea', 'dir_maxQ_array13_fea',\n",
    "#  'dir_t_array_14_fea', 'dir_p_array14_fea', 'dir_maxQ_array14_fea',\n",
    "#  'dir_t_array_15_fea', 'dir_p_array15_fea', 'dir_maxQ_array15_fea',\n",
    "#  'dir_t_array_16_fea', 'dir_p_array16_fea', 'dir_maxQ_array16_fea',\n",
    "#  'dir_t_array_17_fea', 'dir_p_array17_fea', 'dir_maxQ_array17_fea',\n",
    "#  'dir_t_array_18_fea', 'dir_p_array18_fea', 'dir_maxQ_array18_fea',\n",
    "#  'dir_t_array_19_fea', 'dir_p_array19_fea', 'dir_maxQ_array19_fea',\n",
    "#  'dir_t_array_20_fea', 'dir_p_array20_fea', 'dir_maxQ_array20_fea',\n",
    "#  'dir_t_array_21_fea', 'dir_p_array21_fea', 'dir_maxQ_array21_fea',\n",
    "#  'dir_t_array_22_fea', 'dir_p_array22_fea', 'dir_maxQ_array22_fea',\n",
    "#  'dir_t_array_23_fea', 'dir_p_array23_fea', 'dir_maxQ_array23_fea',\n",
    "#  'dir_t_array_24_fea', 'dir_p_array24_fea', 'dir_maxQ_array24_fea',\n",
    "#  'dir_t_array_25_fea', 'dir_p_array25_fea', 'dir_maxQ_array25_fea',\n",
    "#  'dir_t_array_26_fea', 'dir_p_array26_fea', 'dir_maxQ_array26_fea',\n",
    "#  'dir_t_array_27_fea', 'dir_p_array27_fea', 'dir_maxQ_array27_fea',\n",
    "#  'dir_t_array_28_fea', 'dir_p_array28_fea', 'dir_maxQ_array28_fea',\n",
    "#  'dir_t_array_29_fea', 'dir_p_array29_fea', 'dir_maxQ_array29_fea',\n",
    "    \n",
    "#     'dir_t_p_array_0_fea', 'dir_t_maxQ_array0_fea', 'dir_t_p_array_1_fea',\n",
    "#  'dir_t_maxQ_array1_fea', 'dir_t_p_array_2_fea', 'dir_t_maxQ_array2_fea',\n",
    "#  'dir_t_p_array_3_fea', 'dir_t_maxQ_array3_fea', 'dir_t_p_array_4_fea',\n",
    "#  'dir_t_maxQ_array4_fea', 'dir_t_p_array_5_fea', 'dir_t_maxQ_array5_fea',\n",
    "#  'dir_t_p_array_6_fea', 'dir_t_maxQ_array6_fea', 'dir_t_p_array_7_fea',\n",
    "#  'dir_t_maxQ_array7_fea', 'dir_t_p_array_8_fea', 'dir_t_maxQ_array8_fea',\n",
    "#  'dir_t_p_array_9_fea', 'dir_t_maxQ_array9_fea', 'dir_t_p_array_10_fea',\n",
    "#  'dir_t_maxQ_array10_fea', 'dir_t_p_array_11_fea', 'dir_t_maxQ_array11_fea',\n",
    "#  'dir_t_p_array_12_fea', 'dir_t_maxQ_array12_fea', 'dir_t_p_array_13_fea',\n",
    "#  'dir_t_maxQ_array13_fea', 'dir_t_p_array_14_fea', 'dir_t_maxQ_array14_fea',\n",
    "#  'dir_t_p_array_15_fea', 'dir_t_maxQ_array15_fea', 'dir_t_p_array_16_fea',\n",
    "#  'dir_t_maxQ_array16_fea', 'dir_t_p_array_17_fea', 'dir_t_maxQ_array17_fea',\n",
    "#  'dir_t_p_array_18_fea', 'dir_t_maxQ_array18_fea', 'dir_t_p_array_19_fea',\n",
    "#  'dir_t_maxQ_array19_fea', 'dir_t_p_array_20_fea', 'dir_t_maxQ_array20_fea',\n",
    "#  'dir_t_p_array_21_fea', 'dir_t_maxQ_array21_fea', 'dir_t_p_array_22_fea',\n",
    "#  'dir_t_maxQ_array22_fea', 'dir_t_p_array_23_fea', 'dir_t_maxQ_array23_fea',\n",
    "#  'dir_t_p_array_24_fea', 'dir_t_maxQ_array24_fea', 'dir_t_p_array_25_fea',\n",
    "#  'dir_t_maxQ_array25_fea', 'dir_t_p_array_26_fea', 'dir_t_maxQ_array26_fea',\n",
    "#  'dir_t_p_array_27_fea', 'dir_t_maxQ_array27_fea', 'dir_t_p_array_28_fea',\n",
    "#  'dir_t_maxQ_array28_fea', 'dir_t_p_array_29_fea', 'dir_t_maxQ_array29_fea',\n",
    "    \n",
    "      ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's auc: 0.870449\tvalid_1's auc: 0.860477\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[2]\tvalid_0's auc: 0.871108\tvalid_1's auc: 0.864553\n",
      "[3]\tvalid_0's auc: 0.87155\tvalid_1's auc: 0.865492\n",
      "[4]\tvalid_0's auc: 0.871637\tvalid_1's auc: 0.865038\n",
      "[5]\tvalid_0's auc: 0.871715\tvalid_1's auc: 0.865244\n",
      "[6]\tvalid_0's auc: 0.871844\tvalid_1's auc: 0.865219\n",
      "[7]\tvalid_0's auc: 0.871886\tvalid_1's auc: 0.864944\n",
      "[8]\tvalid_0's auc: 0.872025\tvalid_1's auc: 0.865179\n",
      "[9]\tvalid_0's auc: 0.872161\tvalid_1's auc: 0.865336\n",
      "[10]\tvalid_0's auc: 0.872398\tvalid_1's auc: 0.865709\n",
      "[11]\tvalid_0's auc: 0.872446\tvalid_1's auc: 0.865949\n",
      "[12]\tvalid_0's auc: 0.872471\tvalid_1's auc: 0.865832\n",
      "[13]\tvalid_0's auc: 0.872504\tvalid_1's auc: 0.865984\n",
      "[14]\tvalid_0's auc: 0.872601\tvalid_1's auc: 0.866228\n",
      "[15]\tvalid_0's auc: 0.872665\tvalid_1's auc: 0.866172\n",
      "[16]\tvalid_0's auc: 0.872667\tvalid_1's auc: 0.866035\n",
      "[17]\tvalid_0's auc: 0.872768\tvalid_1's auc: 0.866252\n",
      "[18]\tvalid_0's auc: 0.872825\tvalid_1's auc: 0.86639\n",
      "[19]\tvalid_0's auc: 0.87287\tvalid_1's auc: 0.866356\n",
      "[20]\tvalid_0's auc: 0.872936\tvalid_1's auc: 0.8663\n",
      "[21]\tvalid_0's auc: 0.872978\tvalid_1's auc: 0.866366\n",
      "[22]\tvalid_0's auc: 0.87303\tvalid_1's auc: 0.866494\n",
      "[23]\tvalid_0's auc: 0.873081\tvalid_1's auc: 0.866591\n",
      "[24]\tvalid_0's auc: 0.873132\tvalid_1's auc: 0.86685\n",
      "[25]\tvalid_0's auc: 0.873176\tvalid_1's auc: 0.866943\n",
      "[26]\tvalid_0's auc: 0.873217\tvalid_1's auc: 0.867055\n",
      "[27]\tvalid_0's auc: 0.873249\tvalid_1's auc: 0.867201\n",
      "[28]\tvalid_0's auc: 0.873279\tvalid_1's auc: 0.867367\n",
      "[29]\tvalid_0's auc: 0.873318\tvalid_1's auc: 0.867415\n",
      "[30]\tvalid_0's auc: 0.873353\tvalid_1's auc: 0.867444\n",
      "[31]\tvalid_0's auc: 0.873392\tvalid_1's auc: 0.867507\n",
      "[32]\tvalid_0's auc: 0.8734\tvalid_1's auc: 0.867511\n",
      "[33]\tvalid_0's auc: 0.873426\tvalid_1's auc: 0.867523\n",
      "[34]\tvalid_0's auc: 0.873506\tvalid_1's auc: 0.867727\n",
      "[35]\tvalid_0's auc: 0.873541\tvalid_1's auc: 0.867827\n",
      "[36]\tvalid_0's auc: 0.873569\tvalid_1's auc: 0.867921\n",
      "[37]\tvalid_0's auc: 0.873609\tvalid_1's auc: 0.86801\n",
      "[38]\tvalid_0's auc: 0.873639\tvalid_1's auc: 0.868113\n",
      "[39]\tvalid_0's auc: 0.873677\tvalid_1's auc: 0.86821\n",
      "[40]\tvalid_0's auc: 0.873756\tvalid_1's auc: 0.868302\n",
      "[41]\tvalid_0's auc: 0.873811\tvalid_1's auc: 0.868426\n",
      "[42]\tvalid_0's auc: 0.873853\tvalid_1's auc: 0.868467\n",
      "[43]\tvalid_0's auc: 0.873883\tvalid_1's auc: 0.868537\n",
      "[44]\tvalid_0's auc: 0.873917\tvalid_1's auc: 0.868579\n",
      "[45]\tvalid_0's auc: 0.873938\tvalid_1's auc: 0.868658\n",
      "[46]\tvalid_0's auc: 0.873955\tvalid_1's auc: 0.868728\n",
      "[47]\tvalid_0's auc: 0.873989\tvalid_1's auc: 0.868775\n",
      "[48]\tvalid_0's auc: 0.874022\tvalid_1's auc: 0.868857\n",
      "[49]\tvalid_0's auc: 0.874048\tvalid_1's auc: 0.868874\n",
      "[50]\tvalid_0's auc: 0.874096\tvalid_1's auc: 0.868937\n",
      "[51]\tvalid_0's auc: 0.874119\tvalid_1's auc: 0.868979\n",
      "[52]\tvalid_0's auc: 0.874155\tvalid_1's auc: 0.869037\n",
      "[53]\tvalid_0's auc: 0.874173\tvalid_1's auc: 0.869035\n",
      "[54]\tvalid_0's auc: 0.874268\tvalid_1's auc: 0.86915\n",
      "[55]\tvalid_0's auc: 0.874301\tvalid_1's auc: 0.869174\n",
      "[56]\tvalid_0's auc: 0.87433\tvalid_1's auc: 0.8692\n",
      "[57]\tvalid_0's auc: 0.874394\tvalid_1's auc: 0.869291\n",
      "[58]\tvalid_0's auc: 0.874409\tvalid_1's auc: 0.869336\n",
      "[59]\tvalid_0's auc: 0.874473\tvalid_1's auc: 0.869406\n",
      "[60]\tvalid_0's auc: 0.874489\tvalid_1's auc: 0.869428\n",
      "[61]\tvalid_0's auc: 0.87454\tvalid_1's auc: 0.869491\n",
      "[62]\tvalid_0's auc: 0.874563\tvalid_1's auc: 0.869421\n",
      "[63]\tvalid_0's auc: 0.8746\tvalid_1's auc: 0.869475\n",
      "[64]\tvalid_0's auc: 0.874646\tvalid_1's auc: 0.869476\n",
      "[65]\tvalid_0's auc: 0.874694\tvalid_1's auc: 0.869512\n",
      "[66]\tvalid_0's auc: 0.874732\tvalid_1's auc: 0.869566\n",
      "[67]\tvalid_0's auc: 0.874777\tvalid_1's auc: 0.869608\n",
      "[68]\tvalid_0's auc: 0.87481\tvalid_1's auc: 0.869647\n",
      "[69]\tvalid_0's auc: 0.874847\tvalid_1's auc: 0.869702\n",
      "[70]\tvalid_0's auc: 0.874884\tvalid_1's auc: 0.869749\n",
      "[71]\tvalid_0's auc: 0.874912\tvalid_1's auc: 0.869756\n",
      "[72]\tvalid_0's auc: 0.874946\tvalid_1's auc: 0.869769\n",
      "[73]\tvalid_0's auc: 0.874973\tvalid_1's auc: 0.869845\n",
      "[74]\tvalid_0's auc: 0.875005\tvalid_1's auc: 0.869856\n",
      "[75]\tvalid_0's auc: 0.875023\tvalid_1's auc: 0.8699\n",
      "[76]\tvalid_0's auc: 0.875052\tvalid_1's auc: 0.869887\n",
      "[77]\tvalid_0's auc: 0.875079\tvalid_1's auc: 0.86991\n",
      "[78]\tvalid_0's auc: 0.875105\tvalid_1's auc: 0.869947\n",
      "[79]\tvalid_0's auc: 0.875128\tvalid_1's auc: 0.869981\n",
      "[80]\tvalid_0's auc: 0.875158\tvalid_1's auc: 0.869996\n",
      "[81]\tvalid_0's auc: 0.875183\tvalid_1's auc: 0.870014\n",
      "[82]\tvalid_0's auc: 0.875216\tvalid_1's auc: 0.870005\n",
      "[83]\tvalid_0's auc: 0.875242\tvalid_1's auc: 0.870003\n",
      "[84]\tvalid_0's auc: 0.875268\tvalid_1's auc: 0.870039\n",
      "[85]\tvalid_0's auc: 0.87529\tvalid_1's auc: 0.870059\n",
      "[86]\tvalid_0's auc: 0.875314\tvalid_1's auc: 0.870074\n",
      "[87]\tvalid_0's auc: 0.875343\tvalid_1's auc: 0.870094\n",
      "[88]\tvalid_0's auc: 0.875365\tvalid_1's auc: 0.870138\n",
      "[89]\tvalid_0's auc: 0.875394\tvalid_1's auc: 0.870125\n",
      "[90]\tvalid_0's auc: 0.875417\tvalid_1's auc: 0.870146\n",
      "[91]\tvalid_0's auc: 0.875439\tvalid_1's auc: 0.870164\n",
      "[92]\tvalid_0's auc: 0.875467\tvalid_1's auc: 0.870159\n",
      "[93]\tvalid_0's auc: 0.875489\tvalid_1's auc: 0.870159\n",
      "[94]\tvalid_0's auc: 0.875513\tvalid_1's auc: 0.870179\n",
      "[95]\tvalid_0's auc: 0.875531\tvalid_1's auc: 0.870178\n",
      "[96]\tvalid_0's auc: 0.875552\tvalid_1's auc: 0.870188\n",
      "[97]\tvalid_0's auc: 0.875576\tvalid_1's auc: 0.870197\n",
      "[98]\tvalid_0's auc: 0.875595\tvalid_1's auc: 0.870229\n",
      "[99]\tvalid_0's auc: 0.875618\tvalid_1's auc: 0.870244\n",
      "[100]\tvalid_0's auc: 0.875637\tvalid_1's auc: 0.870263\n",
      "[101]\tvalid_0's auc: 0.875661\tvalid_1's auc: 0.870262\n",
      "[102]\tvalid_0's auc: 0.875679\tvalid_1's auc: 0.870261\n",
      "[103]\tvalid_0's auc: 0.875698\tvalid_1's auc: 0.870281\n",
      "[104]\tvalid_0's auc: 0.875719\tvalid_1's auc: 0.870294\n",
      "[105]\tvalid_0's auc: 0.875752\tvalid_1's auc: 0.870335\n",
      "[106]\tvalid_0's auc: 0.875773\tvalid_1's auc: 0.870352\n",
      "[107]\tvalid_0's auc: 0.875796\tvalid_1's auc: 0.870386\n",
      "[108]\tvalid_0's auc: 0.875816\tvalid_1's auc: 0.870412\n",
      "[109]\tvalid_0's auc: 0.875835\tvalid_1's auc: 0.870439\n",
      "[110]\tvalid_0's auc: 0.875858\tvalid_1's auc: 0.870446\n",
      "[111]\tvalid_0's auc: 0.87588\tvalid_1's auc: 0.87045\n",
      "[112]\tvalid_0's auc: 0.875901\tvalid_1's auc: 0.870448\n",
      "[113]\tvalid_0's auc: 0.875923\tvalid_1's auc: 0.870465\n",
      "[114]\tvalid_0's auc: 0.875945\tvalid_1's auc: 0.870474\n",
      "[115]\tvalid_0's auc: 0.875967\tvalid_1's auc: 0.870507\n",
      "[116]\tvalid_0's auc: 0.875984\tvalid_1's auc: 0.870521\n",
      "[117]\tvalid_0's auc: 0.876019\tvalid_1's auc: 0.870552\n",
      "[118]\tvalid_0's auc: 0.87604\tvalid_1's auc: 0.870563\n",
      "[119]\tvalid_0's auc: 0.876061\tvalid_1's auc: 0.870591\n",
      "[120]\tvalid_0's auc: 0.876081\tvalid_1's auc: 0.870602\n",
      "[121]\tvalid_0's auc: 0.876106\tvalid_1's auc: 0.870613\n",
      "[122]\tvalid_0's auc: 0.876131\tvalid_1's auc: 0.870631\n",
      "[123]\tvalid_0's auc: 0.876158\tvalid_1's auc: 0.870657\n",
      "[124]\tvalid_0's auc: 0.876179\tvalid_1's auc: 0.870666\n",
      "[125]\tvalid_0's auc: 0.876202\tvalid_1's auc: 0.870683\n",
      "[126]\tvalid_0's auc: 0.876227\tvalid_1's auc: 0.870727\n",
      "[127]\tvalid_0's auc: 0.876256\tvalid_1's auc: 0.870759\n",
      "[128]\tvalid_0's auc: 0.87628\tvalid_1's auc: 0.870769\n",
      "[129]\tvalid_0's auc: 0.876306\tvalid_1's auc: 0.870792\n",
      "[130]\tvalid_0's auc: 0.87634\tvalid_1's auc: 0.870824\n",
      "[131]\tvalid_0's auc: 0.876374\tvalid_1's auc: 0.870875\n",
      "[132]\tvalid_0's auc: 0.876394\tvalid_1's auc: 0.870907\n",
      "[133]\tvalid_0's auc: 0.876415\tvalid_1's auc: 0.870916\n",
      "[134]\tvalid_0's auc: 0.876433\tvalid_1's auc: 0.87092\n",
      "[135]\tvalid_0's auc: 0.876453\tvalid_1's auc: 0.870932\n",
      "[136]\tvalid_0's auc: 0.876474\tvalid_1's auc: 0.870959\n",
      "[137]\tvalid_0's auc: 0.876495\tvalid_1's auc: 0.870988\n",
      "[138]\tvalid_0's auc: 0.876515\tvalid_1's auc: 0.871026\n",
      "[139]\tvalid_0's auc: 0.876535\tvalid_1's auc: 0.871063\n",
      "[140]\tvalid_0's auc: 0.876563\tvalid_1's auc: 0.871085\n",
      "[141]\tvalid_0's auc: 0.876581\tvalid_1's auc: 0.87111\n",
      "[142]\tvalid_0's auc: 0.876609\tvalid_1's auc: 0.87114\n",
      "[143]\tvalid_0's auc: 0.876636\tvalid_1's auc: 0.871165\n",
      "[144]\tvalid_0's auc: 0.876652\tvalid_1's auc: 0.871179\n",
      "[145]\tvalid_0's auc: 0.876671\tvalid_1's auc: 0.871197\n",
      "[146]\tvalid_0's auc: 0.876691\tvalid_1's auc: 0.871195\n",
      "[147]\tvalid_0's auc: 0.876714\tvalid_1's auc: 0.871219\n",
      "[148]\tvalid_0's auc: 0.876733\tvalid_1's auc: 0.871248\n",
      "[149]\tvalid_0's auc: 0.876761\tvalid_1's auc: 0.871278\n",
      "[150]\tvalid_0's auc: 0.87678\tvalid_1's auc: 0.871316\n",
      "[151]\tvalid_0's auc: 0.876799\tvalid_1's auc: 0.871335\n",
      "[152]\tvalid_0's auc: 0.876819\tvalid_1's auc: 0.871364\n",
      "[153]\tvalid_0's auc: 0.876837\tvalid_1's auc: 0.871376\n",
      "[154]\tvalid_0's auc: 0.876853\tvalid_1's auc: 0.871408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[155]\tvalid_0's auc: 0.876875\tvalid_1's auc: 0.871427\n",
      "[156]\tvalid_0's auc: 0.876892\tvalid_1's auc: 0.871435\n",
      "[157]\tvalid_0's auc: 0.876917\tvalid_1's auc: 0.871461\n",
      "[158]\tvalid_0's auc: 0.87694\tvalid_1's auc: 0.8715\n",
      "[159]\tvalid_0's auc: 0.876961\tvalid_1's auc: 0.871521\n",
      "[160]\tvalid_0's auc: 0.876979\tvalid_1's auc: 0.871537\n",
      "[161]\tvalid_0's auc: 0.877002\tvalid_1's auc: 0.871567\n",
      "[162]\tvalid_0's auc: 0.877019\tvalid_1's auc: 0.87159\n",
      "[163]\tvalid_0's auc: 0.877045\tvalid_1's auc: 0.871618\n",
      "[164]\tvalid_0's auc: 0.87706\tvalid_1's auc: 0.871632\n",
      "[165]\tvalid_0's auc: 0.87708\tvalid_1's auc: 0.871669\n",
      "[166]\tvalid_0's auc: 0.877097\tvalid_1's auc: 0.871687\n",
      "[167]\tvalid_0's auc: 0.877115\tvalid_1's auc: 0.871701\n",
      "[168]\tvalid_0's auc: 0.877137\tvalid_1's auc: 0.871739\n",
      "[169]\tvalid_0's auc: 0.877153\tvalid_1's auc: 0.87174\n",
      "[170]\tvalid_0's auc: 0.877171\tvalid_1's auc: 0.871766\n",
      "[171]\tvalid_0's auc: 0.877192\tvalid_1's auc: 0.871794\n",
      "[172]\tvalid_0's auc: 0.877208\tvalid_1's auc: 0.871805\n",
      "[173]\tvalid_0's auc: 0.877231\tvalid_1's auc: 0.871829\n",
      "[174]\tvalid_0's auc: 0.877251\tvalid_1's auc: 0.871844\n",
      "[175]\tvalid_0's auc: 0.877271\tvalid_1's auc: 0.871867\n",
      "[176]\tvalid_0's auc: 0.877286\tvalid_1's auc: 0.871891\n",
      "[177]\tvalid_0's auc: 0.877305\tvalid_1's auc: 0.871902\n",
      "[178]\tvalid_0's auc: 0.87732\tvalid_1's auc: 0.871908\n",
      "[179]\tvalid_0's auc: 0.877339\tvalid_1's auc: 0.871928\n",
      "[180]\tvalid_0's auc: 0.87736\tvalid_1's auc: 0.871959\n",
      "[181]\tvalid_0's auc: 0.877375\tvalid_1's auc: 0.871981\n",
      "[182]\tvalid_0's auc: 0.87739\tvalid_1's auc: 0.872009\n",
      "[183]\tvalid_0's auc: 0.877408\tvalid_1's auc: 0.872013\n",
      "[184]\tvalid_0's auc: 0.877428\tvalid_1's auc: 0.872038\n",
      "[185]\tvalid_0's auc: 0.877447\tvalid_1's auc: 0.872056\n",
      "[186]\tvalid_0's auc: 0.877469\tvalid_1's auc: 0.872076\n",
      "[187]\tvalid_0's auc: 0.877487\tvalid_1's auc: 0.872096\n",
      "[188]\tvalid_0's auc: 0.877507\tvalid_1's auc: 0.872116\n",
      "[189]\tvalid_0's auc: 0.877522\tvalid_1's auc: 0.872134\n",
      "[190]\tvalid_0's auc: 0.877542\tvalid_1's auc: 0.872155\n",
      "[191]\tvalid_0's auc: 0.877562\tvalid_1's auc: 0.872169\n",
      "[192]\tvalid_0's auc: 0.877582\tvalid_1's auc: 0.872192\n",
      "[193]\tvalid_0's auc: 0.877594\tvalid_1's auc: 0.872199\n",
      "[194]\tvalid_0's auc: 0.877612\tvalid_1's auc: 0.872212\n",
      "[195]\tvalid_0's auc: 0.877634\tvalid_1's auc: 0.872239\n",
      "[196]\tvalid_0's auc: 0.877653\tvalid_1's auc: 0.872257\n",
      "[197]\tvalid_0's auc: 0.877672\tvalid_1's auc: 0.872279\n",
      "[198]\tvalid_0's auc: 0.877689\tvalid_1's auc: 0.872291\n",
      "[199]\tvalid_0's auc: 0.877706\tvalid_1's auc: 0.872315\n",
      "[200]\tvalid_0's auc: 0.877723\tvalid_1's auc: 0.872336\n",
      "[201]\tvalid_0's auc: 0.877742\tvalid_1's auc: 0.872366\n",
      "[202]\tvalid_0's auc: 0.877757\tvalid_1's auc: 0.872388\n",
      "[203]\tvalid_0's auc: 0.877776\tvalid_1's auc: 0.872408\n",
      "[204]\tvalid_0's auc: 0.877792\tvalid_1's auc: 0.872417\n",
      "[205]\tvalid_0's auc: 0.877809\tvalid_1's auc: 0.872414\n",
      "[206]\tvalid_0's auc: 0.877826\tvalid_1's auc: 0.872442\n",
      "[207]\tvalid_0's auc: 0.877845\tvalid_1's auc: 0.872464\n",
      "[208]\tvalid_0's auc: 0.87786\tvalid_1's auc: 0.872478\n",
      "[209]\tvalid_0's auc: 0.877878\tvalid_1's auc: 0.872494\n",
      "[210]\tvalid_0's auc: 0.877897\tvalid_1's auc: 0.872509\n",
      "[211]\tvalid_0's auc: 0.877914\tvalid_1's auc: 0.872521\n",
      "[212]\tvalid_0's auc: 0.877931\tvalid_1's auc: 0.87253\n",
      "[213]\tvalid_0's auc: 0.877948\tvalid_1's auc: 0.872545\n",
      "[214]\tvalid_0's auc: 0.87796\tvalid_1's auc: 0.872544\n",
      "[215]\tvalid_0's auc: 0.877976\tvalid_1's auc: 0.872554\n",
      "[216]\tvalid_0's auc: 0.877994\tvalid_1's auc: 0.872569\n",
      "[217]\tvalid_0's auc: 0.87801\tvalid_1's auc: 0.872579\n",
      "[218]\tvalid_0's auc: 0.878025\tvalid_1's auc: 0.872592\n",
      "[219]\tvalid_0's auc: 0.878041\tvalid_1's auc: 0.872596\n",
      "[220]\tvalid_0's auc: 0.878056\tvalid_1's auc: 0.872601\n",
      "[221]\tvalid_0's auc: 0.878073\tvalid_1's auc: 0.872601\n",
      "[222]\tvalid_0's auc: 0.878091\tvalid_1's auc: 0.872626\n",
      "[223]\tvalid_0's auc: 0.878106\tvalid_1's auc: 0.872637\n",
      "[224]\tvalid_0's auc: 0.878123\tvalid_1's auc: 0.872644\n",
      "[225]\tvalid_0's auc: 0.878139\tvalid_1's auc: 0.872654\n",
      "[226]\tvalid_0's auc: 0.878157\tvalid_1's auc: 0.872667\n",
      "[227]\tvalid_0's auc: 0.878173\tvalid_1's auc: 0.872676\n",
      "[228]\tvalid_0's auc: 0.87819\tvalid_1's auc: 0.872688\n",
      "[229]\tvalid_0's auc: 0.878205\tvalid_1's auc: 0.872696\n",
      "[230]\tvalid_0's auc: 0.878223\tvalid_1's auc: 0.872706\n",
      "[231]\tvalid_0's auc: 0.878238\tvalid_1's auc: 0.87272\n",
      "[232]\tvalid_0's auc: 0.878254\tvalid_1's auc: 0.872719\n",
      "[233]\tvalid_0's auc: 0.87827\tvalid_1's auc: 0.872727\n",
      "[234]\tvalid_0's auc: 0.878284\tvalid_1's auc: 0.872731\n",
      "[235]\tvalid_0's auc: 0.878299\tvalid_1's auc: 0.872723\n",
      "[236]\tvalid_0's auc: 0.878315\tvalid_1's auc: 0.872723\n",
      "[237]\tvalid_0's auc: 0.878329\tvalid_1's auc: 0.872732\n",
      "[238]\tvalid_0's auc: 0.878344\tvalid_1's auc: 0.872729\n",
      "[239]\tvalid_0's auc: 0.878359\tvalid_1's auc: 0.872753\n",
      "[240]\tvalid_0's auc: 0.878374\tvalid_1's auc: 0.872759\n",
      "[241]\tvalid_0's auc: 0.878391\tvalid_1's auc: 0.872769\n",
      "[242]\tvalid_0's auc: 0.878406\tvalid_1's auc: 0.872773\n",
      "[243]\tvalid_0's auc: 0.878419\tvalid_1's auc: 0.872767\n",
      "[244]\tvalid_0's auc: 0.878435\tvalid_1's auc: 0.872777\n",
      "[245]\tvalid_0's auc: 0.878451\tvalid_1's auc: 0.872787\n",
      "[246]\tvalid_0's auc: 0.878464\tvalid_1's auc: 0.872792\n",
      "[247]\tvalid_0's auc: 0.878478\tvalid_1's auc: 0.872796\n",
      "[248]\tvalid_0's auc: 0.878492\tvalid_1's auc: 0.872806\n",
      "[249]\tvalid_0's auc: 0.878507\tvalid_1's auc: 0.872796\n",
      "[250]\tvalid_0's auc: 0.878522\tvalid_1's auc: 0.872798\n",
      "[251]\tvalid_0's auc: 0.878539\tvalid_1's auc: 0.87282\n",
      "[252]\tvalid_0's auc: 0.878553\tvalid_1's auc: 0.872836\n",
      "[253]\tvalid_0's auc: 0.878569\tvalid_1's auc: 0.872854\n",
      "[254]\tvalid_0's auc: 0.878583\tvalid_1's auc: 0.872855\n",
      "[255]\tvalid_0's auc: 0.878598\tvalid_1's auc: 0.87287\n",
      "[256]\tvalid_0's auc: 0.878615\tvalid_1's auc: 0.872879\n",
      "[257]\tvalid_0's auc: 0.87863\tvalid_1's auc: 0.872885\n",
      "[258]\tvalid_0's auc: 0.878647\tvalid_1's auc: 0.872905\n",
      "[259]\tvalid_0's auc: 0.878661\tvalid_1's auc: 0.872909\n",
      "[260]\tvalid_0's auc: 0.878677\tvalid_1's auc: 0.872917\n",
      "[261]\tvalid_0's auc: 0.878694\tvalid_1's auc: 0.87292\n",
      "[262]\tvalid_0's auc: 0.878707\tvalid_1's auc: 0.872924\n",
      "[263]\tvalid_0's auc: 0.87872\tvalid_1's auc: 0.872919\n",
      "[264]\tvalid_0's auc: 0.878735\tvalid_1's auc: 0.87293\n",
      "[265]\tvalid_0's auc: 0.878748\tvalid_1's auc: 0.872934\n",
      "[266]\tvalid_0's auc: 0.878764\tvalid_1's auc: 0.872945\n",
      "[267]\tvalid_0's auc: 0.878777\tvalid_1's auc: 0.872957\n",
      "[268]\tvalid_0's auc: 0.878791\tvalid_1's auc: 0.872966\n",
      "[269]\tvalid_0's auc: 0.878807\tvalid_1's auc: 0.872973\n",
      "[270]\tvalid_0's auc: 0.878822\tvalid_1's auc: 0.872978\n",
      "[271]\tvalid_0's auc: 0.878836\tvalid_1's auc: 0.872983\n",
      "[272]\tvalid_0's auc: 0.878849\tvalid_1's auc: 0.872994\n",
      "[273]\tvalid_0's auc: 0.878863\tvalid_1's auc: 0.873003\n",
      "[274]\tvalid_0's auc: 0.878878\tvalid_1's auc: 0.872998\n",
      "[275]\tvalid_0's auc: 0.878892\tvalid_1's auc: 0.873\n",
      "[276]\tvalid_0's auc: 0.878906\tvalid_1's auc: 0.873002\n",
      "[277]\tvalid_0's auc: 0.878921\tvalid_1's auc: 0.873011\n",
      "[278]\tvalid_0's auc: 0.878936\tvalid_1's auc: 0.873001\n",
      "[279]\tvalid_0's auc: 0.878951\tvalid_1's auc: 0.873007\n",
      "[280]\tvalid_0's auc: 0.878964\tvalid_1's auc: 0.873009\n",
      "[281]\tvalid_0's auc: 0.878977\tvalid_1's auc: 0.873015\n",
      "[282]\tvalid_0's auc: 0.878991\tvalid_1's auc: 0.873023\n",
      "[283]\tvalid_0's auc: 0.879006\tvalid_1's auc: 0.87303\n",
      "[284]\tvalid_0's auc: 0.87902\tvalid_1's auc: 0.873037\n",
      "[285]\tvalid_0's auc: 0.879033\tvalid_1's auc: 0.873045\n",
      "[286]\tvalid_0's auc: 0.879047\tvalid_1's auc: 0.873047\n",
      "[287]\tvalid_0's auc: 0.879062\tvalid_1's auc: 0.873049\n",
      "[288]\tvalid_0's auc: 0.879076\tvalid_1's auc: 0.873056\n",
      "[289]\tvalid_0's auc: 0.879092\tvalid_1's auc: 0.873053\n",
      "[290]\tvalid_0's auc: 0.879106\tvalid_1's auc: 0.873062\n",
      "[291]\tvalid_0's auc: 0.87912\tvalid_1's auc: 0.873069\n",
      "[292]\tvalid_0's auc: 0.879132\tvalid_1's auc: 0.873053\n",
      "[293]\tvalid_0's auc: 0.879145\tvalid_1's auc: 0.873065\n",
      "[294]\tvalid_0's auc: 0.87916\tvalid_1's auc: 0.873074\n",
      "[295]\tvalid_0's auc: 0.879173\tvalid_1's auc: 0.873078\n",
      "[296]\tvalid_0's auc: 0.879188\tvalid_1's auc: 0.873085\n",
      "[297]\tvalid_0's auc: 0.879201\tvalid_1's auc: 0.873083\n",
      "[298]\tvalid_0's auc: 0.879214\tvalid_1's auc: 0.873083\n",
      "[299]\tvalid_0's auc: 0.879227\tvalid_1's auc: 0.873088\n",
      "[300]\tvalid_0's auc: 0.87924\tvalid_1's auc: 0.873095\n",
      "[301]\tvalid_0's auc: 0.879254\tvalid_1's auc: 0.873098\n",
      "[302]\tvalid_0's auc: 0.879267\tvalid_1's auc: 0.873081\n",
      "[303]\tvalid_0's auc: 0.879282\tvalid_1's auc: 0.873086\n",
      "[304]\tvalid_0's auc: 0.879295\tvalid_1's auc: 0.873094\n",
      "[305]\tvalid_0's auc: 0.87931\tvalid_1's auc: 0.873089\n",
      "[306]\tvalid_0's auc: 0.879322\tvalid_1's auc: 0.873092\n",
      "[307]\tvalid_0's auc: 0.879334\tvalid_1's auc: 0.873092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[308]\tvalid_0's auc: 0.879347\tvalid_1's auc: 0.873079\n",
      "[309]\tvalid_0's auc: 0.879361\tvalid_1's auc: 0.873089\n",
      "[310]\tvalid_0's auc: 0.879374\tvalid_1's auc: 0.873089\n",
      "[311]\tvalid_0's auc: 0.879386\tvalid_1's auc: 0.873088\n",
      "[312]\tvalid_0's auc: 0.879399\tvalid_1's auc: 0.873096\n",
      "[313]\tvalid_0's auc: 0.879411\tvalid_1's auc: 0.873102\n",
      "[314]\tvalid_0's auc: 0.879424\tvalid_1's auc: 0.873107\n",
      "[315]\tvalid_0's auc: 0.879434\tvalid_1's auc: 0.873107\n",
      "[316]\tvalid_0's auc: 0.879448\tvalid_1's auc: 0.873118\n",
      "[317]\tvalid_0's auc: 0.87946\tvalid_1's auc: 0.873119\n",
      "[318]\tvalid_0's auc: 0.879472\tvalid_1's auc: 0.873115\n",
      "[319]\tvalid_0's auc: 0.879484\tvalid_1's auc: 0.873116\n",
      "[320]\tvalid_0's auc: 0.8795\tvalid_1's auc: 0.873131\n",
      "[321]\tvalid_0's auc: 0.879512\tvalid_1's auc: 0.873125\n",
      "[322]\tvalid_0's auc: 0.879523\tvalid_1's auc: 0.87312\n",
      "[323]\tvalid_0's auc: 0.879535\tvalid_1's auc: 0.873125\n",
      "[324]\tvalid_0's auc: 0.879548\tvalid_1's auc: 0.873133\n",
      "[325]\tvalid_0's auc: 0.87956\tvalid_1's auc: 0.873139\n",
      "[326]\tvalid_0's auc: 0.879573\tvalid_1's auc: 0.873148\n",
      "[327]\tvalid_0's auc: 0.879587\tvalid_1's auc: 0.873154\n",
      "[328]\tvalid_0's auc: 0.879604\tvalid_1's auc: 0.873162\n",
      "[329]\tvalid_0's auc: 0.879616\tvalid_1's auc: 0.87317\n",
      "[330]\tvalid_0's auc: 0.879629\tvalid_1's auc: 0.873182\n",
      "[331]\tvalid_0's auc: 0.879641\tvalid_1's auc: 0.873194\n",
      "[332]\tvalid_0's auc: 0.879654\tvalid_1's auc: 0.873204\n",
      "[333]\tvalid_0's auc: 0.879669\tvalid_1's auc: 0.873211\n",
      "[334]\tvalid_0's auc: 0.879683\tvalid_1's auc: 0.873213\n",
      "[335]\tvalid_0's auc: 0.879695\tvalid_1's auc: 0.873226\n",
      "[336]\tvalid_0's auc: 0.879706\tvalid_1's auc: 0.873238\n",
      "[337]\tvalid_0's auc: 0.879718\tvalid_1's auc: 0.873236\n",
      "[338]\tvalid_0's auc: 0.879731\tvalid_1's auc: 0.873231\n",
      "[339]\tvalid_0's auc: 0.879744\tvalid_1's auc: 0.873234\n",
      "[340]\tvalid_0's auc: 0.879759\tvalid_1's auc: 0.873242\n",
      "[341]\tvalid_0's auc: 0.87977\tvalid_1's auc: 0.873251\n",
      "[342]\tvalid_0's auc: 0.879786\tvalid_1's auc: 0.873261\n",
      "[343]\tvalid_0's auc: 0.879798\tvalid_1's auc: 0.873266\n",
      "[344]\tvalid_0's auc: 0.879813\tvalid_1's auc: 0.873273\n",
      "[345]\tvalid_0's auc: 0.879827\tvalid_1's auc: 0.873285\n",
      "[346]\tvalid_0's auc: 0.87984\tvalid_1's auc: 0.873285\n",
      "[347]\tvalid_0's auc: 0.879852\tvalid_1's auc: 0.87329\n",
      "[348]\tvalid_0's auc: 0.879866\tvalid_1's auc: 0.873292\n",
      "[349]\tvalid_0's auc: 0.879882\tvalid_1's auc: 0.873301\n",
      "[350]\tvalid_0's auc: 0.879894\tvalid_1's auc: 0.873302\n",
      "[351]\tvalid_0's auc: 0.879907\tvalid_1's auc: 0.873307\n",
      "[352]\tvalid_0's auc: 0.879919\tvalid_1's auc: 0.873314\n",
      "[353]\tvalid_0's auc: 0.87993\tvalid_1's auc: 0.873306\n",
      "[354]\tvalid_0's auc: 0.879945\tvalid_1's auc: 0.873313\n",
      "[355]\tvalid_0's auc: 0.879959\tvalid_1's auc: 0.873322\n",
      "[356]\tvalid_0's auc: 0.879973\tvalid_1's auc: 0.873329\n",
      "[357]\tvalid_0's auc: 0.879986\tvalid_1's auc: 0.873332\n",
      "[358]\tvalid_0's auc: 0.880001\tvalid_1's auc: 0.873343\n",
      "[359]\tvalid_0's auc: 0.880012\tvalid_1's auc: 0.873357\n",
      "[360]\tvalid_0's auc: 0.880025\tvalid_1's auc: 0.873351\n",
      "[361]\tvalid_0's auc: 0.880039\tvalid_1's auc: 0.873359\n",
      "[362]\tvalid_0's auc: 0.880054\tvalid_1's auc: 0.873373\n",
      "[363]\tvalid_0's auc: 0.880068\tvalid_1's auc: 0.873363\n",
      "[364]\tvalid_0's auc: 0.880082\tvalid_1's auc: 0.87337\n",
      "[365]\tvalid_0's auc: 0.880095\tvalid_1's auc: 0.873375\n",
      "[366]\tvalid_0's auc: 0.880107\tvalid_1's auc: 0.873384\n",
      "[367]\tvalid_0's auc: 0.880122\tvalid_1's auc: 0.873385\n",
      "[368]\tvalid_0's auc: 0.880136\tvalid_1's auc: 0.873384\n",
      "[369]\tvalid_0's auc: 0.880149\tvalid_1's auc: 0.873393\n",
      "[370]\tvalid_0's auc: 0.880162\tvalid_1's auc: 0.873398\n",
      "[371]\tvalid_0's auc: 0.88018\tvalid_1's auc: 0.873412\n",
      "[372]\tvalid_0's auc: 0.880192\tvalid_1's auc: 0.873426\n",
      "[373]\tvalid_0's auc: 0.880204\tvalid_1's auc: 0.873435\n",
      "[374]\tvalid_0's auc: 0.880218\tvalid_1's auc: 0.873446\n",
      "[375]\tvalid_0's auc: 0.880232\tvalid_1's auc: 0.873452\n",
      "[376]\tvalid_0's auc: 0.880246\tvalid_1's auc: 0.87346\n",
      "[377]\tvalid_0's auc: 0.88026\tvalid_1's auc: 0.873465\n",
      "[378]\tvalid_0's auc: 0.880274\tvalid_1's auc: 0.873459\n",
      "[379]\tvalid_0's auc: 0.88029\tvalid_1's auc: 0.873475\n",
      "[380]\tvalid_0's auc: 0.880303\tvalid_1's auc: 0.873475\n",
      "[381]\tvalid_0's auc: 0.880317\tvalid_1's auc: 0.873483\n",
      "[382]\tvalid_0's auc: 0.88033\tvalid_1's auc: 0.873495\n",
      "[383]\tvalid_0's auc: 0.880345\tvalid_1's auc: 0.873497\n",
      "[384]\tvalid_0's auc: 0.880357\tvalid_1's auc: 0.873512\n",
      "[385]\tvalid_0's auc: 0.88037\tvalid_1's auc: 0.873511\n",
      "[386]\tvalid_0's auc: 0.880385\tvalid_1's auc: 0.873522\n",
      "[387]\tvalid_0's auc: 0.880401\tvalid_1's auc: 0.873529\n",
      "[388]\tvalid_0's auc: 0.880412\tvalid_1's auc: 0.873529\n",
      "[389]\tvalid_0's auc: 0.880425\tvalid_1's auc: 0.873539\n",
      "[390]\tvalid_0's auc: 0.880438\tvalid_1's auc: 0.873536\n",
      "[391]\tvalid_0's auc: 0.880451\tvalid_1's auc: 0.873542\n",
      "[392]\tvalid_0's auc: 0.880466\tvalid_1's auc: 0.873555\n",
      "[393]\tvalid_0's auc: 0.88048\tvalid_1's auc: 0.873561\n",
      "[394]\tvalid_0's auc: 0.880494\tvalid_1's auc: 0.873576\n",
      "[395]\tvalid_0's auc: 0.880509\tvalid_1's auc: 0.873585\n",
      "[396]\tvalid_0's auc: 0.88052\tvalid_1's auc: 0.873588\n",
      "[397]\tvalid_0's auc: 0.880533\tvalid_1's auc: 0.873594\n",
      "[398]\tvalid_0's auc: 0.880545\tvalid_1's auc: 0.873599\n",
      "[399]\tvalid_0's auc: 0.880559\tvalid_1's auc: 0.873611\n",
      "[400]\tvalid_0's auc: 0.880571\tvalid_1's auc: 0.873608\n",
      "[401]\tvalid_0's auc: 0.880585\tvalid_1's auc: 0.873616\n",
      "[402]\tvalid_0's auc: 0.880598\tvalid_1's auc: 0.873623\n",
      "[403]\tvalid_0's auc: 0.880609\tvalid_1's auc: 0.873631\n",
      "[404]\tvalid_0's auc: 0.880621\tvalid_1's auc: 0.873632\n",
      "[405]\tvalid_0's auc: 0.880635\tvalid_1's auc: 0.873637\n",
      "[406]\tvalid_0's auc: 0.880647\tvalid_1's auc: 0.873642\n",
      "[407]\tvalid_0's auc: 0.880658\tvalid_1's auc: 0.873643\n",
      "[408]\tvalid_0's auc: 0.880669\tvalid_1's auc: 0.873646\n",
      "[409]\tvalid_0's auc: 0.880684\tvalid_1's auc: 0.873658\n",
      "[410]\tvalid_0's auc: 0.880696\tvalid_1's auc: 0.873664\n",
      "[411]\tvalid_0's auc: 0.880708\tvalid_1's auc: 0.873668\n",
      "[412]\tvalid_0's auc: 0.880721\tvalid_1's auc: 0.873668\n",
      "[413]\tvalid_0's auc: 0.880734\tvalid_1's auc: 0.873677\n",
      "[414]\tvalid_0's auc: 0.880747\tvalid_1's auc: 0.873685\n",
      "[415]\tvalid_0's auc: 0.880757\tvalid_1's auc: 0.873688\n",
      "[416]\tvalid_0's auc: 0.880771\tvalid_1's auc: 0.8737\n",
      "[417]\tvalid_0's auc: 0.880782\tvalid_1's auc: 0.873712\n",
      "[418]\tvalid_0's auc: 0.880795\tvalid_1's auc: 0.873716\n",
      "[419]\tvalid_0's auc: 0.880807\tvalid_1's auc: 0.873726\n",
      "[420]\tvalid_0's auc: 0.880818\tvalid_1's auc: 0.873733\n",
      "[421]\tvalid_0's auc: 0.880831\tvalid_1's auc: 0.873742\n",
      "[422]\tvalid_0's auc: 0.880841\tvalid_1's auc: 0.873749\n",
      "[423]\tvalid_0's auc: 0.880855\tvalid_1's auc: 0.873753\n",
      "[424]\tvalid_0's auc: 0.880868\tvalid_1's auc: 0.873759\n",
      "[425]\tvalid_0's auc: 0.88088\tvalid_1's auc: 0.873766\n",
      "[426]\tvalid_0's auc: 0.880892\tvalid_1's auc: 0.87377\n",
      "[427]\tvalid_0's auc: 0.880905\tvalid_1's auc: 0.873772\n",
      "[428]\tvalid_0's auc: 0.880916\tvalid_1's auc: 0.873777\n",
      "[429]\tvalid_0's auc: 0.880929\tvalid_1's auc: 0.873785\n",
      "[430]\tvalid_0's auc: 0.880945\tvalid_1's auc: 0.873802\n",
      "[431]\tvalid_0's auc: 0.880958\tvalid_1's auc: 0.873813\n",
      "[432]\tvalid_0's auc: 0.880971\tvalid_1's auc: 0.873826\n",
      "[433]\tvalid_0's auc: 0.880984\tvalid_1's auc: 0.873836\n",
      "[434]\tvalid_0's auc: 0.880999\tvalid_1's auc: 0.873834\n",
      "[435]\tvalid_0's auc: 0.881012\tvalid_1's auc: 0.873846\n",
      "[436]\tvalid_0's auc: 0.881023\tvalid_1's auc: 0.873846\n",
      "[437]\tvalid_0's auc: 0.881035\tvalid_1's auc: 0.873859\n",
      "[438]\tvalid_0's auc: 0.881046\tvalid_1's auc: 0.873865\n",
      "[439]\tvalid_0's auc: 0.881057\tvalid_1's auc: 0.87387\n",
      "[440]\tvalid_0's auc: 0.88107\tvalid_1's auc: 0.873873\n",
      "[441]\tvalid_0's auc: 0.88108\tvalid_1's auc: 0.873873\n",
      "[442]\tvalid_0's auc: 0.881093\tvalid_1's auc: 0.873881\n",
      "[443]\tvalid_0's auc: 0.881106\tvalid_1's auc: 0.873886\n",
      "[444]\tvalid_0's auc: 0.881118\tvalid_1's auc: 0.873888\n",
      "[445]\tvalid_0's auc: 0.881129\tvalid_1's auc: 0.873891\n",
      "[446]\tvalid_0's auc: 0.881141\tvalid_1's auc: 0.873905\n",
      "[447]\tvalid_0's auc: 0.881152\tvalid_1's auc: 0.873903\n",
      "[448]\tvalid_0's auc: 0.881163\tvalid_1's auc: 0.87391\n",
      "[449]\tvalid_0's auc: 0.881174\tvalid_1's auc: 0.873917\n",
      "[450]\tvalid_0's auc: 0.881184\tvalid_1's auc: 0.873915\n",
      "[451]\tvalid_0's auc: 0.881195\tvalid_1's auc: 0.873915\n",
      "[452]\tvalid_0's auc: 0.881207\tvalid_1's auc: 0.87392\n",
      "[453]\tvalid_0's auc: 0.881218\tvalid_1's auc: 0.873925\n",
      "[454]\tvalid_0's auc: 0.881231\tvalid_1's auc: 0.873933\n",
      "[455]\tvalid_0's auc: 0.881242\tvalid_1's auc: 0.873939\n",
      "[456]\tvalid_0's auc: 0.881256\tvalid_1's auc: 0.873944\n",
      "[457]\tvalid_0's auc: 0.881271\tvalid_1's auc: 0.873958\n",
      "[458]\tvalid_0's auc: 0.881284\tvalid_1's auc: 0.873971\n",
      "[459]\tvalid_0's auc: 0.881295\tvalid_1's auc: 0.873981\n",
      "[460]\tvalid_0's auc: 0.881308\tvalid_1's auc: 0.87399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[461]\tvalid_0's auc: 0.881318\tvalid_1's auc: 0.873995\n",
      "[462]\tvalid_0's auc: 0.881333\tvalid_1's auc: 0.874004\n",
      "[463]\tvalid_0's auc: 0.881344\tvalid_1's auc: 0.87401\n",
      "[464]\tvalid_0's auc: 0.881356\tvalid_1's auc: 0.874014\n",
      "[465]\tvalid_0's auc: 0.881368\tvalid_1's auc: 0.874013\n",
      "[466]\tvalid_0's auc: 0.881381\tvalid_1's auc: 0.874027\n",
      "[467]\tvalid_0's auc: 0.88139\tvalid_1's auc: 0.87403\n",
      "[468]\tvalid_0's auc: 0.881402\tvalid_1's auc: 0.87404\n",
      "[469]\tvalid_0's auc: 0.881412\tvalid_1's auc: 0.874044\n",
      "[470]\tvalid_0's auc: 0.881423\tvalid_1's auc: 0.874048\n",
      "[471]\tvalid_0's auc: 0.881436\tvalid_1's auc: 0.874043\n",
      "[472]\tvalid_0's auc: 0.881447\tvalid_1's auc: 0.874044\n",
      "[473]\tvalid_0's auc: 0.881459\tvalid_1's auc: 0.874051\n",
      "[474]\tvalid_0's auc: 0.881471\tvalid_1's auc: 0.874052\n",
      "[475]\tvalid_0's auc: 0.881485\tvalid_1's auc: 0.874058\n",
      "[476]\tvalid_0's auc: 0.881499\tvalid_1's auc: 0.874064\n",
      "[477]\tvalid_0's auc: 0.881511\tvalid_1's auc: 0.874067\n",
      "[478]\tvalid_0's auc: 0.881522\tvalid_1's auc: 0.874073\n",
      "[479]\tvalid_0's auc: 0.881535\tvalid_1's auc: 0.874076\n",
      "[480]\tvalid_0's auc: 0.881546\tvalid_1's auc: 0.874086\n",
      "[481]\tvalid_0's auc: 0.881557\tvalid_1's auc: 0.874089\n",
      "[482]\tvalid_0's auc: 0.881569\tvalid_1's auc: 0.874089\n",
      "[483]\tvalid_0's auc: 0.88158\tvalid_1's auc: 0.874087\n",
      "[484]\tvalid_0's auc: 0.88159\tvalid_1's auc: 0.874093\n",
      "[485]\tvalid_0's auc: 0.881599\tvalid_1's auc: 0.874102\n",
      "[486]\tvalid_0's auc: 0.88161\tvalid_1's auc: 0.874112\n",
      "[487]\tvalid_0's auc: 0.88162\tvalid_1's auc: 0.874113\n",
      "[488]\tvalid_0's auc: 0.88163\tvalid_1's auc: 0.874115\n",
      "[489]\tvalid_0's auc: 0.881642\tvalid_1's auc: 0.874114\n",
      "[490]\tvalid_0's auc: 0.881652\tvalid_1's auc: 0.874114\n",
      "[491]\tvalid_0's auc: 0.881664\tvalid_1's auc: 0.874117\n",
      "[492]\tvalid_0's auc: 0.881674\tvalid_1's auc: 0.874117\n",
      "[493]\tvalid_0's auc: 0.881685\tvalid_1's auc: 0.874118\n",
      "[494]\tvalid_0's auc: 0.881698\tvalid_1's auc: 0.874134\n",
      "[495]\tvalid_0's auc: 0.881708\tvalid_1's auc: 0.874136\n",
      "[496]\tvalid_0's auc: 0.881719\tvalid_1's auc: 0.874136\n",
      "[497]\tvalid_0's auc: 0.88173\tvalid_1's auc: 0.874149\n",
      "[498]\tvalid_0's auc: 0.88174\tvalid_1's auc: 0.874149\n",
      "[499]\tvalid_0's auc: 0.881749\tvalid_1's auc: 0.874151\n",
      "[500]\tvalid_0's auc: 0.881759\tvalid_1's auc: 0.874151\n",
      "[501]\tvalid_0's auc: 0.881769\tvalid_1's auc: 0.874159\n",
      "[502]\tvalid_0's auc: 0.881781\tvalid_1's auc: 0.874168\n",
      "[503]\tvalid_0's auc: 0.881791\tvalid_1's auc: 0.874168\n",
      "[504]\tvalid_0's auc: 0.881802\tvalid_1's auc: 0.874168\n",
      "[505]\tvalid_0's auc: 0.881812\tvalid_1's auc: 0.874172\n",
      "[506]\tvalid_0's auc: 0.881823\tvalid_1's auc: 0.87418\n",
      "[507]\tvalid_0's auc: 0.881834\tvalid_1's auc: 0.874185\n",
      "[508]\tvalid_0's auc: 0.881844\tvalid_1's auc: 0.874185\n",
      "[509]\tvalid_0's auc: 0.881856\tvalid_1's auc: 0.874191\n",
      "[510]\tvalid_0's auc: 0.881866\tvalid_1's auc: 0.874193\n",
      "[511]\tvalid_0's auc: 0.881877\tvalid_1's auc: 0.874198\n",
      "[512]\tvalid_0's auc: 0.881886\tvalid_1's auc: 0.8742\n",
      "[513]\tvalid_0's auc: 0.881897\tvalid_1's auc: 0.874202\n",
      "[514]\tvalid_0's auc: 0.881909\tvalid_1's auc: 0.874217\n",
      "[515]\tvalid_0's auc: 0.881919\tvalid_1's auc: 0.874224\n",
      "[516]\tvalid_0's auc: 0.881929\tvalid_1's auc: 0.874224\n",
      "[517]\tvalid_0's auc: 0.881939\tvalid_1's auc: 0.874224\n",
      "[518]\tvalid_0's auc: 0.881949\tvalid_1's auc: 0.87423\n",
      "[519]\tvalid_0's auc: 0.881959\tvalid_1's auc: 0.874231\n",
      "[520]\tvalid_0's auc: 0.881968\tvalid_1's auc: 0.874233\n",
      "[521]\tvalid_0's auc: 0.881977\tvalid_1's auc: 0.874229\n",
      "[522]\tvalid_0's auc: 0.881985\tvalid_1's auc: 0.874233\n",
      "[523]\tvalid_0's auc: 0.881996\tvalid_1's auc: 0.874232\n",
      "[524]\tvalid_0's auc: 0.882006\tvalid_1's auc: 0.874232\n",
      "[525]\tvalid_0's auc: 0.882016\tvalid_1's auc: 0.874235\n",
      "[526]\tvalid_0's auc: 0.882027\tvalid_1's auc: 0.874237\n",
      "[527]\tvalid_0's auc: 0.882036\tvalid_1's auc: 0.874238\n",
      "[528]\tvalid_0's auc: 0.882044\tvalid_1's auc: 0.874238\n",
      "[529]\tvalid_0's auc: 0.882054\tvalid_1's auc: 0.874233\n",
      "[530]\tvalid_0's auc: 0.882063\tvalid_1's auc: 0.874236\n",
      "[531]\tvalid_0's auc: 0.882073\tvalid_1's auc: 0.874238\n",
      "[532]\tvalid_0's auc: 0.882083\tvalid_1's auc: 0.874239\n",
      "[533]\tvalid_0's auc: 0.882091\tvalid_1's auc: 0.874239\n",
      "[534]\tvalid_0's auc: 0.882101\tvalid_1's auc: 0.874241\n",
      "[535]\tvalid_0's auc: 0.882111\tvalid_1's auc: 0.874241\n",
      "[536]\tvalid_0's auc: 0.882122\tvalid_1's auc: 0.874253\n",
      "[537]\tvalid_0's auc: 0.882131\tvalid_1's auc: 0.874255\n",
      "[538]\tvalid_0's auc: 0.88214\tvalid_1's auc: 0.874257\n",
      "[539]\tvalid_0's auc: 0.88215\tvalid_1's auc: 0.874262\n",
      "[540]\tvalid_0's auc: 0.882162\tvalid_1's auc: 0.874265\n",
      "[541]\tvalid_0's auc: 0.882173\tvalid_1's auc: 0.874272\n",
      "[542]\tvalid_0's auc: 0.882182\tvalid_1's auc: 0.874265\n",
      "[543]\tvalid_0's auc: 0.88219\tvalid_1's auc: 0.874265\n",
      "[544]\tvalid_0's auc: 0.882201\tvalid_1's auc: 0.874276\n",
      "[545]\tvalid_0's auc: 0.882209\tvalid_1's auc: 0.874274\n",
      "[546]\tvalid_0's auc: 0.882219\tvalid_1's auc: 0.874272\n",
      "[547]\tvalid_0's auc: 0.882227\tvalid_1's auc: 0.874273\n",
      "[548]\tvalid_0's auc: 0.882235\tvalid_1's auc: 0.874273\n",
      "[549]\tvalid_0's auc: 0.882245\tvalid_1's auc: 0.874272\n",
      "[550]\tvalid_0's auc: 0.882255\tvalid_1's auc: 0.874272\n",
      "[551]\tvalid_0's auc: 0.882265\tvalid_1's auc: 0.874275\n",
      "[552]\tvalid_0's auc: 0.882275\tvalid_1's auc: 0.874274\n",
      "[553]\tvalid_0's auc: 0.882285\tvalid_1's auc: 0.874274\n",
      "[554]\tvalid_0's auc: 0.882295\tvalid_1's auc: 0.874265\n",
      "[555]\tvalid_0's auc: 0.882304\tvalid_1's auc: 0.874266\n",
      "[556]\tvalid_0's auc: 0.882313\tvalid_1's auc: 0.874267\n",
      "[557]\tvalid_0's auc: 0.882322\tvalid_1's auc: 0.874264\n",
      "[558]\tvalid_0's auc: 0.882332\tvalid_1's auc: 0.874267\n",
      "[559]\tvalid_0's auc: 0.882341\tvalid_1's auc: 0.87427\n",
      "[560]\tvalid_0's auc: 0.88235\tvalid_1's auc: 0.874272\n",
      "[561]\tvalid_0's auc: 0.882357\tvalid_1's auc: 0.874269\n",
      "[562]\tvalid_0's auc: 0.882365\tvalid_1's auc: 0.874262\n",
      "[563]\tvalid_0's auc: 0.882376\tvalid_1's auc: 0.874261\n",
      "[564]\tvalid_0's auc: 0.882384\tvalid_1's auc: 0.874259\n",
      "[565]\tvalid_0's auc: 0.882394\tvalid_1's auc: 0.874258\n",
      "[566]\tvalid_0's auc: 0.882402\tvalid_1's auc: 0.874257\n",
      "[567]\tvalid_0's auc: 0.882411\tvalid_1's auc: 0.874259\n",
      "[568]\tvalid_0's auc: 0.882418\tvalid_1's auc: 0.874252\n",
      "[569]\tvalid_0's auc: 0.882425\tvalid_1's auc: 0.874252\n",
      "[570]\tvalid_0's auc: 0.882436\tvalid_1's auc: 0.874256\n",
      "[571]\tvalid_0's auc: 0.882446\tvalid_1's auc: 0.874269\n",
      "[572]\tvalid_0's auc: 0.882455\tvalid_1's auc: 0.874267\n",
      "[573]\tvalid_0's auc: 0.882463\tvalid_1's auc: 0.874268\n",
      "[574]\tvalid_0's auc: 0.882474\tvalid_1's auc: 0.874275\n",
      "[575]\tvalid_0's auc: 0.882483\tvalid_1's auc: 0.874279\n",
      "[576]\tvalid_0's auc: 0.882492\tvalid_1's auc: 0.874278\n",
      "[577]\tvalid_0's auc: 0.882502\tvalid_1's auc: 0.874274\n",
      "[578]\tvalid_0's auc: 0.88251\tvalid_1's auc: 0.874276\n",
      "[579]\tvalid_0's auc: 0.882519\tvalid_1's auc: 0.874279\n",
      "[580]\tvalid_0's auc: 0.882528\tvalid_1's auc: 0.874286\n",
      "[581]\tvalid_0's auc: 0.882538\tvalid_1's auc: 0.874286\n",
      "[582]\tvalid_0's auc: 0.882546\tvalid_1's auc: 0.874287\n",
      "[583]\tvalid_0's auc: 0.882554\tvalid_1's auc: 0.874286\n",
      "[584]\tvalid_0's auc: 0.882564\tvalid_1's auc: 0.874286\n",
      "[585]\tvalid_0's auc: 0.882571\tvalid_1's auc: 0.874288\n",
      "[586]\tvalid_0's auc: 0.882579\tvalid_1's auc: 0.87429\n",
      "[587]\tvalid_0's auc: 0.882587\tvalid_1's auc: 0.874292\n",
      "[588]\tvalid_0's auc: 0.882596\tvalid_1's auc: 0.874291\n",
      "[589]\tvalid_0's auc: 0.882604\tvalid_1's auc: 0.874295\n",
      "[590]\tvalid_0's auc: 0.882611\tvalid_1's auc: 0.874294\n",
      "[591]\tvalid_0's auc: 0.88262\tvalid_1's auc: 0.8743\n",
      "[592]\tvalid_0's auc: 0.882629\tvalid_1's auc: 0.874303\n",
      "[593]\tvalid_0's auc: 0.882635\tvalid_1's auc: 0.874303\n",
      "[594]\tvalid_0's auc: 0.882644\tvalid_1's auc: 0.874307\n",
      "[595]\tvalid_0's auc: 0.882652\tvalid_1's auc: 0.874311\n",
      "[596]\tvalid_0's auc: 0.882661\tvalid_1's auc: 0.87431\n",
      "[597]\tvalid_0's auc: 0.882669\tvalid_1's auc: 0.874308\n",
      "[598]\tvalid_0's auc: 0.882678\tvalid_1's auc: 0.874308\n",
      "[599]\tvalid_0's auc: 0.882686\tvalid_1's auc: 0.874306\n",
      "[600]\tvalid_0's auc: 0.882693\tvalid_1's auc: 0.874305\n",
      "[601]\tvalid_0's auc: 0.882701\tvalid_1's auc: 0.874305\n",
      "[602]\tvalid_0's auc: 0.882708\tvalid_1's auc: 0.8743\n",
      "[603]\tvalid_0's auc: 0.882718\tvalid_1's auc: 0.874297\n",
      "[604]\tvalid_0's auc: 0.882727\tvalid_1's auc: 0.874302\n",
      "[605]\tvalid_0's auc: 0.882737\tvalid_1's auc: 0.874298\n",
      "[606]\tvalid_0's auc: 0.882746\tvalid_1's auc: 0.8743\n",
      "[607]\tvalid_0's auc: 0.882755\tvalid_1's auc: 0.874301\n",
      "[608]\tvalid_0's auc: 0.882765\tvalid_1's auc: 0.874299\n",
      "[609]\tvalid_0's auc: 0.882772\tvalid_1's auc: 0.874302\n",
      "[610]\tvalid_0's auc: 0.882781\tvalid_1's auc: 0.874299\n",
      "[611]\tvalid_0's auc: 0.882789\tvalid_1's auc: 0.874302\n",
      "[612]\tvalid_0's auc: 0.882797\tvalid_1's auc: 0.874302\n",
      "[613]\tvalid_0's auc: 0.882803\tvalid_1's auc: 0.874304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[614]\tvalid_0's auc: 0.882812\tvalid_1's auc: 0.874304\n",
      "[615]\tvalid_0's auc: 0.882821\tvalid_1's auc: 0.874303\n",
      "[616]\tvalid_0's auc: 0.882829\tvalid_1's auc: 0.874308\n",
      "[617]\tvalid_0's auc: 0.882838\tvalid_1's auc: 0.874306\n",
      "[618]\tvalid_0's auc: 0.882847\tvalid_1's auc: 0.874309\n",
      "[619]\tvalid_0's auc: 0.882854\tvalid_1's auc: 0.874306\n",
      "[620]\tvalid_0's auc: 0.882862\tvalid_1's auc: 0.874309\n",
      "[621]\tvalid_0's auc: 0.882871\tvalid_1's auc: 0.874306\n",
      "[622]\tvalid_0's auc: 0.88288\tvalid_1's auc: 0.874291\n",
      "[623]\tvalid_0's auc: 0.882888\tvalid_1's auc: 0.87429\n",
      "[624]\tvalid_0's auc: 0.882896\tvalid_1's auc: 0.874288\n",
      "[625]\tvalid_0's auc: 0.882904\tvalid_1's auc: 0.874293\n",
      "[626]\tvalid_0's auc: 0.882911\tvalid_1's auc: 0.874294\n",
      "[627]\tvalid_0's auc: 0.882918\tvalid_1's auc: 0.874292\n",
      "[628]\tvalid_0's auc: 0.882925\tvalid_1's auc: 0.874291\n",
      "[629]\tvalid_0's auc: 0.882932\tvalid_1's auc: 0.874289\n",
      "[630]\tvalid_0's auc: 0.88294\tvalid_1's auc: 0.874291\n",
      "[631]\tvalid_0's auc: 0.882946\tvalid_1's auc: 0.87429\n",
      "[632]\tvalid_0's auc: 0.882954\tvalid_1's auc: 0.874291\n",
      "[633]\tvalid_0's auc: 0.882963\tvalid_1's auc: 0.874292\n",
      "[634]\tvalid_0's auc: 0.88297\tvalid_1's auc: 0.87429\n",
      "[635]\tvalid_0's auc: 0.882976\tvalid_1's auc: 0.874291\n",
      "[636]\tvalid_0's auc: 0.882985\tvalid_1's auc: 0.874291\n",
      "[637]\tvalid_0's auc: 0.882992\tvalid_1's auc: 0.874293\n",
      "[638]\tvalid_0's auc: 0.883001\tvalid_1's auc: 0.874293\n",
      "[639]\tvalid_0's auc: 0.883009\tvalid_1's auc: 0.874294\n",
      "[640]\tvalid_0's auc: 0.883017\tvalid_1's auc: 0.874292\n",
      "[641]\tvalid_0's auc: 0.883025\tvalid_1's auc: 0.874289\n",
      "[642]\tvalid_0's auc: 0.883032\tvalid_1's auc: 0.874278\n",
      "[643]\tvalid_0's auc: 0.88304\tvalid_1's auc: 0.874273\n",
      "[644]\tvalid_0's auc: 0.883048\tvalid_1's auc: 0.874276\n",
      "[645]\tvalid_0's auc: 0.883055\tvalid_1's auc: 0.874277\n",
      "Early stopping, best iteration is:\n",
      "[595]\tvalid_0's auc: 0.882652\tvalid_1's auc: 0.874311\n",
      "0.38899347394804296\n",
      "                                 importance\n",
      "prefix_title_rate                      7703\n",
      "prefix_tag_rate                        6213\n",
      "prefix_rate                            5116\n",
      "title_tag_rate                         3435\n",
      "query_prediction_rate                  2341\n",
      "tag_rate                               2286\n",
      "title_rate                             2235\n",
      "prefix_click_number                    2069\n",
      "prefix_title_click_number              1710\n",
      "title_tag_count                        1684\n",
      "prefix_title_count                     1681\n",
      "q_t_word_match                         1471\n",
      "tag_count                              1453\n",
      "prefix_title_types                     1415\n",
      "prefix_tag_count                       1300\n",
      "title_query_norm_similarity_std        1207\n",
      "query_prediction_click_number          1198\n",
      "prefix_tag_click_number                1045\n",
      "title_tag_click_number                 1017\n",
      "title_query_norm_similarity             906\n",
      "title_query_leven_min                   881\n",
      "prefix_tag_types                        852\n",
      "title_query_norm_similarity_min         829\n",
      "tag_title_types                         820\n",
      "title_prefix_leven_rate                 818\n",
      "title_query_cosine_similarity           782\n",
      "title_query_leven_sum                   762\n",
      "title_count                             735\n",
      "prefix_count                            735\n",
      "q_t_common_words                        717\n",
      "...                                     ...\n",
      "title_query_dot_similarity_min          327\n",
      "query_prediction_count                  316\n",
      "query_prediction_std                    310\n",
      "title_query_leven_std                   302\n",
      "query_prediction_key_len_max            296\n",
      "q_t_wc_ratio                            287\n",
      "p_q_wc_ratio                            260\n",
      "p_q_word_match                          228\n",
      "p_t_total_unique_words                  220\n",
      "len_title-prefix                        217\n",
      "q_t_wc_diff                             215\n",
      "title_query_prediction_types            196\n",
      "prefix_len                              181\n",
      "p_q_wc_diff                             173\n",
      "is_title_in_query                       163\n",
      "title_prefix_leven                      158\n",
      "p_t_wc_ratio                            145\n",
      "p_t_wc_ratio_unique                     131\n",
      "query_prediction_key_len_min            116\n",
      "p_q_total_unique_words                  115\n",
      "p_t_wc_diff                             115\n",
      "q_t_total_unique_words                  114\n",
      "q_t_wc_diff_unique                      106\n",
      "p_q_wc_diff_unique                      100\n",
      "tag_prefix_types                         94\n",
      "p_t_common_words                         77\n",
      "p_q_common_words                         63\n",
      "p_t_wc_diff_unique                       60\n",
      "query_prediction_number                  60\n",
      "tag_query_prediction_types                0\n",
      "\n",
      "[90 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: FutureWarning: by argument to sort_index is deprecated, pls use .sort_values(by=...)\n"
     ]
    }
   ],
   "source": [
    "lgb_model = lgb.LGBMClassifier(\n",
    "    boosting_type='gbdt', num_leaves=127, max_depth=-1, n_estimators=5000, objective='binary',\n",
    "    subsample=0.8, colsample_bytree=1, subsample_freq=1,\n",
    "    learning_rate=0.01, random_state=2018, n_jobs=-1\n",
    ")\n",
    "\n",
    "valid_df['predicted_score'] = 0\n",
    "\n",
    "lgb_model.fit(train_df[fea], train_df['label'], eval_set=[(train_df[fea], train_df['label']),\n",
    "                            (valid_df[fea], valid_df['label'])], early_stopping_rounds=50, eval_metric='auc')\n",
    "valid_pred = lgb_model.predict_proba(valid_df[fea], num_iteration=lgb_model.best_iteration_)[:, 1]\n",
    "print(np.mean(valid_pred))\n",
    "\n",
    "fscore = lgb_model.booster_.feature_importance()\n",
    "feaNames = lgb_model.booster_.feature_name()\n",
    "scoreDf = pd.DataFrame(index=feaNames, columns=['importance'], data=fscore)\n",
    "print(scoreDf.sort_index(by=['importance'], ascending=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3717271210240937\n",
      "0.4541913149487511\n"
     ]
    }
   ],
   "source": [
    "valid_df['predicted_score'] = valid_pred\n",
    "\n",
    "train_prefix_set = set(train_df['prefix'])\n",
    "\n",
    "valid_df['is_prefix_in_train'] = valid_df['prefix'].map(lambda x : 1 if x in train_prefix_set else 0)\n",
    "print(np.mean(valid_df[valid_df.is_prefix_in_train == 1]['predicted_score']))\n",
    "print(np.mean(valid_df[valid_df.is_prefix_in_train == 0]['predicted_score']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # 导出预测结果\n",
    "# def exportResult(df, fileName):\n",
    "#     df.to_csv('../result/%s.csv' % fileName, header=True, index=False)\n",
    "\n",
    "# exportResult(valid_df[['predicted_score', 'is_prefix_in_train']], 'valid_29_pred')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original mean :  0.3728237517271243\n",
      "0.45672466695017666\n"
     ]
    }
   ],
   "source": [
    "valid_prefix0_df = valid_df[valid_df.is_prefix_in_train == 1].copy()\n",
    "\n",
    "#定义调整函数\n",
    "def resultAdjustment(result_df, t):\n",
    "    result_df_temp = result_df.copy()\n",
    "    result_df_temp['x'] = result_df_temp.predicted_score.map(lambda x: -(math.log(((1 - x) / x), math.e)))\n",
    "    result_df_temp['adjust_result'] = result_df_temp.x.map(lambda x: 1 / (1 + math.exp(-(x + t)))) \n",
    "    print(result_df_temp['adjust_result'].mean())\n",
    "    return result_df_temp['adjust_result']\n",
    "\n",
    "print('original mean : ', valid_prefix0_df['predicted_score'].mean())\n",
    "valid_df_after = resultAdjustment(valid_prefix0_df, 0.61455)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.456716771801509\n",
      "0.45672466695017666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "valid_df['predicted_score'][valid_df.is_prefix_in_train == 1] = valid_df_after\n",
    "print(np.mean(valid_df['predicted_score'][valid_df.is_prefix_in_train == 0]))\n",
    "print(np.mean(valid_df['predicted_score'][valid_df.is_prefix_in_train == 1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0.35': 0.7226205354206408, '0.352': 0.7231884057971014, '0.354': 0.7236888419570574, '0.356': 0.7242314049586777, '0.358': 0.7247355986840652, '0.36': 0.7253583436559901, '0.362': 0.725551910630375, '0.364': 0.7256904360181156, '0.366': 0.7260359238840477, '0.368': 0.7265233374178456, '0.37': 0.7270575097710776, '0.372': 0.7278507937218727, '0.374': 0.7281165919282511, '0.376': 0.7284649240995239, '0.378': 0.7286964739767331, '0.38': 0.7287894843640791, '0.382': 0.729214650228063, '0.384': 0.7295017077197983, '0.386': 0.7301205366263365, '0.388': 0.730686183148851, '0.39': 0.7315783475783475, '0.392': 0.7323043200292263, '0.394': 0.7327412561801867, '0.396': 0.7331987705858066, '0.398': 0.7336106068613708, '0.4': 0.7342390052958784, '0.402': 0.7344272888827345, '0.404': 0.734647652945663, '0.406': 0.7342628190282089, '0.408': 0.7344619666048238, '0.41': 0.7348835048433181, '0.412': 0.7354022239799003, '0.414': 0.735841141099147, '0.416': 0.7358983937243182, '0.418': 0.736148127367092, '0.42': 0.7359692682469784, '0.422': 0.7366024271730711, '0.424': 0.7371790345736543, '0.426': 0.7376488621624809, '0.428': 0.7380912980780597, '0.43': 0.7376723035384397, '0.432': 0.7380059792151095, '0.434': 0.7383226419577096, '0.436': 0.7388580548343298, '0.438': 0.7392258865078986, '0.44': 0.739571373899732, '0.442': 0.7403073824537847, '0.444': 0.7402195057518193, '0.446': 0.7403827659165224, '0.448': 0.7404265574165562, '0.45': 0.7406692095987638, '0.452': 0.7405777831500061, '0.454': 0.7410818146568436, '0.456': 0.7411470595373898, '0.458': 0.7413114275992806, '0.46': 0.7412621477458168, '0.462': 0.7415121951219512, '0.464': 0.7416054116095631, '0.466': 0.7417824300528271, '0.468': 0.7423469387755103, '0.47': 0.7420932347086722, '0.472': 0.7421603645769184, '0.474': 0.7424934001134933, '0.476': 0.7436195757110677, '0.478': 0.7428571428571428, '0.48': 0.7429351874844797, '0.482': 0.7433619509767325, '0.484': 0.7432344879142785, '0.486': 0.7431437626331945, '0.488': 0.7434083922725115, '0.49': 0.7435525063848967, '0.492': 0.7432452754524037, '0.494': 0.7432690375728351, '0.496': 0.7434568149788604, '0.498': 0.7434747556182607, '0.5': 0.7434552332912988, '0.502': 0.7437356191054135, '0.504': 0.7436781026706533, '0.506': 0.7434010152284264, '0.508': 0.7432917058778645, '0.51': 0.7431423986959734, '0.512': 0.7432291001914486, '0.514': 0.7430763329497506, '0.516': 0.742898127513512, '0.518': 0.7427471116816432, '0.52': 0.7425574579669906, '0.522': 0.7429351606172234, '0.524': 0.7407906326097093, '0.526': 0.7409405722289953, '0.528': 0.7408215761166107, '0.53': 0.7407233931824095, '0.532': 0.7410865652956364, '0.534': 0.7410912703600093, '0.536': 0.7396477618198973, '0.538': 0.7390585208327864, '0.54': 0.7392310524378057, '0.542': 0.7391647796092475, '0.544': 0.738804789282135, '0.546': 0.7386654696205539, '0.548': 0.7385071152727081, '0.55': 0.7377153458786112, '0.552': 0.7375132837407015, '0.554': 0.7370802065038055, '0.556': 0.7366344025811269, '0.558': 0.7366144150399231, '0.56': 0.7367435389801488, '0.562': 0.7365798778527805, '0.564': 0.7359184221119013, '0.566': 0.7355944958073534, '0.568': 0.7352608625424003, '0.57': 0.7348358225049872, '0.572': 0.7343817552961521, '0.574': 0.7340511877062929, '0.576': 0.7342033153368241, '0.578': 0.7339324618736385, '0.58': 0.7338058887677208, '0.582': 0.7334315973359536, '0.584': 0.7334463338618842, '0.586': 0.7329883074563925, '0.588': 0.7327080247929351, '0.59': 0.7323556873729884, '0.592': 0.7308497740548882, '0.594': 0.730655624568668, '0.596': 0.7306448492219243, '0.598': 0.7301376311927112, '0.6': 0.7295625398762794, '0.602': 0.7291319328665111, '0.604': 0.7288824544796482, '0.606': 0.7285813921048961, '0.608': 0.7280552217533467, '0.61': 0.7276899398348957, '0.612': 0.7273287920836486, '0.614': 0.7270991294580174, '0.616': 0.7261988468569822, '0.618': 0.7253298748167364, '0.62': 0.7246131254941827, '0.622': 0.7238391845979615, '0.624': 0.7232715104078044, '0.626': 0.722348527907241, '0.628': 0.7218473278095726, '0.63': 0.720144028805761, '0.632': 0.7193725669796199, '0.634': 0.718829771977628, '0.636': 0.7187015225509911, '0.638': 0.7173956921657392, '0.64': 0.7167326189307067, '0.642': 0.7165923953932519, '0.644': 0.7159627815298995, '0.646': 0.7142109851787273, '0.648': 0.7132459970887919, '0.65': 0.7125598225749971, '0.652': 0.7123928749012841, '0.654': 0.7123111163172074, '0.656': 0.7115091015854376, '0.658': 0.7112823076244523, '0.66': 0.7102594339622642, '0.662': 0.7097478892365827, '0.664': 0.708751368788659, '0.666': 0.7081529795434331, '0.668': 0.7073185220387311, '0.67': 0.7066595250848062, '0.672': 0.7047198256560289, '0.674': 0.7043491256912271, '0.676': 0.7036981584069472, '0.678': 0.7028146192162276, '0.68': 0.7021014340257945, '0.682': 0.7013401596145158, '0.684': 0.7003832342557107, '0.686': 0.6992096417648306, '0.688': 0.6954404334327631, '0.69': 0.6945198377603611, '0.692': 0.6937303212790022, '0.694': 0.6922511034820991, '0.696': 0.691118148000123, '0.698': 0.690476923550602}\n"
     ]
    }
   ],
   "source": [
    "yuzhi_dict = {}\n",
    "# 定义搜索方法获取最佳F1对应的阈值\n",
    "for yuzhi in range(350, 700, 2):\n",
    "    real_yuzhi = yuzhi / 1000\n",
    "    valid_df['predicted_label'] = valid_df['predicted_score'].map(lambda x : 1 if x > real_yuzhi else 0)\n",
    "    f1 = f1_score(valid_df['label'], valid_df['predicted_label'])\n",
    "    yuzhi_dict[str(real_yuzhi)] = f1\n",
    "print(yuzhi_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
