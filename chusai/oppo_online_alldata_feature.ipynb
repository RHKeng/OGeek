{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import gc\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import chi2, SelectPercentile\n",
    "import math\n",
    "from sklearn.metrics import f1_score\n",
    "import jieba\n",
    "import jieba.posseg as psg\n",
    "from collections import Counter\n",
    "import functools\n",
    "from gensim.models import word2vec\n",
    "import Levenshtein\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 99036 entries, 0 to 99035\n",
      "Data columns (total 5 columns):\n",
      "index               99036 non-null int64\n",
      "prefix              99036 non-null object\n",
      "query_prediction    99036 non-null object\n",
      "title               99036 non-null object\n",
      "tag                 99036 non-null object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 3.8+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "train_df_26 = pd.read_table('../data/oppo_round1_train_20180926.txt', names=['prefix', 'query_prediction', 'title', 'tag', 'label'], header=None, quoting=3)\n",
    "train_df_29 = pd.read_table('../data/oppo_round1_train_20180929.txt', names=['prefix', 'query_prediction', 'title', 'tag', 'label'], header=None, quoting=3)\n",
    "valid_df_26 = pd.read_table('../data/oppo_round1_vali_20180926.txt', names=['prefix', 'query_prediction', 'title', 'tag', 'label'], header=None, quoting=3)\n",
    "valid_df_29 = pd.read_table('../data/oppo_round1_vali_20180929.txt', names=['prefix', 'query_prediction', 'title', 'tag', 'label'], header=None, quoting=3)\n",
    "train_df = pd.concat([train_df_26, train_df_29, valid_df_26, valid_df_29])\n",
    "train_df = train_df[train_df.query_prediction.notnull()]\n",
    "train_df.reset_index(inplace=True)\n",
    "train_df['index'] = train_df.index\n",
    "\n",
    "test_df_26 = pd.read_table('../data/oppo_round1_test_A_20180926.txt', names=['prefix', 'query_prediction', 'title', 'tag'], header=None, quoting=3)\n",
    "test_df_29 = pd.read_table('../data/oppo_round1_test_A_20180929.txt', names=['prefix', 'query_prediction', 'title', 'tag'], header=None, quoting=3)\n",
    "test_df = pd.concat([test_df_26, test_df_29])\n",
    "test_df = test_df[test_df.query_prediction.notnull()]\n",
    "test_df.reset_index(inplace=True)\n",
    "print(test_df.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37248\n",
      "0\n",
      "964\n",
      "0\n",
      "958\n",
      "0\n",
      "Empty DataFrame\n",
      "Columns: [index, prefix, query_prediction, title, tag, label]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df_26[train_df_26.query_prediction.isnull()]))\n",
    "print(len(train_df_29[train_df_29.query_prediction.isnull()]))\n",
    "print(len(test_df_26[test_df_26.query_prediction.isnull()]))\n",
    "print(len(test_df_29[test_df_29.query_prediction.isnull()]))\n",
    "print(len(valid_df_26[valid_df_26.query_prediction.isnull()]))\n",
    "print(len(valid_df_29[valid_df_29.query_prediction.isnull()]))\n",
    "\n",
    "print(train_df[train_df.query_prediction.isnull()].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nan_to_string(df):\n",
    "    df['prefix'] = df['prefix'].astype(str)\n",
    "    df['title'] = df['title'].astype(str)\n",
    "    df['tag'] = df['tag'].astype(str)\n",
    "    return df\n",
    "\n",
    "train_df = nan_to_string(train_df)\n",
    "test_df = nan_to_string(test_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index prefix                                   query_prediction  \\\n",
      "0      0     逆行  {\"逆行我的1997\": \"0.008\", \"逆行性遗忘\": \"0.040\", \"逆行遗忘症...   \n",
      "1      1      攸  {\"攸县\": \"0.065\", \"攸妍化妆品怎么样\": \"0.021\", \"攸\": \"0.0...   \n",
      "2      2    四物汤  {\"四物汤处方\": \"0.094\", \"四物汤的功效与作用\": \"0.293\", \"四物汤怎...   \n",
      "3      3      优  {\"优速快递查询\": \"0.012\", \"优信二手车\": \"0.016\", \"优速快递\": ...   \n",
      "4      4     夫西  {\"夫西地酸乳膏的作用\": \"0.055\", \"夫西地酸乳膏\": \"0.612\", \"夫西地...   \n",
      "\n",
      "         title tag  label                              query_prediction_dict  \\\n",
      "0  右拐拐到逆行车道扣几分  知道      1  {'逆行我的1997': '0.008', '逆行性遗忘': '0.040', '逆行遗忘症...   \n",
      "1            攸  百科      0  {'攸县': '0.065', '攸妍化妆品怎么样': '0.021', '攸': '0.0...   \n",
      "2          四物汤  百科      1  {'四物汤处方': '0.094', '四物汤的功效与作用': '0.293', '四物汤怎...   \n",
      "3           优酷  百科      0  {'优速快递查询': '0.012', '优信二手车': '0.016', '优速快递': ...   \n",
      "4   夫西地酸乳膏有激素吗  健康      0  {'夫西地酸乳膏的作用': '0.055', '夫西地酸乳膏': '0.612', '夫西地...   \n",
      "\n",
      "                               query_prediction_keys  \\\n",
      "0  [逆行我的1997, 逆行性遗忘, 逆行遗忘症, 逆行性遗忘症, 逆行扣几分, 逆行我的19...   \n",
      "1  [攸县, 攸妍化妆品怎么样, 攸, 攸县政府门户网站, 攸妍, 攸怎么读, 攸怎么读音, 攸...   \n",
      "2  [四物汤处方, 四物汤的功效与作用, 四物汤怎么煮, 四物汤怎么熬, 四物汤_百度百科, 四...   \n",
      "3  [优速快递查询, 优信二手车, 优速快递, 优, 优漫画, 优酷下载, 优速快递单号查询, ...   \n",
      "4  [夫西地酸乳膏的作用, 夫西地酸乳膏, 夫西地酸祛痘更严重了, 夫西, 夫西地酸乳膏祛痘, ...   \n",
      "\n",
      "                             query_prediction_values  query_prediction_number  \\\n",
      "0  [0.008, 0.04, 0.008, 0.039, 0.07, 0.008, 0.648...                       11   \n",
      "1  [0.065, 0.021, 0.0, 0.024, 0.015, 0.079, 0.04,...                       11   \n",
      "2  [0.094, 0.293, 0.006, 0.005, 0.008, 0.005, 0.0...                       11   \n",
      "3  [0.012, 0.016, 0.01, 0.0, 0.012, 0.016, 0.026,...                       11   \n",
      "4  [0.055, 0.612, 0.012, 0.002, 0.011, 0.017, 0.0...                       11   \n",
      "\n",
      "   query_prediction_max  query_prediction_min  query_prediction_mean  \\\n",
      "0                 0.648                 0.008               0.081545   \n",
      "1                 0.079                 0.000               0.029545   \n",
      "2                 0.340                 0.005               0.080727   \n",
      "3                 0.126                 0.000               0.024455   \n",
      "4                 0.612                 0.002               0.081545   \n",
      "\n",
      "   query_prediction_std  \n",
      "0              0.180307  \n",
      "1              0.022207  \n",
      "2              0.115338  \n",
      "3              0.032839  \n",
      "4              0.169174  \n"
     ]
    }
   ],
   "source": [
    "def get_float_list(x):\n",
    "    return_list = []\n",
    "    for temp in x:\n",
    "        return_list.append(float(temp))\n",
    "    return return_list\n",
    "\n",
    "# 处理跟query_prediction相关的统计特征\n",
    "def get_query_prediction_feature(df):\n",
    "    df['query_prediction_dict'] = df['query_prediction'].map(lambda x : dict() if x is np.nan else eval(x))\n",
    "    df['query_prediction_keys'] = df['query_prediction_dict'].map(lambda x : list(x.keys()))\n",
    "    df['query_prediction_values'] = df['query_prediction_dict'].map(lambda x : get_float_list(list(x.values())))\n",
    "    df['query_prediction_number'] = df['query_prediction_keys'].map(lambda x : len(x))\n",
    "    df['query_prediction_max'] = df['query_prediction_values'].map(lambda x : np.nan if len(x) == 0 else np.max(x))\n",
    "    df['query_prediction_min'] = df['query_prediction_values'].map(lambda x : np.nan if len(x) == 0 else np.min(x))\n",
    "    df['query_prediction_mean'] = df['query_prediction_values'].map(lambda x : np.nan if len(x) == 0 else np.mean(x))\n",
    "    df['query_prediction_std'] = df['query_prediction_values'].map(lambda x : np.nan if len(x) == 0 else np.std(x))\n",
    "    return df\n",
    "\n",
    "train_df = get_query_prediction_feature(train_df)\n",
    "test_df = get_query_prediction_feature(test_df)\n",
    "print(train_df.head())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefix : finish!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:40: FutureWarning: by argument to sort_index is deprecated, pls use .sort_values(by=...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title : finish!!!\n",
      "tag : finish!!!\n",
      "query_prediction : finish!!!\n",
      "   index prefix                                   query_prediction  \\\n",
      "0      0     逆行  {\"逆行我的1997\": \"0.008\", \"逆行性遗忘\": \"0.040\", \"逆行遗忘症...   \n",
      "1      1      攸  {\"攸县\": \"0.065\", \"攸妍化妆品怎么样\": \"0.021\", \"攸\": \"0.0...   \n",
      "0      2    四物汤  {\"四物汤处方\": \"0.094\", \"四物汤的功效与作用\": \"0.293\", \"四物汤怎...   \n",
      "1      3      优  {\"优速快递查询\": \"0.012\", \"优信二手车\": \"0.016\", \"优速快递\": ...   \n",
      "2      4     夫西  {\"夫西地酸乳膏的作用\": \"0.055\", \"夫西地酸乳膏\": \"0.612\", \"夫西地...   \n",
      "\n",
      "         title tag  label                              query_prediction_dict  \\\n",
      "0  右拐拐到逆行车道扣几分  知道      1  {'逆行我的1997': '0.008', '逆行性遗忘': '0.040', '逆行遗忘症...   \n",
      "1            攸  百科      0  {'攸县': '0.065', '攸妍化妆品怎么样': '0.021', '攸': '0.0...   \n",
      "0          四物汤  百科      1  {'四物汤处方': '0.094', '四物汤的功效与作用': '0.293', '四物汤怎...   \n",
      "1           优酷  百科      0  {'优速快递查询': '0.012', '优信二手车': '0.016', '优速快递': ...   \n",
      "2   夫西地酸乳膏有激素吗  健康      0  {'夫西地酸乳膏的作用': '0.055', '夫西地酸乳膏': '0.612', '夫西地...   \n",
      "\n",
      "                               query_prediction_keys  \\\n",
      "0  [逆行我的1997, 逆行性遗忘, 逆行遗忘症, 逆行性遗忘症, 逆行扣几分, 逆行我的19...   \n",
      "1  [攸县, 攸妍化妆品怎么样, 攸, 攸县政府门户网站, 攸妍, 攸怎么读, 攸怎么读音, 攸...   \n",
      "0  [四物汤处方, 四物汤的功效与作用, 四物汤怎么煮, 四物汤怎么熬, 四物汤_百度百科, 四...   \n",
      "1  [优速快递查询, 优信二手车, 优速快递, 优, 优漫画, 优酷下载, 优速快递单号查询, ...   \n",
      "2  [夫西地酸乳膏的作用, 夫西地酸乳膏, 夫西地酸祛痘更严重了, 夫西, 夫西地酸乳膏祛痘, ...   \n",
      "\n",
      "                             query_prediction_values  query_prediction_number  \\\n",
      "0  [0.008, 0.04, 0.008, 0.039, 0.07, 0.008, 0.648...                       11   \n",
      "1  [0.065, 0.021, 0.0, 0.024, 0.015, 0.079, 0.04,...                       11   \n",
      "0  [0.094, 0.293, 0.006, 0.005, 0.008, 0.005, 0.0...                       11   \n",
      "1  [0.012, 0.016, 0.01, 0.0, 0.012, 0.016, 0.026,...                       11   \n",
      "2  [0.055, 0.612, 0.012, 0.002, 0.011, 0.017, 0.0...                       11   \n",
      "\n",
      "               ...                prefix_click_number  title_count  \\\n",
      "0              ...                               26.0         26.0   \n",
      "1              ...                               20.0         25.0   \n",
      "0              ...                               27.0         89.0   \n",
      "1              ...                             1403.0       2455.0   \n",
      "2              ...                               29.0         29.0   \n",
      "\n",
      "   title_rate  title_click_number  tag_count  tag_rate  tag_click_number  \\\n",
      "0    0.955232                25.0     210332  0.245414             51618   \n",
      "1    0.083304                 2.0    1180976  0.404180            477327   \n",
      "0    0.650805                58.0    1180103  0.404409            477245   \n",
      "1    0.155219               381.0    1180103  0.404409            477245   \n",
      "2    0.140230                 4.0     215555  0.297303             64085   \n",
      "\n",
      "   query_prediction_count  query_prediction_rate  \\\n",
      "0                    27.0               0.513949   \n",
      "1                    23.0               0.474716   \n",
      "0                    44.0               0.386659   \n",
      "1                  1713.0               0.379463   \n",
      "2                    32.0               0.526972   \n",
      "\n",
      "   query_prediction_click_number  \n",
      "0                           14.0  \n",
      "1                           11.0  \n",
      "0                           17.0  \n",
      "1                          650.0  \n",
      "2                           17.0  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "def getBayesSmoothParam(origion_rate):\n",
    "    origion_rate_mean = origion_rate.mean()\n",
    "    origion_rate_var = origion_rate.var()\n",
    "    alpha = origion_rate_mean / origion_rate_var * (origion_rate_mean * (1 - origion_rate_mean) - origion_rate_var)\n",
    "    beta = (1 - origion_rate_mean) / origion_rate_var * (origion_rate_mean * (1 - origion_rate_mean) - origion_rate_var)\n",
    "#     print('origion_rate_mean : ', origion_rate_mean)\n",
    "#     print('origion_rate_var : ', origion_rate_var)\n",
    "#     print('alpha : ', alpha)\n",
    "#     print('beta : ', beta)\n",
    "    return alpha, beta\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, random_state=2018, shuffle=True)\n",
    "\n",
    "# 统计单维度的转化率特征\n",
    "def get_single_dimension_rate_feature(train_df, valid_df, fea_set):\n",
    "    for fea in fea_set:\n",
    "        train_temp_df = pd.DataFrame()\n",
    "        for index, (train_index, test_index) in enumerate(skf.split(train_df, train_df['label'])):\n",
    "            temp_df = train_df[[fea, 'label']].iloc[train_index].copy()\n",
    "            temp_pivot_table = pd.pivot_table(temp_df, index=fea, values='label', aggfunc={len, np.mean, np.sum})\n",
    "            temp_pivot_table.reset_index(inplace=True)\n",
    "            temp_pivot_table.rename(columns={'len':fea + '_count', 'mean':fea + '_rate', 'sum':fea + '_click_number'}, inplace=True)\n",
    "            alpha, beta = getBayesSmoothParam(temp_pivot_table[fea + '_rate'])\n",
    "            temp_pivot_table[fea + '_rate'] = (temp_pivot_table[fea + '_click_number'] + alpha) / (temp_pivot_table[fea + '_count'] + alpha + beta)\n",
    "#             del temp_pivot_table[fea + '_click_number']\n",
    "            fea_df = train_df.iloc[test_index].copy()\n",
    "            fea_df = pd.merge(fea_df, temp_pivot_table, on=fea, how='left')\n",
    "#             print(fea_df.head())\n",
    "            train_temp_df = pd.concat([train_temp_df, fea_df])\n",
    "        temp_df = train_df[[fea, 'label']].copy()\n",
    "        temp_pivot_table = pd.pivot_table(temp_df, index=fea, values='label', aggfunc={len, np.mean, np.sum})\n",
    "        temp_pivot_table.reset_index(inplace=True)\n",
    "        temp_pivot_table.rename(columns={'len':fea + '_count', 'mean':fea + '_rate', 'sum':fea + '_click_number'}, inplace=True)\n",
    "        alpha, beta = getBayesSmoothParam(temp_pivot_table[fea + '_rate'])\n",
    "        temp_pivot_table[fea + '_rate'] = (temp_pivot_table[fea + '_click_number'] + alpha) / (temp_pivot_table[fea + '_count'] + alpha + beta)\n",
    "#             del temp_pivot_table[fea + '_click_number']\n",
    "        valid_df = pd.merge(valid_df, temp_pivot_table, on=fea, how='left')\n",
    "        print(fea + ' : finish!!!')\n",
    "        train_df = train_temp_df\n",
    "        train_df.sort_index(by='index', ascending=True, inplace=True)\n",
    "    return train_df, valid_df\n",
    "    \n",
    "fea_set = ['prefix', 'title', 'tag', 'query_prediction']\n",
    "train_df, test_df = get_single_dimension_rate_feature(train_df, test_df, fea_set)\n",
    "print(train_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefix_title : finish!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:29: FutureWarning: by argument to sort_index is deprecated, pls use .sort_values(by=...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefix_tag : finish!!!\n",
      "title_tag : finish!!!\n",
      "   index prefix                                   query_prediction  \\\n",
      "0      0     逆行  {\"逆行我的1997\": \"0.008\", \"逆行性遗忘\": \"0.040\", \"逆行遗忘症...   \n",
      "1      1      攸  {\"攸县\": \"0.065\", \"攸妍化妆品怎么样\": \"0.021\", \"攸\": \"0.0...   \n",
      "0      2    四物汤  {\"四物汤处方\": \"0.094\", \"四物汤的功效与作用\": \"0.293\", \"四物汤怎...   \n",
      "1      3      优  {\"优速快递查询\": \"0.012\", \"优信二手车\": \"0.016\", \"优速快递\": ...   \n",
      "2      4     夫西  {\"夫西地酸乳膏的作用\": \"0.055\", \"夫西地酸乳膏\": \"0.612\", \"夫西地...   \n",
      "\n",
      "         title tag  label                              query_prediction_dict  \\\n",
      "0  右拐拐到逆行车道扣几分  知道      1  {'逆行我的1997': '0.008', '逆行性遗忘': '0.040', '逆行遗忘症...   \n",
      "1            攸  百科      0  {'攸县': '0.065', '攸妍化妆品怎么样': '0.021', '攸': '0.0...   \n",
      "0          四物汤  百科      1  {'四物汤处方': '0.094', '四物汤的功效与作用': '0.293', '四物汤怎...   \n",
      "1           优酷  百科      0  {'优速快递查询': '0.012', '优信二手车': '0.016', '优速快递': ...   \n",
      "2   夫西地酸乳膏有激素吗  健康      0  {'夫西地酸乳膏的作用': '0.055', '夫西地酸乳膏': '0.612', '夫西地...   \n",
      "\n",
      "                               query_prediction_keys  \\\n",
      "0  [逆行我的1997, 逆行性遗忘, 逆行遗忘症, 逆行性遗忘症, 逆行扣几分, 逆行我的19...   \n",
      "1  [攸县, 攸妍化妆品怎么样, 攸, 攸县政府门户网站, 攸妍, 攸怎么读, 攸怎么读音, 攸...   \n",
      "0  [四物汤处方, 四物汤的功效与作用, 四物汤怎么煮, 四物汤怎么熬, 四物汤_百度百科, 四...   \n",
      "1  [优速快递查询, 优信二手车, 优速快递, 优, 优漫画, 优酷下载, 优速快递单号查询, ...   \n",
      "2  [夫西地酸乳膏的作用, 夫西地酸乳膏, 夫西地酸祛痘更严重了, 夫西, 夫西地酸乳膏祛痘, ...   \n",
      "\n",
      "                             query_prediction_values  query_prediction_number  \\\n",
      "0  [0.008, 0.04, 0.008, 0.039, 0.07, 0.008, 0.648...                       11   \n",
      "1  [0.065, 0.021, 0.0, 0.024, 0.015, 0.079, 0.04,...                       11   \n",
      "0  [0.094, 0.293, 0.006, 0.005, 0.008, 0.005, 0.0...                       11   \n",
      "1  [0.012, 0.016, 0.01, 0.0, 0.012, 0.016, 0.026,...                       11   \n",
      "2  [0.055, 0.612, 0.012, 0.002, 0.011, 0.017, 0.0...                       11   \n",
      "\n",
      "            ...            query_prediction_click_number  prefix_title_count  \\\n",
      "0           ...                                     14.0                26.0   \n",
      "1           ...                                     11.0                25.0   \n",
      "0           ...                                     17.0                27.0   \n",
      "1           ...                                    650.0              1295.0   \n",
      "2           ...                                     17.0                21.0   \n",
      "\n",
      "   prefix_title_rate  prefix_title_click_number  prefix_tag_count  \\\n",
      "0           0.956022                       25.0              26.0   \n",
      "1           0.082885                        2.0              25.0   \n",
      "0           0.700712                       19.0              27.0   \n",
      "1           0.099667                      129.0            1292.0   \n",
      "2           0.192628                        4.0              46.0   \n",
      "\n",
      "   prefix_tag_rate  prefix_tag_click_number  title_tag_count  title_tag_rate  \\\n",
      "0         0.950986                     25.0             26.0        0.956090   \n",
      "1         0.085718                      2.0             25.0        0.082878   \n",
      "0         0.698069                     19.0             89.0        0.650928   \n",
      "1         0.097629                    126.0           2449.0        0.153146   \n",
      "2         0.455752                     21.0             29.0        0.139935   \n",
      "\n",
      "   title_tag_click_number  \n",
      "0                    25.0  \n",
      "1                     2.0  \n",
      "0                    58.0  \n",
      "1                   375.0  \n",
      "2                     4.0  \n",
      "\n",
      "[5 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "# 统计双维度交叉转化率\n",
    "def get_jiaocha_dimension_rate_feature(train_df, valid_df, fea_set):\n",
    "    for i in range(len(fea_set)):\n",
    "        for j in range((i+1), len(fea_set)):\n",
    "            fea1 = fea_set[i]\n",
    "            fea2 = fea_set[j]\n",
    "            train_temp_df = pd.DataFrame()\n",
    "            for index, (train_index, test_index) in enumerate(skf.split(train_df, train_df['label'])):\n",
    "                temp_df = train_df[[fea1, fea2, 'label']].iloc[train_index].copy()\n",
    "                temp_pivot_table = pd.pivot_table(temp_df, index=[fea1, fea2], values='label', aggfunc={len, np.mean, np.sum})\n",
    "                temp_pivot_table.reset_index(inplace=True)\n",
    "                temp_pivot_table.rename(columns={'len':fea1 + '_' + fea2 + '_count', 'mean':fea1 + '_' + fea2 + '_rate', 'sum':fea1 + '_' + fea2 + '_click_number'}, inplace=True)\n",
    "                alpha, beta = getBayesSmoothParam(temp_pivot_table[fea1 + '_' + fea2 + '_rate'])\n",
    "                temp_pivot_table[fea1 + '_' + fea2 + '_rate'] = (temp_pivot_table[fea1 + '_' + fea2 + '_click_number'] + alpha) / (temp_pivot_table[fea1 + '_' + fea2 + '_count'] + alpha + beta)\n",
    "#                 del temp_pivot_table[fea1 + '_' + fea2 + '_click_number']\n",
    "                fea_df = train_df.iloc[test_index].copy()\n",
    "                fea_df = pd.merge(fea_df, temp_pivot_table, on=[fea1, fea2], how='left')\n",
    "                train_temp_df = pd.concat([train_temp_df, fea_df])\n",
    "            temp_df = train_df[[fea1, fea2, 'label']].copy()\n",
    "            temp_pivot_table = pd.pivot_table(temp_df, index=[fea1, fea2], values='label', aggfunc={len, np.mean, np.sum})\n",
    "            temp_pivot_table.reset_index(inplace=True)\n",
    "            temp_pivot_table.rename(columns={'len':fea1 + '_' + fea2 + '_count', 'mean':fea1 + '_' + fea2 + '_rate', 'sum':fea1 + '_' + fea2 + '_click_number'}, inplace=True)\n",
    "            alpha, beta = getBayesSmoothParam(temp_pivot_table[fea1 + '_' + fea2 + '_rate'])\n",
    "            temp_pivot_table[fea1 + '_' + fea2 + '_rate'] = (temp_pivot_table[fea1 + '_' + fea2 + '_click_number'] + alpha) / (temp_pivot_table[fea1 + '_' + fea2 + '_count'] + alpha + beta)\n",
    "#             del temp_pivot_table[fea1 + '_' + fea2 + '_click_number']\n",
    "            print(fea1 + '_' + fea2 + ' : finish!!!')\n",
    "            valid_df = pd.merge(valid_df, temp_pivot_table, on=[fea1, fea2], how='left')\n",
    "            train_df = train_temp_df\n",
    "            train_df.sort_index(by='index', ascending=True, inplace=True)\n",
    "    return train_df, valid_df\n",
    "\n",
    "jiaocha_fea_set = ['prefix', 'title', 'tag']\n",
    "train_df, test_df = get_jiaocha_dimension_rate_feature(train_df, test_df, jiaocha_fea_set)\n",
    "print(train_df.head())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 统计一些是否交叉的特征\n",
    "def get_is_title_in_query_feature(df):\n",
    "    x = df['title']\n",
    "    y = df['query_prediction_keys']\n",
    "    is_title_in_query = np.nan\n",
    "    if len(y) > 0:\n",
    "        if str(x) in str(y):\n",
    "            is_title_in_query = 1\n",
    "        else:\n",
    "            is_title_in_query = 0\n",
    "    return is_title_in_query\n",
    "\n",
    "def get_is_prefix_in_title_feature(df):\n",
    "    x = df['prefix']\n",
    "    y = df['title']\n",
    "    is_prefix_in_title = np.nan\n",
    "    if str(x) in str(y):\n",
    "        is_prefix_in_title = 1\n",
    "    else:\n",
    "        is_prefix_in_title = 0\n",
    "    return is_prefix_in_title\n",
    "\n",
    "train_df['is_title_in_query'] = train_df[['title', 'query_prediction_keys']].apply(get_is_title_in_query_feature, axis = 1)\n",
    "test_df['is_title_in_query'] = test_df[['title', 'query_prediction_keys']].apply(get_is_title_in_query_feature, axis = 1)\n",
    "\n",
    "train_df['is_prefix_in_title'] = train_df[['prefix', 'title']].apply(get_is_prefix_in_title_feature, axis = 1)\n",
    "test_df['is_prefix_in_title'] = test_df[['prefix', 'title']].apply(get_is_prefix_in_title_feature, axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index prefix                                   query_prediction  \\\n",
      "0      0     逆行  {\"逆行我的1997\": \"0.008\", \"逆行性遗忘\": \"0.040\", \"逆行遗忘症...   \n",
      "1      1      攸  {\"攸县\": \"0.065\", \"攸妍化妆品怎么样\": \"0.021\", \"攸\": \"0.0...   \n",
      "2      2    四物汤  {\"四物汤处方\": \"0.094\", \"四物汤的功效与作用\": \"0.293\", \"四物汤怎...   \n",
      "3      3      优  {\"优速快递查询\": \"0.012\", \"优信二手车\": \"0.016\", \"优速快递\": ...   \n",
      "4      4     夫西  {\"夫西地酸乳膏的作用\": \"0.055\", \"夫西地酸乳膏\": \"0.612\", \"夫西地...   \n",
      "\n",
      "         title tag  label                              query_prediction_dict  \\\n",
      "0  右拐拐到逆行车道扣几分  知道      1  {'逆行我的1997': '0.008', '逆行性遗忘': '0.040', '逆行遗忘症...   \n",
      "1            攸  百科      0  {'攸县': '0.065', '攸妍化妆品怎么样': '0.021', '攸': '0.0...   \n",
      "2          四物汤  百科      1  {'四物汤处方': '0.094', '四物汤的功效与作用': '0.293', '四物汤怎...   \n",
      "3           优酷  百科      0  {'优速快递查询': '0.012', '优信二手车': '0.016', '优速快递': ...   \n",
      "4   夫西地酸乳膏有激素吗  健康      0  {'夫西地酸乳膏的作用': '0.055', '夫西地酸乳膏': '0.612', '夫西地...   \n",
      "\n",
      "                               query_prediction_keys  \\\n",
      "0  [逆行我的1997, 逆行性遗忘, 逆行遗忘症, 逆行性遗忘症, 逆行扣几分, 逆行我的19...   \n",
      "1  [攸县, 攸妍化妆品怎么样, 攸, 攸县政府门户网站, 攸妍, 攸怎么读, 攸怎么读音, 攸...   \n",
      "2  [四物汤处方, 四物汤的功效与作用, 四物汤怎么煮, 四物汤怎么熬, 四物汤_百度百科, 四...   \n",
      "3  [优速快递查询, 优信二手车, 优速快递, 优, 优漫画, 优酷下载, 优速快递单号查询, ...   \n",
      "4  [夫西地酸乳膏的作用, 夫西地酸乳膏, 夫西地酸祛痘更严重了, 夫西, 夫西地酸乳膏祛痘, ...   \n",
      "\n",
      "                             query_prediction_values  query_prediction_number  \\\n",
      "0  [0.008, 0.04, 0.008, 0.039, 0.07, 0.008, 0.648...                       11   \n",
      "1  [0.065, 0.021, 0.0, 0.024, 0.015, 0.079, 0.04,...                       11   \n",
      "2  [0.094, 0.293, 0.006, 0.005, 0.008, 0.005, 0.0...                       11   \n",
      "3  [0.012, 0.016, 0.01, 0.0, 0.012, 0.016, 0.026,...                       11   \n",
      "4  [0.055, 0.612, 0.012, 0.002, 0.011, 0.017, 0.0...                       11   \n",
      "\n",
      "               ...               is_title_in_query  is_prefix_in_title  \\\n",
      "0              ...                             0.0                   1   \n",
      "1              ...                             1.0                   1   \n",
      "2              ...                             1.0                   1   \n",
      "3              ...                             1.0                   1   \n",
      "4              ...                             1.0                   1   \n",
      "\n",
      "   title_tag_types  prefix_tag_types  tag_title_types  tag_prefix_types  \\\n",
      "0                1                 4            24823             30411   \n",
      "1                1                 3           167774            138097   \n",
      "2                1                 3           167774            138097   \n",
      "3                2                 3           167774            138097   \n",
      "4                1                 2            58982             47609   \n",
      "\n",
      "   title_prefix_types  prefix_title_types  tag_query_prediction_types  \\\n",
      "0                   1                   4                       47949   \n",
      "1                   1                   3                      220606   \n",
      "2                   2                   3                      220606   \n",
      "3                   3                   5                      220606   \n",
      "4                   2                   2                       74603   \n",
      "\n",
      "   title_query_prediction_types  \n",
      "0                             2  \n",
      "1                             2  \n",
      "2                             4  \n",
      "3                             5  \n",
      "4                             4  \n",
      "\n",
      "[5 rows x 45 columns]\n"
     ]
    }
   ],
   "source": [
    "# 统计一些交叉种类特征\n",
    "def get_jiaocha_type_feature(train_df, valid_df, jiaocha_type_list):\n",
    "    for jiaocha_type in jiaocha_type_list:\n",
    "        fea1 = jiaocha_type[0]\n",
    "        fea2 = jiaocha_type[1]\n",
    "        temp_df = pd.concat([train_df, valid_df])\n",
    "        temp_pivot_table = pd.pivot_table(temp_df[[fea1, fea2, 'label']], index=[fea1, fea2], values='label', aggfunc=len)\n",
    "        temp_pivot_table.reset_index(inplace=True)\n",
    "        final_pivot_table = pd.pivot_table(temp_pivot_table, index=fea1, values=fea2, aggfunc=len)\n",
    "        final_pivot_table.reset_index(inplace=True)\n",
    "        final_pivot_table.rename(columns={fea2 : fea1 + '_' + fea2 + '_types'}, inplace=True)\n",
    "        train_df = pd.merge(train_df, final_pivot_table[[fea1, fea1 + '_' + fea2 + '_types']], on=fea1, how='left')\n",
    "        valid_df = pd.merge(valid_df, final_pivot_table[[fea1, fea1 + '_' + fea2 + '_types']], on=fea1, how='left')\n",
    "    return train_df, valid_df\n",
    "\n",
    "jiaocha_type_list = [['title', 'tag'], ['prefix', 'tag'], ['tag', 'title'], ['tag', 'prefix'], \n",
    "                     ['title', 'prefix'], ['prefix', 'title'], ['tag', 'query_prediction'], ['title', 'query_prediction']]\n",
    "train_df, test_df = get_jiaocha_type_feature(train_df, test_df, jiaocha_type_list)\n",
    "print(train_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index prefix                                   query_prediction  \\\n",
      "0      0     逆行  {\"逆行我的1997\": \"0.008\", \"逆行性遗忘\": \"0.040\", \"逆行遗忘症...   \n",
      "1      1      攸  {\"攸县\": \"0.065\", \"攸妍化妆品怎么样\": \"0.021\", \"攸\": \"0.0...   \n",
      "2      2    四物汤  {\"四物汤处方\": \"0.094\", \"四物汤的功效与作用\": \"0.293\", \"四物汤怎...   \n",
      "3      3      优  {\"优速快递查询\": \"0.012\", \"优信二手车\": \"0.016\", \"优速快递\": ...   \n",
      "4      4     夫西  {\"夫西地酸乳膏的作用\": \"0.055\", \"夫西地酸乳膏\": \"0.612\", \"夫西地...   \n",
      "\n",
      "         title tag  label                              query_prediction_dict  \\\n",
      "0  右拐拐到逆行车道扣几分  知道      1  {'逆行我的1997': '0.008', '逆行性遗忘': '0.040', '逆行遗忘症...   \n",
      "1            攸  百科      0  {'攸县': '0.065', '攸妍化妆品怎么样': '0.021', '攸': '0.0...   \n",
      "2          四物汤  百科      1  {'四物汤处方': '0.094', '四物汤的功效与作用': '0.293', '四物汤怎...   \n",
      "3           优酷  百科      0  {'优速快递查询': '0.012', '优信二手车': '0.016', '优速快递': ...   \n",
      "4   夫西地酸乳膏有激素吗  健康      0  {'夫西地酸乳膏的作用': '0.055', '夫西地酸乳膏': '0.612', '夫西地...   \n",
      "\n",
      "                               query_prediction_keys  \\\n",
      "0  [逆行我的1997, 逆行性遗忘, 逆行遗忘症, 逆行性遗忘症, 逆行扣几分, 逆行我的19...   \n",
      "1  [攸县, 攸妍化妆品怎么样, 攸, 攸县政府门户网站, 攸妍, 攸怎么读, 攸怎么读音, 攸...   \n",
      "2  [四物汤处方, 四物汤的功效与作用, 四物汤怎么煮, 四物汤怎么熬, 四物汤_百度百科, 四...   \n",
      "3  [优速快递查询, 优信二手车, 优速快递, 优, 优漫画, 优酷下载, 优速快递单号查询, ...   \n",
      "4  [夫西地酸乳膏的作用, 夫西地酸乳膏, 夫西地酸祛痘更严重了, 夫西, 夫西地酸乳膏祛痘, ...   \n",
      "\n",
      "                             query_prediction_values  query_prediction_number  \\\n",
      "0  [0.008, 0.04, 0.008, 0.039, 0.07, 0.008, 0.648...                       11   \n",
      "1  [0.065, 0.021, 0.0, 0.024, 0.015, 0.079, 0.04,...                       11   \n",
      "2  [0.094, 0.293, 0.006, 0.005, 0.008, 0.005, 0.0...                       11   \n",
      "3  [0.012, 0.016, 0.01, 0.0, 0.012, 0.016, 0.026,...                       11   \n",
      "4  [0.055, 0.612, 0.012, 0.002, 0.011, 0.017, 0.0...                       11   \n",
      "\n",
      "        ...        prefix_len  title_len  query_prediction_key_len_max  \\\n",
      "0       ...                 2         11                          13.0   \n",
      "1       ...                 1          1                          10.0   \n",
      "2       ...                 3          3                          10.0   \n",
      "3       ...                 1          2                           8.0   \n",
      "4       ...                 2         10                          10.0   \n",
      "\n",
      "   query_prediction_key_len_min  query_prediction_key_len_mean  \\\n",
      "0                           2.0                       6.454545   \n",
      "1                           1.0                       5.181818   \n",
      "2                           3.0                       6.818182   \n",
      "3                           1.0                       4.272727   \n",
      "4                           2.0                       7.454545   \n",
      "\n",
      "   query_prediction_key_len_std  len_title-prefix  len_prefix/title  \\\n",
      "0                      2.675262                 9          0.181818   \n",
      "1                      2.757409                 0          1.000000   \n",
      "2                      2.124240                 0          1.000000   \n",
      "3                      2.004128                 1          0.500000   \n",
      "4                      2.499587                 8          0.200000   \n",
      "\n",
      "   len_mean-title  len_mean/title  \n",
      "0       -4.545455        0.586777  \n",
      "1        4.181818        5.181818  \n",
      "2        3.818182        2.272727  \n",
      "3        2.272727        2.136364  \n",
      "4       -2.545455        0.745455  \n",
      "\n",
      "[5 rows x 55 columns]\n"
     ]
    }
   ],
   "source": [
    "def get_key_len_list(x):\n",
    "    return_list = []\n",
    "    for temp in x:\n",
    "        return_list.append(len(temp))\n",
    "    return return_list\n",
    "\n",
    "# 统计一些跟字符串长度相关的特征\n",
    "def get_string_len_feature(df):\n",
    "    df['prefix_len'] = df['prefix'].map(lambda x : np.nan if x is np.nan else len(x))\n",
    "    df['title_len'] = df['title'].map(lambda x : np.nan if x is np.nan else len(x))\n",
    "    df['query_prediction_key_len_list'] = df['query_prediction_keys'].map(lambda x : get_key_len_list(x))\n",
    "    df['query_prediction_key_len_max'] = df['query_prediction_key_len_list'].map(lambda x : np.nan if len(x) == 0 else np.max(x))\n",
    "    df['query_prediction_key_len_min'] = df['query_prediction_key_len_list'].map(lambda x : np.nan if len(x) == 0 else np.min(x))\n",
    "    df['query_prediction_key_len_mean'] = df['query_prediction_key_len_list'].map(lambda x : np.nan if len(x) == 0 else np.mean(x))\n",
    "    df['query_prediction_key_len_std'] = df['query_prediction_key_len_list'].map(lambda x : np.nan if len(x) == 0 else np.std(x))\n",
    "    df['len_title-prefix'] = df['title_len'] - df['prefix_len']\n",
    "    df['len_prefix/title'] = df['prefix_len'] / df['title_len']\n",
    "    df['len_mean-title'] = df['query_prediction_key_len_mean'] - df['title_len']\n",
    "    df['len_mean/title'] = df['query_prediction_key_len_mean'] / df['title_len']\n",
    "    del df['query_prediction_key_len_list']\n",
    "    return df\n",
    "\n",
    "train_df = get_string_len_feature(train_df)\n",
    "test_df = get_string_len_feature(test_df)\n",
    "print(train_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index prefix                                   query_prediction  \\\n",
      "0      0     逆行  {\"逆行我的1997\": \"0.008\", \"逆行性遗忘\": \"0.040\", \"逆行遗忘症...   \n",
      "1      1      攸  {\"攸县\": \"0.065\", \"攸妍化妆品怎么样\": \"0.021\", \"攸\": \"0.0...   \n",
      "2      2    四物汤  {\"四物汤处方\": \"0.094\", \"四物汤的功效与作用\": \"0.293\", \"四物汤怎...   \n",
      "3      3      优  {\"优速快递查询\": \"0.012\", \"优信二手车\": \"0.016\", \"优速快递\": ...   \n",
      "4      4     夫西  {\"夫西地酸乳膏的作用\": \"0.055\", \"夫西地酸乳膏\": \"0.612\", \"夫西地...   \n",
      "\n",
      "         title tag  label                              query_prediction_dict  \\\n",
      "0  右拐拐到逆行车道扣几分  知道      1  {'逆行我的1997': '0.008', '逆行性遗忘': '0.040', '逆行遗忘症...   \n",
      "1            攸  百科      0  {'攸县': '0.065', '攸妍化妆品怎么样': '0.021', '攸': '0.0...   \n",
      "2          四物汤  百科      1  {'四物汤处方': '0.094', '四物汤的功效与作用': '0.293', '四物汤怎...   \n",
      "3           优酷  百科      0  {'优速快递查询': '0.012', '优信二手车': '0.016', '优速快递': ...   \n",
      "4   夫西地酸乳膏有激素吗  健康      0  {'夫西地酸乳膏的作用': '0.055', '夫西地酸乳膏': '0.612', '夫西地...   \n",
      "\n",
      "                               query_prediction_keys  \\\n",
      "0  [逆行我的1997, 逆行性遗忘, 逆行遗忘症, 逆行性遗忘症, 逆行扣几分, 逆行我的19...   \n",
      "1  [攸县, 攸妍化妆品怎么样, 攸, 攸县政府门户网站, 攸妍, 攸怎么读, 攸怎么读音, 攸...   \n",
      "2  [四物汤处方, 四物汤的功效与作用, 四物汤怎么煮, 四物汤怎么熬, 四物汤_百度百科, 四...   \n",
      "3  [优速快递查询, 优信二手车, 优速快递, 优, 优漫画, 优酷下载, 优速快递单号查询, ...   \n",
      "4  [夫西地酸乳膏的作用, 夫西地酸乳膏, 夫西地酸祛痘更严重了, 夫西, 夫西地酸乳膏祛痘, ...   \n",
      "\n",
      "                             query_prediction_values  query_prediction_number  \\\n",
      "0  [0.008, 0.04, 0.008, 0.039, 0.07, 0.008, 0.648...                       11   \n",
      "1  [0.065, 0.021, 0.0, 0.024, 0.015, 0.079, 0.04,...                       11   \n",
      "2  [0.094, 0.293, 0.006, 0.005, 0.008, 0.005, 0.0...                       11   \n",
      "3  [0.012, 0.016, 0.01, 0.0, 0.012, 0.016, 0.026,...                       11   \n",
      "4  [0.055, 0.612, 0.012, 0.002, 0.011, 0.017, 0.0...                       11   \n",
      "\n",
      "            ...             query_prediction_key_len_max  \\\n",
      "0           ...                                     13.0   \n",
      "1           ...                                     10.0   \n",
      "2           ...                                     10.0   \n",
      "3           ...                                      8.0   \n",
      "4           ...                                     10.0   \n",
      "\n",
      "   query_prediction_key_len_min  query_prediction_key_len_mean  \\\n",
      "0                           2.0                       6.454545   \n",
      "1                           1.0                       5.181818   \n",
      "2                           3.0                       6.818182   \n",
      "3                           1.0                       4.272727   \n",
      "4                           2.0                       7.454545   \n",
      "\n",
      "   query_prediction_key_len_std  len_title-prefix  len_prefix/title  \\\n",
      "0                      2.675262                 9          0.181818   \n",
      "1                      2.757409                 0          1.000000   \n",
      "2                      2.124240                 0          1.000000   \n",
      "3                      2.004128                 1          0.500000   \n",
      "4                      2.499587                 8          0.200000   \n",
      "\n",
      "   len_mean-title  len_mean/title  title_prefix_leven  title_prefix_leven_rate  \n",
      "0       -4.545455        0.586777                   9                 0.642857  \n",
      "1        4.181818        5.181818                   0                 0.000000  \n",
      "2        3.818182        2.272727                   0                 0.000000  \n",
      "3        2.272727        2.136364                   1                 0.200000  \n",
      "4       -2.545455        0.745455                   8                 0.615385  \n",
      "\n",
      "[5 rows x 57 columns]\n"
     ]
    }
   ],
   "source": [
    "# 统计title跟prefix的编辑距离\n",
    "def get_title_prefix_levenshtein_distance(df):\n",
    "    title = str(df['title'])\n",
    "    prefix = str(df['prefix'])\n",
    "    return Levenshtein.distance(title, prefix)\n",
    "\n",
    "def get_title_prefix_levenshtein_distance_rate(df):\n",
    "    title_prefix_leven = df['title_prefix_leven']\n",
    "    title = df['title']\n",
    "    return (title_prefix_leven / (len(title) + 3))\n",
    "\n",
    "train_df['title_prefix_leven'] = train_df[['title', 'prefix']].apply(get_title_prefix_levenshtein_distance, axis=1)\n",
    "test_df['title_prefix_leven'] = test_df[['title', 'prefix']].apply(get_title_prefix_levenshtein_distance, axis=1)\n",
    "\n",
    "train_df['title_prefix_leven_rate'] = train_df[['title', 'title_prefix_leven']].apply(get_title_prefix_levenshtein_distance_rate, axis=1)\n",
    "test_df['title_prefix_leven_rate'] = test_df[['title', 'title_prefix_leven']].apply(get_title_prefix_levenshtein_distance_rate, axis=1)\n",
    "\n",
    "print(train_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index prefix                                   query_prediction  \\\n",
      "0      0     逆行  {\"逆行我的1997\": \"0.008\", \"逆行性遗忘\": \"0.040\", \"逆行遗忘症...   \n",
      "1      1      攸  {\"攸县\": \"0.065\", \"攸妍化妆品怎么样\": \"0.021\", \"攸\": \"0.0...   \n",
      "2      2    四物汤  {\"四物汤处方\": \"0.094\", \"四物汤的功效与作用\": \"0.293\", \"四物汤怎...   \n",
      "3      3      优  {\"优速快递查询\": \"0.012\", \"优信二手车\": \"0.016\", \"优速快递\": ...   \n",
      "4      4     夫西  {\"夫西地酸乳膏的作用\": \"0.055\", \"夫西地酸乳膏\": \"0.612\", \"夫西地...   \n",
      "\n",
      "         title tag  label                              query_prediction_dict  \\\n",
      "0  右拐拐到逆行车道扣几分  知道      1  {'逆行我的1997': '0.008', '逆行性遗忘': '0.040', '逆行遗忘症...   \n",
      "1            攸  百科      0  {'攸县': '0.065', '攸妍化妆品怎么样': '0.021', '攸': '0.0...   \n",
      "2          四物汤  百科      1  {'四物汤处方': '0.094', '四物汤的功效与作用': '0.293', '四物汤怎...   \n",
      "3           优酷  百科      0  {'优速快递查询': '0.012', '优信二手车': '0.016', '优速快递': ...   \n",
      "4   夫西地酸乳膏有激素吗  健康      0  {'夫西地酸乳膏的作用': '0.055', '夫西地酸乳膏': '0.612', '夫西地...   \n",
      "\n",
      "                               query_prediction_keys  \\\n",
      "0  [逆行我的1997, 逆行性遗忘, 逆行遗忘症, 逆行性遗忘症, 逆行扣几分, 逆行我的19...   \n",
      "1  [攸县, 攸妍化妆品怎么样, 攸, 攸县政府门户网站, 攸妍, 攸怎么读, 攸怎么读音, 攸...   \n",
      "2  [四物汤处方, 四物汤的功效与作用, 四物汤怎么煮, 四物汤怎么熬, 四物汤_百度百科, 四...   \n",
      "3  [优速快递查询, 优信二手车, 优速快递, 优, 优漫画, 优酷下载, 优速快递单号查询, ...   \n",
      "4  [夫西地酸乳膏的作用, 夫西地酸乳膏, 夫西地酸祛痘更严重了, 夫西, 夫西地酸乳膏祛痘, ...   \n",
      "\n",
      "                             query_prediction_values  query_prediction_number  \\\n",
      "0  [0.008, 0.04, 0.008, 0.039, 0.07, 0.008, 0.648...                       11   \n",
      "1  [0.065, 0.021, 0.0, 0.024, 0.015, 0.079, 0.04,...                       11   \n",
      "2  [0.094, 0.293, 0.006, 0.005, 0.008, 0.005, 0.0...                       11   \n",
      "3  [0.012, 0.016, 0.01, 0.0, 0.012, 0.016, 0.026,...                       11   \n",
      "4  [0.055, 0.612, 0.012, 0.002, 0.011, 0.017, 0.0...                       11   \n",
      "\n",
      "           ...            len_mean-title  len_mean/title  title_prefix_leven  \\\n",
      "0          ...                 -4.545455        0.586777                   9   \n",
      "1          ...                  4.181818        5.181818                   0   \n",
      "2          ...                  3.818182        2.272727                   0   \n",
      "3          ...                  2.272727        2.136364                   1   \n",
      "4          ...                 -2.545455        0.745455                   8   \n",
      "\n",
      "   title_prefix_leven_rate                             title_query_leven_list  \\\n",
      "0                 0.642857  [0.08, 0.36, 0.07200000000000001, 0.351, 0.420...   \n",
      "1                 0.000000  [0.065, 0.14700000000000002, 0.0, 0.168, 0.015...   \n",
      "2                 0.000000  [0.188, 1.758, 0.018000000000000002, 0.015, 0....   \n",
      "3                 0.200000  [0.06, 0.064, 0.03, 0.0, 0.024, 0.032, 0.182, ...   \n",
      "4                 0.615385  [0.22, 2.448, 0.07200000000000001, 0.016, 0.04...   \n",
      "\n",
      "   title_query_leven_sum  title_query_leven_max  title_query_leven_min  \\\n",
      "0                  7.944                  5.832                  0.064   \n",
      "1                  1.234                  0.237                  0.000   \n",
      "2                  2.771                  1.758                  0.000   \n",
      "3                  0.533                  0.182                  0.000   \n",
      "4                  3.497                  2.448                  0.000   \n",
      "\n",
      "   title_query_leven_mean  title_query_leven_std  \n",
      "0                0.722182               1.623287  \n",
      "1                0.112182               0.067950  \n",
      "2                0.251909               0.499171  \n",
      "3                0.048455               0.047672  \n",
      "4                0.317909               0.681203  \n",
      "\n",
      "[5 rows x 63 columns]\n"
     ]
    }
   ],
   "source": [
    "# 统计title跟query_prediction编辑距离相关的特征\n",
    "def get_title_query_levenshtein_distance_list(df):\n",
    "    query_keys_list = df['query_prediction_keys']\n",
    "    query_values_list = df['query_prediction_values']\n",
    "    title = df['title']\n",
    "    return_list = list()\n",
    "    for i in range(len(query_keys_list)):\n",
    "        distance = Levenshtein.distance(title, query_keys_list[i])\n",
    "        return_list.append(distance * query_values_list[i])\n",
    "    return return_list\n",
    "\n",
    "def get_title_query_levenshtein_distance_feature(df):\n",
    "    df['title_query_leven_list'] = df[['query_prediction_keys', 'query_prediction_values', 'title']].apply(get_title_query_levenshtein_distance_list, axis=1)\n",
    "    df['title_query_leven_sum'] = df['title_query_leven_list'].map(lambda x : np.nan if len(x) == 0 else np.sum(x))\n",
    "    df['title_query_leven_max'] = df['title_query_leven_list'].map(lambda x : np.nan if len(x) == 0 else np.max(x))\n",
    "    df['title_query_leven_min'] = df['title_query_leven_list'].map(lambda x : np.nan if len(x) == 0 else np.min(x))\n",
    "    df['title_query_leven_mean'] = df['title_query_leven_list'].map(lambda x : np.nan if len(x) == 0 else np.mean(x))\n",
    "    df['title_query_leven_std'] = df['title_query_leven_list'].map(lambda x : np.nan if len(x) == 0 else np.std(x))\n",
    "    return df\n",
    "\n",
    "train_df = get_title_query_levenshtein_distance_feature(train_df)\n",
    "test_df = get_title_query_levenshtein_distance_feature(test_df)\n",
    "print(train_df.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Dump cache file failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/jieba/__init__.py\", line 152, in initialize\n",
      "    _replace_file(fpath, cache_file)\n",
      "PermissionError: [Errno 1] Operation not permitted: '/tmp/tmpchguqi5l' -> '/tmp/jieba.cache'\n",
      "Loading model cost 1.147 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index prefix                                   query_prediction  \\\n",
      "0      0     逆行  {\"逆行我的1997\": \"0.008\", \"逆行性遗忘\": \"0.040\", \"逆行遗忘症...   \n",
      "1      1      攸  {\"攸县\": \"0.065\", \"攸妍化妆品怎么样\": \"0.021\", \"攸\": \"0.0...   \n",
      "2      2    四物汤  {\"四物汤处方\": \"0.094\", \"四物汤的功效与作用\": \"0.293\", \"四物汤怎...   \n",
      "3      3      优  {\"优速快递查询\": \"0.012\", \"优信二手车\": \"0.016\", \"优速快递\": ...   \n",
      "4      4     夫西  {\"夫西地酸乳膏的作用\": \"0.055\", \"夫西地酸乳膏\": \"0.612\", \"夫西地...   \n",
      "\n",
      "         title tag  label                              query_prediction_dict  \\\n",
      "0  右拐拐到逆行车道扣几分  知道      1  {'逆行我的1997': '0.008', '逆行性遗忘': '0.040', '逆行遗忘症...   \n",
      "1            攸  百科      0  {'攸县': '0.065', '攸妍化妆品怎么样': '0.021', '攸': '0.0...   \n",
      "2          四物汤  百科      1  {'四物汤处方': '0.094', '四物汤的功效与作用': '0.293', '四物汤怎...   \n",
      "3           优酷  百科      0  {'优速快递查询': '0.012', '优信二手车': '0.016', '优速快递': ...   \n",
      "4   夫西地酸乳膏有激素吗  健康      0  {'夫西地酸乳膏的作用': '0.055', '夫西地酸乳膏': '0.612', '夫西地...   \n",
      "\n",
      "                               query_prediction_keys  \\\n",
      "0  [逆行我的1997, 逆行性遗忘, 逆行遗忘症, 逆行性遗忘症, 逆行扣几分, 逆行我的19...   \n",
      "1  [攸县, 攸妍化妆品怎么样, 攸, 攸县政府门户网站, 攸妍, 攸怎么读, 攸怎么读音, 攸...   \n",
      "2  [四物汤处方, 四物汤的功效与作用, 四物汤怎么煮, 四物汤怎么熬, 四物汤_百度百科, 四...   \n",
      "3  [优速快递查询, 优信二手车, 优速快递, 优, 优漫画, 优酷下载, 优速快递单号查询, ...   \n",
      "4  [夫西地酸乳膏的作用, 夫西地酸乳膏, 夫西地酸祛痘更严重了, 夫西, 夫西地酸乳膏祛痘, ...   \n",
      "\n",
      "                             query_prediction_values  query_prediction_number  \\\n",
      "0  [0.008, 0.04, 0.008, 0.039, 0.07, 0.008, 0.648...                       11   \n",
      "1  [0.065, 0.021, 0.0, 0.024, 0.015, 0.079, 0.04,...                       11   \n",
      "2  [0.094, 0.293, 0.006, 0.005, 0.008, 0.005, 0.0...                       11   \n",
      "3  [0.012, 0.016, 0.01, 0.0, 0.012, 0.016, 0.026,...                       11   \n",
      "4  [0.055, 0.612, 0.012, 0.002, 0.011, 0.017, 0.0...                       11   \n",
      "\n",
      "          ...          title_query_leven_sum  title_query_leven_max  \\\n",
      "0         ...                          7.944                  5.832   \n",
      "1         ...                          1.234                  0.237   \n",
      "2         ...                          2.771                  1.758   \n",
      "3         ...                          0.533                  0.182   \n",
      "4         ...                          3.497                  2.448   \n",
      "\n",
      "   title_query_leven_min  title_query_leven_mean  title_query_leven_std  \\\n",
      "0                  0.064                0.722182               1.623287   \n",
      "1                  0.000                0.112182               0.067950   \n",
      "2                  0.000                0.251909               0.499171   \n",
      "3                  0.000                0.048455               0.047672   \n",
      "4                  0.000                0.317909               0.681203   \n",
      "\n",
      "                      query_prediction_key_sentences  \\\n",
      "0  逆行我的1997逆行性遗忘逆行遗忘症逆行性遗忘症逆行扣几分逆行我的1997 白色米饭逆行诸天...   \n",
      "1  攸县攸妍化妆品怎么样攸攸县政府门户网站攸妍攸怎么读攸怎么读音攸县天气攸县公众信息网攸县天气预...   \n",
      "2  四物汤处方四物汤的功效与作用四物汤怎么煮四物汤怎么熬四物汤_百度百科四物汤的做法四物汤配方四...   \n",
      "3    优速快递查询优信二手车优速快递优优漫画优酷下载优速快递单号查询优酷app下载优酷优衣库优酷视频   \n",
      "4  夫西地酸乳膏的作用夫西地酸乳膏夫西地酸祛痘更严重了夫西夫西地酸乳膏祛痘夫西地酸乳膏多少钱夫西...   \n",
      "\n",
      "                    query_prediction_key_jieba_words  \\\n",
      "0  [逆行, 我, 的, 1997, 逆行, 性, 遗忘, 逆行, 遗忘症, 逆行, 性, 遗忘...   \n",
      "1  [攸县, 攸妍, 化妆品, 怎么样, 攸, 攸县, 政府, 门户网站, 攸妍, 攸, 怎么,...   \n",
      "2  [四物汤, 处方, 四物汤, 的, 功效, 与, 作用, 四物汤, 怎么, 煮, 四物汤, ...   \n",
      "3  [优速, 快递, 查询, 优信, 二手车, 优速, 快递, 优优, 漫画, 优酷, 下载, ...   \n",
      "4  [夫西, 地酸, 乳膏, 的, 作用, 夫西, 地酸, 乳膏, 夫西, 地酸, 祛痘, 更,...   \n",
      "\n",
      "                              query_prediction_words  \\\n",
      "0  [[逆行, 我, 的, 1997], [逆行, 性, 遗忘], [逆行, 遗忘症], [逆行...   \n",
      "1  [[攸县], [攸妍, 化妆品, 怎么样], [攸], [攸县, 政府, 门户网站], [攸...   \n",
      "2  [[四物汤, 处方], [四物汤, 的, 功效, 与, 作用], [四物汤, 怎么, 煮],...   \n",
      "3  [[优速, 快递, 查询], [优信, 二手车], [优速, 快递], [优], [优, 漫...   \n",
      "4  [[夫西, 地酸, 乳膏, 的, 作用], [夫西, 地酸, 乳膏], [夫西, 地酸, 祛...   \n",
      "\n",
      "           title_jieba_words  prefix_jieba_words  \n",
      "0  [右, 拐拐, 到, 逆, 行车道, 扣, 几分]                [逆行]  \n",
      "1                        [攸]                 [攸]  \n",
      "2                      [四物汤]               [四物汤]  \n",
      "3                       [优酷]                 [优]  \n",
      "4     [夫西, 地酸, 乳膏, 有, 激素, 吗]                [夫西]  \n",
      "\n",
      "[5 rows x 68 columns]\n"
     ]
    }
   ],
   "source": [
    "#分词方法，调用结巴接口\n",
    "def jieba_seg_to_list(sentence, pos=False):\n",
    "    if not pos:\n",
    "        #不进行词性标注的分词方法\n",
    "        seg_list = jieba.cut(sentence)\n",
    "    else:\n",
    "        #进行词性标注的分词方法\n",
    "        seg_list = psg.cut(sentence)\n",
    "    return seg_list\n",
    "\n",
    "#去除干扰词\n",
    "def jieba_word_filter(seg_list, pos=False):\n",
    "    \n",
    "    filter_list = []\n",
    "    #根据pos参数选择是否词性过滤\n",
    "    #不进行词性过滤，则将词性都标记为n，表示全部保留\n",
    "    for seg in seg_list:\n",
    "        if not pos:\n",
    "            word = seg\n",
    "            flag = 'n'\n",
    "        else:\n",
    "            word = seg.word\n",
    "            flag = seg.flag\n",
    "        if not flag.startswith('n'):\n",
    "            continue\n",
    "        filter_list.append(word)\n",
    "    return filter_list\n",
    "\n",
    "def jieba_word_deal(sentence, pos=False):\n",
    "    #调用上面方式对数据集进行处理，处理后的每条数据仅保留非干扰词\n",
    "    seg_list = jieba_seg_to_list(sentence, pos)\n",
    "    filter_list = jieba_word_filter(seg_list, pos)\n",
    "    return filter_list\n",
    "\n",
    "def get_prefix_prediction_key_sentences(x):\n",
    "    prefix_prediction_key_sentences = \"\"\n",
    "    for temp in x:\n",
    "        if len(prefix_prediction_key_sentences) > 0:\n",
    "            prefix_prediction_key_sentences = prefix_prediction_key_sentences + temp\n",
    "        else:\n",
    "            prefix_prediction_key_sentences = temp\n",
    "    return prefix_prediction_key_sentences\n",
    "\n",
    "def get_max_query_key_sentences(x):\n",
    "    if len(x) == 0:\n",
    "        return \"\"\n",
    "    else:\n",
    "        return max(x, key=x.get)\n",
    "\n",
    "def get_jieba_word(df):\n",
    "    df['query_prediction_key_sentences'] = df['query_prediction_keys'].map(lambda x : get_prefix_prediction_key_sentences(x))\n",
    "#     df['query_prediction_key_sentences'] = df['query_prediction_dict'].map(lambda x : get_max_query_key_sentences(x))\n",
    "    df['query_prediction_key_jieba_words'] = df['query_prediction_key_sentences'].map(lambda x : jieba_word_deal(x, False))\n",
    "    df['query_prediction_words'] = df['query_prediction_keys'].map(lambda x : [jieba_word_deal(j, False) for j in x] if len(x) > 0 else np.nan)\n",
    "    df['title_jieba_words'] = df['title'].map(lambda x : jieba_word_deal(x, False))\n",
    "    df['prefix_jieba_words'] = df['prefix'].map(lambda x : jieba_word_deal(x, False))\n",
    "#     del df['query_prediction_key_sentences']\n",
    "    return df\n",
    "\n",
    "train_df = get_jieba_word(train_df)\n",
    "test_df = get_jieba_word(test_df)\n",
    "print(train_df.head())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_t_word_match finish!!!\n",
      "q_t_jaccard finish!!!\n",
      "q_t_common_words finish!!!\n",
      "q_t_total_unique_words finish!!!\n",
      "q_t_wc_diff finish!!!\n",
      "q_t_wc_ratio finish!!!\n",
      "q_t_wc_diff_unique finish!!!\n",
      "q_t_wc_ratio_unique finish!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:67: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_t_tfidf_word_match_share finish!!!\n",
      "p_t_word_match finish!!!\n",
      "p_t_jaccard finish!!!\n",
      "p_t_common_words finish!!!\n",
      "p_t_total_unique_words finish!!!\n",
      "p_t_wc_diff finish!!!\n",
      "p_t_wc_ratio finish!!!\n",
      "p_t_wc_diff_unique finish!!!\n",
      "p_t_wc_ratio_unique finish!!!\n",
      "p_t_tfidf_word_match_share finish!!!\n",
      "p_q_word_match finish!!!\n",
      "p_q_jaccard finish!!!\n",
      "p_q_common_words finish!!!\n",
      "p_q_total_unique_words finish!!!\n",
      "p_q_wc_diff finish!!!\n",
      "p_q_wc_ratio finish!!!\n",
      "p_q_wc_diff_unique finish!!!\n",
      "p_q_wc_ratio_unique finish!!!\n",
      "p_q_tfidf_word_match_share finish!!!\n",
      "   index prefix                                   query_prediction  \\\n",
      "0      0     逆行  {\"逆行我的1997\": \"0.008\", \"逆行性遗忘\": \"0.040\", \"逆行遗忘症...   \n",
      "1      1      攸  {\"攸县\": \"0.065\", \"攸妍化妆品怎么样\": \"0.021\", \"攸\": \"0.0...   \n",
      "2      2    四物汤  {\"四物汤处方\": \"0.094\", \"四物汤的功效与作用\": \"0.293\", \"四物汤怎...   \n",
      "3      3      优  {\"优速快递查询\": \"0.012\", \"优信二手车\": \"0.016\", \"优速快递\": ...   \n",
      "4      4     夫西  {\"夫西地酸乳膏的作用\": \"0.055\", \"夫西地酸乳膏\": \"0.612\", \"夫西地...   \n",
      "\n",
      "         title tag  label                              query_prediction_dict  \\\n",
      "0  右拐拐到逆行车道扣几分  知道      1  {'逆行我的1997': '0.008', '逆行性遗忘': '0.040', '逆行遗忘症...   \n",
      "1            攸  百科      0  {'攸县': '0.065', '攸妍化妆品怎么样': '0.021', '攸': '0.0...   \n",
      "2          四物汤  百科      1  {'四物汤处方': '0.094', '四物汤的功效与作用': '0.293', '四物汤怎...   \n",
      "3           优酷  百科      0  {'优速快递查询': '0.012', '优信二手车': '0.016', '优速快递': ...   \n",
      "4   夫西地酸乳膏有激素吗  健康      0  {'夫西地酸乳膏的作用': '0.055', '夫西地酸乳膏': '0.612', '夫西地...   \n",
      "\n",
      "                               query_prediction_keys  \\\n",
      "0  [逆行我的1997, 逆行性遗忘, 逆行遗忘症, 逆行性遗忘症, 逆行扣几分, 逆行我的19...   \n",
      "1  [攸县, 攸妍化妆品怎么样, 攸, 攸县政府门户网站, 攸妍, 攸怎么读, 攸怎么读音, 攸...   \n",
      "2  [四物汤处方, 四物汤的功效与作用, 四物汤怎么煮, 四物汤怎么熬, 四物汤_百度百科, 四...   \n",
      "3  [优速快递查询, 优信二手车, 优速快递, 优, 优漫画, 优酷下载, 优速快递单号查询, ...   \n",
      "4  [夫西地酸乳膏的作用, 夫西地酸乳膏, 夫西地酸祛痘更严重了, 夫西, 夫西地酸乳膏祛痘, ...   \n",
      "\n",
      "                             query_prediction_values  query_prediction_number  \\\n",
      "0  [0.008, 0.04, 0.008, 0.039, 0.07, 0.008, 0.648...                       11   \n",
      "1  [0.065, 0.021, 0.0, 0.024, 0.015, 0.079, 0.04,...                       11   \n",
      "2  [0.094, 0.293, 0.006, 0.005, 0.008, 0.005, 0.0...                       11   \n",
      "3  [0.012, 0.016, 0.01, 0.0, 0.012, 0.016, 0.026,...                       11   \n",
      "4  [0.055, 0.612, 0.012, 0.002, 0.011, 0.017, 0.0...                       11   \n",
      "\n",
      "              ...              p_t_tfidf_word_match_share  p_q_word_match  \\\n",
      "0             ...                                0.000000        0.100000   \n",
      "1             ...                                1.000000        0.111111   \n",
      "2             ...                                1.000000        0.083333   \n",
      "3             ...                                0.000000        0.000000   \n",
      "4             ...                                0.475738        0.090909   \n",
      "\n",
      "   p_q_jaccard  p_q_common_words  p_q_total_unique_words  p_q_wc_diff  \\\n",
      "0     0.052632                 1                      19           38   \n",
      "1     0.058824                 1                      17           26   \n",
      "2     0.043478                 1                      23           36   \n",
      "3     0.000000                 0                      15           22   \n",
      "4     0.047619                 1                      21           40   \n",
      "\n",
      "   p_q_wc_ratio  p_q_wc_diff_unique  p_q_wc_ratio_unique  \\\n",
      "0          39.0                  18                 19.0   \n",
      "1          27.0                  16                 17.0   \n",
      "2          37.0                  22                 23.0   \n",
      "3          23.0                  13                 14.0   \n",
      "4          41.0                  20                 21.0   \n",
      "\n",
      "   p_q_tfidf_word_match_share  \n",
      "0                    0.169008  \n",
      "1                    0.254828  \n",
      "2                    0.262631  \n",
      "3                    0.000000  \n",
      "4                    0.173506  \n",
      "\n",
      "[5 rows x 95 columns]\n"
     ]
    }
   ],
   "source": [
    "def word_match_share(df):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in df[0]:\n",
    "        q1words[word] = 1\n",
    "    for word in df[1]:\n",
    "        q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
    "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
    "    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n",
    "    return R\n",
    "\n",
    "def jaccard(df):\n",
    "    wic = set(df[0]).intersection(set(df[1]))\n",
    "    uw = set(df[0]).union(df[1])\n",
    "    if len(uw) == 0:\n",
    "        uw = [1]\n",
    "    return (len(wic) / len(uw))\n",
    "\n",
    "def common_words(df):\n",
    "    return len(set(df[0]).intersection(set(df[1])))\n",
    "\n",
    "def total_unique_words(df):\n",
    "    return len(set(df[0]).union(df[1]))\n",
    "\n",
    "def wc_diff(df):\n",
    "    return abs(len(df[0]) - len(df[1]))\n",
    "\n",
    "def wc_ratio(df):\n",
    "    l1 = len(df[0])*1.0 \n",
    "    l2 = len(df[1])\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "\n",
    "def wc_diff_unique(df):\n",
    "    return abs(len(set(df[0])) - len(set(df[1])))\n",
    "    \n",
    "def wc_ratio_unique(df):\n",
    "    l1 = len(set(df[0])) * 1.0\n",
    "    l2 = len(set(df[1]))\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "    \n",
    "def tfidf_word_match_share(df, weights=None):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in df[0]:\n",
    "        q1words[word] = 1\n",
    "    for word in df[1]:\n",
    "        q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
    "    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "    R = np.sum(shared_weights) / np.sum(total_weights)\n",
    "    return R\n",
    "\n",
    "def deal_word_for_all(train_df, valid_df, fea1, fea2, func, colName):\n",
    "    train_df[colName] = train_df[[fea1, fea2]].apply(func, axis=1)\n",
    "    valid_df[colName] = valid_df[[fea1, fea2]].apply(func, axis=1)\n",
    "    print(colName + ' finish!!!')\n",
    "    return train_df, valid_df\n",
    "                   \n",
    "def get_weight(count, eps=10000, min_count=2):\n",
    "    if count < min_count:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 / (count + eps)\n",
    "\n",
    "def get_word_statistic_feature(train_df, valid_df, col_list):\n",
    "    df = pd.concat([train_df[['query_prediction_key_jieba_words', 'title_jieba_words', 'prefix_jieba_words']], valid_df[['query_prediction_key_jieba_words', 'title_jieba_words', 'prefix_jieba_words']]])\n",
    "    train_qs = pd.Series(df['query_prediction_key_jieba_words'].tolist() + df['title_jieba_words'].tolist() + df['prefix_jieba_words'].tolist())\n",
    "    words = [x for y in train_qs for x in y]\n",
    "    counts = Counter(words)\n",
    "    weights = {word: get_weight(count) for word, count in counts.items()}\n",
    "    for col in col_list:\n",
    "        fea1 = col[0]\n",
    "        fea2 = col[1]\n",
    "        train_df, valid_df = deal_word_for_all(train_df, valid_df, fea1, fea2, word_match_share, fea1[0] + '_' + fea2[0] + '_word_match')\n",
    "        train_df, valid_df = deal_word_for_all(train_df, valid_df, fea1, fea2, jaccard, fea1[0] + '_' + fea2[0] + '_jaccard')\n",
    "        train_df, valid_df = deal_word_for_all(train_df, valid_df, fea1, fea2, common_words, fea1[0] + '_' + fea2[0] + '_common_words')\n",
    "        train_df, valid_df = deal_word_for_all(train_df, valid_df, fea1, fea2, total_unique_words, fea1[0] + '_' + fea2[0] + '_total_unique_words')\n",
    "        train_df, valid_df = deal_word_for_all(train_df, valid_df, fea1, fea2, wc_diff, fea1[0] + '_' + fea2[0] + '_wc_diff')\n",
    "        train_df, valid_df = deal_word_for_all(train_df, valid_df, fea1, fea2, wc_ratio, fea1[0] + '_' + fea2[0] + '_wc_ratio')\n",
    "        train_df, valid_df = deal_word_for_all(train_df, valid_df, fea1, fea2, wc_diff_unique, fea1[0] + '_' + fea2[0] + '_wc_diff_unique')\n",
    "        train_df, valid_df = deal_word_for_all(train_df, valid_df, fea1, fea2, wc_ratio_unique, fea1[0] + '_' + fea2[0] + '_wc_ratio_unique')\n",
    "        f = functools.partial(tfidf_word_match_share, weights=weights)\n",
    "        train_df, valid_df = deal_word_for_all(train_df, valid_df, fea1, fea2, f, fea1[0] + '_' + fea2[0] + '_tfidf_word_match_share')\n",
    "    return train_df, valid_df\n",
    "\n",
    "col_list = [['query_prediction_key_jieba_words', 'title_jieba_words'], ['prefix_jieba_words', 'title_jieba_words'], ['prefix_jieba_words', 'query_prediction_key_jieba_words']]\n",
    "train_df, test_df = get_word_statistic_feature(train_df, test_df, col_list)\n",
    "print(train_df.head())\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set values for various parameters\n",
    "num_features = 500  # Word vector dimensionality                      \n",
    "min_word_count = 1  # Minimum word count                        \n",
    "num_workers = 20       # Number of threads to run in parallel\n",
    "context = 5          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "word2vec_df = pd.concat([train_df[['query_prediction_words', 'title_jieba_words', 'prefix_jieba_words', 'query_prediction_number']], test_df[['query_prediction_words', 'title_jieba_words', 'prefix_jieba_words', 'query_prediction_number']]])\n",
    "word2vec_df.reset_index(inplace=True)\n",
    "word2vec_list = word2vec_df['title_jieba_words'].tolist() + word2vec_df['prefix_jieba_words'].tolist() + [y for x in word2vec_df['query_prediction_words'][word2vec_df.query_prediction_number > 0] for y in x]\n",
    "model = word2vec.Word2Vec(word2vec_list, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "word_wv = model.wv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_w2v_array(word_list, word_wv, num_features):\n",
    "    word_vectors = np.zeros((len(word_list), num_features))\n",
    "    for i in range(len(word_list)):\n",
    "        word_vectors[i][:] = word_wv[str(word_list[i])]\n",
    "    mean_array = np.mean(word_vectors, axis=0)\n",
    "    return mean_array\n",
    "\n",
    "train_df['title_jieba_array'] = train_df['title_jieba_words'].map(lambda x : get_w2v_array(x, word_wv, num_features))\n",
    "test_df['title_jieba_array'] = test_df['title_jieba_words'].map(lambda x : get_w2v_array(x, word_wv, num_features))\n",
    "\n",
    "train_df['prefix_jieba_array'] = train_df['prefix_jieba_words'].map(lambda x : get_w2v_array(x, word_wv, num_features))\n",
    "test_df['prefix_jieba_array'] = test_df['prefix_jieba_words'].map(lambda x : get_w2v_array(x, word_wv, num_features))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot : finish!!!\n",
      "norm : finish!!!\n",
      "cosine : finish!!!\n"
     ]
    }
   ],
   "source": [
    "def get_title_prefix_similarity(df, f_similarity):\n",
    "    title_array = df['title_jieba_array']\n",
    "    prefix_array = df['prefix_jieba_array']\n",
    "    similarity = 0\n",
    "    if f_similarity == 'dot':\n",
    "        similarity = np.dot(title_array, prefix_array)\n",
    "    elif f_similarity == 'norm':\n",
    "        similarity = np.linalg.norm(title_array - prefix_array)\n",
    "    else:\n",
    "        similarity = np.dot(title_array,prefix_array) / (np.linalg.norm(title_array) * np.linalg.norm(prefix_array))\n",
    "    return similarity\n",
    "\n",
    "# def get_title_query_similarity(df, f_similarity, word_wv, num_features):\n",
    "#     title_array = df['title_jieba_array']\n",
    "#     query_prediction_words = df['query_prediction_words']\n",
    "#     query_prediction_keys = df['query_prediction_keys']\n",
    "#     query_prediction_dict = df['query_prediction_dict']\n",
    "#     if len(query_prediction_keys) <= 0:\n",
    "#         return np.nan\n",
    "#     similarity = 0\n",
    "#     if f_similarity == 'dot':\n",
    "#         i = 0\n",
    "#         for key in query_prediction_keys:\n",
    "#             key_array = get_w2v_array(query_prediction_words[i], word_wv, num_features)\n",
    "#             similarity = similarity + np.dot(title_array, key_array) * float(query_prediction_dict[key])\n",
    "#             i = i + 1\n",
    "#     elif f_similarity == 'norm':\n",
    "#         i = 0\n",
    "#         for key in query_prediction_keys:\n",
    "#             key_array = get_w2v_array(query_prediction_words[i], word_wv, num_features)\n",
    "#             similarity = similarity + np.linalg.norm(title_array - key_array) * float(query_prediction_dict[key])\n",
    "#             i = i + 1\n",
    "#     else:\n",
    "#         i = 0\n",
    "#         for key in query_prediction_keys:\n",
    "#             key_array = get_w2v_array(query_prediction_words[i], word_wv, num_features)\n",
    "#             similarity = similarity + (np.dot(title_array, key_array) / (np.linalg.norm(title_array) * np.linalg.norm(key_array))) * float(query_prediction_dict[key])\n",
    "#             i = i + 1\n",
    "#     return similarity\n",
    "\n",
    "def get_title_query_similarity_list(df, f_similarity, word_wv, num_features):\n",
    "    title_array = df['title_jieba_array']\n",
    "    query_prediction_words = df['query_prediction_words']\n",
    "    query_prediction_keys = df['query_prediction_keys']\n",
    "    query_prediction_dict = df['query_prediction_dict']\n",
    "    similarity_list = list()\n",
    "    if len(query_prediction_keys) <= 0:\n",
    "        return similarity_list\n",
    "    if f_similarity == 'dot':\n",
    "        i = 0\n",
    "        for key in query_prediction_keys:\n",
    "            key_array = get_w2v_array(query_prediction_words[i], word_wv, num_features)\n",
    "            similarity = np.dot(title_array, key_array) * float(query_prediction_dict[key])\n",
    "            similarity_list.append(similarity)\n",
    "            i = i + 1\n",
    "    elif f_similarity == 'norm':\n",
    "        i = 0\n",
    "        for key in query_prediction_keys:\n",
    "            key_array = get_w2v_array(query_prediction_words[i], word_wv, num_features)\n",
    "            similarity = np.linalg.norm(title_array - key_array) * float(query_prediction_dict[key])\n",
    "            similarity_list.append(similarity)\n",
    "            i = i + 1\n",
    "    else:\n",
    "        i = 0\n",
    "        for key in query_prediction_keys:\n",
    "            key_array = get_w2v_array(query_prediction_words[i], word_wv, num_features)\n",
    "            similarity = (np.dot(title_array, key_array) / (np.linalg.norm(title_array) * np.linalg.norm(key_array))) * float(query_prediction_dict[key])\n",
    "            similarity_list.append(similarity)\n",
    "            i = i + 1\n",
    "    return similarity_list\n",
    "\n",
    "def get_similarity_feature(train_df, valid_df):\n",
    "    f_list = ['dot', 'norm', 'cosine']\n",
    "    for fun in f_list:\n",
    "        f_prefix_similarity = functools.partial(get_title_prefix_similarity, f_similarity=fun)\n",
    "        train_df['title_prefix_' + fun + '_similarity'] = train_df[['title_jieba_array', 'prefix_jieba_array']].apply(f_prefix_similarity, axis=1)\n",
    "        valid_df['title_prefix_' + fun + '_similarity'] = valid_df[['title_jieba_array', 'prefix_jieba_array']].apply(f_prefix_similarity, axis=1)\n",
    "#         f_query_similarity = functools.partial(get_title_query_similarity, f_similarity=fun, word_wv=word_wv, num_features=num_features)\n",
    "#         train_df['title_query_' + fun + '_similarity'] = train_df[['title_jieba_array', 'query_prediction_words', 'query_prediction_keys', 'query_prediction_dict']].apply(f_query_similarity, axis=1)\n",
    "#         valid_df['title_query_' + fun + '_similarity'] = valid_df[['title_jieba_array', 'query_prediction_words', 'query_prediction_keys', 'query_prediction_dict']].apply(f_query_similarity, axis=1)\n",
    "        f_query_similarity_list = functools.partial(get_title_query_similarity_list, f_similarity=fun, word_wv=word_wv, num_features=num_features)\n",
    "        train_df['title_query_' + fun + '_similarity_list'] = train_df[['title_jieba_array', 'query_prediction_words', 'query_prediction_keys', 'query_prediction_dict']].apply(f_query_similarity_list, axis=1)\n",
    "        valid_df['title_query_' + fun + '_similarity_list'] = valid_df[['title_jieba_array', 'query_prediction_words', 'query_prediction_keys', 'query_prediction_dict']].apply(f_query_similarity_list, axis=1)\n",
    "        train_df['title_query_' + fun + '_similarity'] = train_df['title_query_' + fun + '_similarity_list'].map(lambda x : np.nan if len(x)==0 else np.sum(x))\n",
    "        train_df['title_query_' + fun + '_similarity_max'] = train_df['title_query_' + fun + '_similarity_list'].map(lambda x : np.nan if len(x)==0 else np.max(x))\n",
    "        train_df['title_query_' + fun + '_similarity_min'] = train_df['title_query_' + fun + '_similarity_list'].map(lambda x : np.nan if len(x)==0 else np.min(x))\n",
    "        train_df['title_query_' + fun + '_similarity_mean'] = train_df['title_query_' + fun + '_similarity_list'].map(lambda x : np.nan if len(x)==0 else np.mean(x))\n",
    "        train_df['title_query_' + fun + '_similarity_std'] = train_df['title_query_' + fun + '_similarity_list'].map(lambda x : np.nan if len(x)==0 else np.std(x))\n",
    "        valid_df['title_query_' + fun + '_similarity'] = valid_df['title_query_' + fun + '_similarity_list'].map(lambda x : np.nan if len(x)==0 else np.sum(x))\n",
    "        valid_df['title_query_' + fun + '_similarity_max'] = valid_df['title_query_' + fun + '_similarity_list'].map(lambda x : np.nan if len(x)==0 else np.max(x))\n",
    "        valid_df['title_query_' + fun + '_similarity_min'] = valid_df['title_query_' + fun + '_similarity_list'].map(lambda x : np.nan if len(x)==0 else np.min(x))\n",
    "        valid_df['title_query_' + fun + '_similarity_mean'] = valid_df['title_query_' + fun + '_similarity_list'].map(lambda x : np.nan if len(x)==0 else np.mean(x))\n",
    "        valid_df['title_query_' + fun + '_similarity_std'] = valid_df['title_query_' + fun + '_similarity_list'].map(lambda x : np.nan if len(x)==0 else np.std(x))\n",
    "        print(fun + ' : finish!!!')\n",
    "    return train_df, valid_df\n",
    "\n",
    "train_df, test_df = get_similarity_feature(train_df, test_df)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['index' 'prefix' 'query_prediction' 'title' 'tag' 'label'\n",
      " 'query_prediction_dict' 'query_prediction_keys' 'query_prediction_values'\n",
      " 'query_prediction_number' 'query_prediction_max' 'query_prediction_min'\n",
      " 'query_prediction_mean' 'query_prediction_std' 'prefix_count'\n",
      " 'prefix_rate' 'prefix_click_number' 'title_count' 'title_rate'\n",
      " 'title_click_number' 'tag_count' 'tag_rate' 'tag_click_number'\n",
      " 'query_prediction_count' 'query_prediction_rate'\n",
      " 'query_prediction_click_number' 'prefix_title_count' 'prefix_title_rate'\n",
      " 'prefix_title_click_number' 'prefix_tag_count' 'prefix_tag_rate'\n",
      " 'prefix_tag_click_number' 'title_tag_count' 'title_tag_rate'\n",
      " 'title_tag_click_number' 'is_title_in_query' 'is_prefix_in_title'\n",
      " 'title_tag_types' 'prefix_tag_types' 'tag_title_types' 'tag_prefix_types'\n",
      " 'title_prefix_types' 'prefix_title_types' 'tag_query_prediction_types'\n",
      " 'title_query_prediction_types' 'prefix_len' 'title_len'\n",
      " 'query_prediction_key_len_max' 'query_prediction_key_len_min'\n",
      " 'query_prediction_key_len_mean' 'query_prediction_key_len_std'\n",
      " 'len_title-prefix' 'len_prefix/title' 'len_mean-title' 'len_mean/title'\n",
      " 'title_prefix_leven' 'title_prefix_leven_rate' 'title_query_leven_list'\n",
      " 'title_query_leven_sum' 'title_query_leven_max' 'title_query_leven_min'\n",
      " 'title_query_leven_mean' 'title_query_leven_std'\n",
      " 'query_prediction_key_sentences' 'query_prediction_key_jieba_words'\n",
      " 'query_prediction_words' 'title_jieba_words' 'prefix_jieba_words'\n",
      " 'q_t_word_match' 'q_t_jaccard' 'q_t_common_words'\n",
      " 'q_t_total_unique_words' 'q_t_wc_diff' 'q_t_wc_ratio'\n",
      " 'q_t_wc_diff_unique' 'q_t_wc_ratio_unique' 'q_t_tfidf_word_match_share'\n",
      " 'p_t_word_match' 'p_t_jaccard' 'p_t_common_words'\n",
      " 'p_t_total_unique_words' 'p_t_wc_diff' 'p_t_wc_ratio'\n",
      " 'p_t_wc_diff_unique' 'p_t_wc_ratio_unique' 'p_t_tfidf_word_match_share'\n",
      " 'p_q_word_match' 'p_q_jaccard' 'p_q_common_words'\n",
      " 'p_q_total_unique_words' 'p_q_wc_diff' 'p_q_wc_ratio'\n",
      " 'p_q_wc_diff_unique' 'p_q_wc_ratio_unique' 'p_q_tfidf_word_match_share'\n",
      " 'title_jieba_array' 'prefix_jieba_array' 'title_prefix_dot_similarity'\n",
      " 'title_query_dot_similarity_list' 'title_query_dot_similarity'\n",
      " 'title_query_dot_similarity_max' 'title_query_dot_similarity_min'\n",
      " 'title_query_dot_similarity_mean' 'title_query_dot_similarity_std'\n",
      " 'title_prefix_norm_similarity' 'title_query_norm_similarity_list'\n",
      " 'title_query_norm_similarity' 'title_query_norm_similarity_max'\n",
      " 'title_query_norm_similarity_min' 'title_query_norm_similarity_mean'\n",
      " 'title_query_norm_similarity_std' 'title_prefix_cosine_similarity'\n",
      " 'title_query_cosine_similarity_list' 'title_query_cosine_similarity'\n",
      " 'title_query_cosine_similarity_max' 'title_query_cosine_similarity_min'\n",
      " 'title_query_cosine_similarity_mean' 'title_query_cosine_similarity_std']\n"
     ]
    }
   ],
   "source": [
    "print(train_df.columns.values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fea = [\n",
    "    'query_prediction_number', 'query_prediction_max', 'query_prediction_min', 'query_prediction_mean', 'query_prediction_std',\n",
    "       'prefix_count', 'prefix_rate',\n",
    " 'title_count', 'title_rate', 'tag_count', 'tag_rate',\n",
    " 'query_prediction_count', 'query_prediction_rate', 'prefix_title_count',\n",
    " 'prefix_title_rate',  'prefix_tag_count', 'prefix_tag_rate',\n",
    " 'title_tag_count', 'title_tag_rate',\n",
    "    'prefix_click_number', 'title_click_number', 'query_prediction_click_number', 'prefix_tag_click_number', \n",
    "    'prefix_title_click_number', 'title_tag_click_number',\n",
    "    'is_title_in_query', 'is_prefix_in_title', \n",
    "    'title_tag_types', 'prefix_tag_types', 'tag_title_types', 'tag_prefix_types',\n",
    " 'title_prefix_types', 'prefix_title_types', 'tag_query_prediction_types', 'title_query_prediction_types',\n",
    "      'prefix_len', 'title_len',\n",
    " 'query_prediction_key_len_max', 'query_prediction_key_len_min',\n",
    " 'query_prediction_key_len_mean', 'query_prediction_key_len_std',\n",
    " 'len_title-prefix', 'len_prefix/title', 'len_mean-title', 'len_mean/title',\n",
    "    'q_t_word_match', 'q_t_jaccard', 'q_t_common_words',\n",
    " 'q_t_total_unique_words', 'q_t_wc_diff', 'q_t_wc_ratio',\n",
    " 'q_t_wc_diff_unique', 'q_t_wc_ratio_unique', 'q_t_tfidf_word_match_share',\n",
    " 'p_t_word_match', 'p_t_jaccard', 'p_t_common_words',\n",
    " 'p_t_total_unique_words', 'p_t_wc_diff', 'p_t_wc_ratio',\n",
    " 'p_t_wc_diff_unique', 'p_t_wc_ratio_unique', 'p_t_tfidf_word_match_share',\n",
    " 'p_q_word_match', 'p_q_jaccard', 'p_q_common_words',\n",
    " 'p_q_total_unique_words', 'p_q_wc_diff', 'p_q_wc_ratio',\n",
    " 'p_q_wc_diff_unique', 'p_q_wc_ratio_unique', 'p_q_tfidf_word_match_share',\n",
    "    'title_prefix_dot_similarity',\n",
    " 'title_query_dot_similarity', 'title_prefix_norm_similarity',\n",
    " 'title_query_norm_similarity', 'title_prefix_cosine_similarity',\n",
    " 'title_query_cosine_similarity',\n",
    "    'title_query_dot_similarity_max', 'title_query_dot_similarity_min',\n",
    " 'title_query_dot_similarity_mean', 'title_query_dot_similarity_std',\n",
    "    'title_query_norm_similarity_min', 'title_query_norm_similarity_mean',\n",
    " 'title_query_norm_similarity_std', 'title_prefix_cosine_similarity',\n",
    "    'title_query_cosine_similarity_max', 'title_query_cosine_similarity_min',\n",
    " 'title_query_cosine_similarity_mean', 'title_query_cosine_similarity_std',\n",
    "    'title_prefix_leven', 'title_prefix_leven_rate',\n",
    " 'title_query_leven_sum', 'title_query_leven_max', 'title_query_leven_min',\n",
    " 'title_query_leven_mean', 'title_query_leven_std',\n",
    "    'prefix', 'query_prediction', 'title', 'tag',\n",
    "      ]\n",
    "\n",
    "train_fea = fea + ['index', 'label']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 导出特征工程文件\n",
    "def exportDf(df, fileName):\n",
    "    df.to_csv('../temp/%s.csv' % fileName, header=True, index=True)\n",
    "\n",
    "exportDf(train_df[train_fea], 'train_online_alldata_df')\n",
    "exportDf(test_df[fea], 'test_online_alldata_df')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
