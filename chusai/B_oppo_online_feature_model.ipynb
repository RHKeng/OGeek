{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import gc\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, log_loss, f1_score\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import chi2, SelectPercentile\n",
    "import math\n",
    "import jieba\n",
    "import jieba.posseg as psg\n",
    "from collections import Counter\n",
    "import functools\n",
    "from gensim.models import word2vec\n",
    "import Levenshtein\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 250000 entries, 0 to 249999\n",
      "Data columns (total 5 columns):\n",
      "prefix              250000 non-null object\n",
      "query_prediction    250000 non-null object\n",
      "title               250000 non-null object\n",
      "tag                 250000 non-null object\n",
      "label               0 non-null float64\n",
      "dtypes: float64(1), object(4)\n",
      "memory usage: 9.5+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_table('../data/oppo_round1_train_20180929.txt', names=['prefix', 'query_prediction', 'title', 'tag', 'label'], header=None, quoting=3)\n",
    "valid_df = pd.read_table('../data/oppo_round1_vali_20180929.txt', names=['prefix', 'query_prediction', 'title', 'tag', 'label'], header=None, quoting=3)\n",
    "train_df = pd.concat([train_df, valid_df])\n",
    "train_df.reset_index(inplace=True)\n",
    "train_df['index'] = train_df.index\n",
    "\n",
    "test_df = pd.read_table('../data/oppo_round1_test_B_20181106.txt', names=['prefix', 'query_prediction', 'title', 'tag', 'label'], header=None, quoting=3)\n",
    "# test_df = pd.read_table('../data/oppo_round1_vali_20180926.txt', names=['prefix', 'query_prediction', 'title', 'tag', 'label'], header=None, quoting=3)\n",
    "print(test_df.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index prefix                                   query_prediction  \\\n",
      "0      0     小品  {\"小品大全宋小宝\": \"0.009\", \"小品相亲\": \"0.012\", \"小品剧本\": ...   \n",
      "1      1   1368  {\"13688cc赛马会\": \"0.059\", \"13685367892\": \"0.124\"...   \n",
      "2      2   1368  {\"13688cc赛马会\": \"0.059\", \"13685367892\": \"0.124\"...   \n",
      "3      3     银耳  {\"银耳汤的功效\": \"0.012\", \"银耳为什么不能天天吃\": \"0.009\", \"银耳...   \n",
      "4      4   月经量少  {\"月经量少喝红糖水好吗\": \"0.010\", \"月经量少该怎么调理\": \"0.016\", ...   \n",
      "\n",
      "             title tag  label  \\\n",
      "0               小品  阅读      0   \n",
      "1  HCG大于1368%2C正常吗  健康      0   \n",
      "2            1368年  百科      1   \n",
      "3         银耳红枣汤的做法  菜谱      1   \n",
      "4         月经量少怎么调理  百科      0   \n",
      "\n",
      "                               query_prediction_dict  \\\n",
      "0  {'小品大全宋小宝': '0.009', '小品相亲': '0.012', '小品剧本': ...   \n",
      "1  {'13688cc赛马会': '0.059', '13685367892': '0.124'...   \n",
      "2  {'13688cc赛马会': '0.059', '13685367892': '0.124'...   \n",
      "3  {'银耳汤的功效': '0.012', '银耳为什么不能天天吃': '0.009', '银耳...   \n",
      "4  {'月经量少喝红糖水好吗': '0.010', '月经量少该怎么调理': '0.016', ...   \n",
      "\n",
      "                               query_prediction_keys  \\\n",
      "0  [小品大全宋小宝, 小品相亲, 小品剧本, 小品搞笑大全, 小品不差钱, 小品搞笑大全剧本,...   \n",
      "1  [13688cc赛马会, 13685367892, 13688cc, 1368个单词就够了,...   \n",
      "2  [13688cc赛马会, 13685367892, 13688cc, 1368个单词就够了,...   \n",
      "3  [银耳汤的功效, 银耳为什么不能天天吃, 银耳莲子羹, 银耳的做法, 银耳的功效, 银耳莲子...   \n",
      "4  [月经量少喝红糖水好吗, 月经量少该怎么调理, 月经量少怎么, 月经量少发黑, 月经量少是什...   \n",
      "\n",
      "                             query_prediction_values  query_prediction_number  \\\n",
      "0  [0.009, 0.012, 0.02, 0.066, 0.007, 0.01, 0.198...                       10   \n",
      "1  [0.059, 0.124, 0.029, 0.07, 0.022, 0.042, 0.08...                        9   \n",
      "2  [0.059, 0.124, 0.029, 0.07, 0.022, 0.042, 0.08...                        9   \n",
      "3  [0.012, 0.009, 0.05, 0.045, 0.053, 0.014, 0.05...                       10   \n",
      "4  [0.01, 0.016, 0.009, 0.009, 0.569, 0.016, 0.02...                       10   \n",
      "\n",
      "   query_prediction_max  query_prediction_min  query_prediction_mean  \\\n",
      "0                 0.198                 0.007               0.037300   \n",
      "1                 0.124                 0.022               0.057778   \n",
      "2                 0.124                 0.022               0.057778   \n",
      "3                 0.114                 0.009               0.040400   \n",
      "4                 0.569                 0.009               0.074700   \n",
      "\n",
      "   query_prediction_std  \n",
      "0              0.056023  \n",
      "1              0.031538  \n",
      "2              0.031538  \n",
      "3              0.030660  \n",
      "4              0.165089  \n"
     ]
    }
   ],
   "source": [
    "def get_float_list(x):\n",
    "    return_list = []\n",
    "    for temp in x:\n",
    "        return_list.append(float(temp))\n",
    "    return return_list\n",
    "\n",
    "# 处理跟query_prediction相关的统计特征\n",
    "def get_query_prediction_feature(df):\n",
    "    df['query_prediction_dict'] = df['query_prediction'].map(lambda x : dict() if x is np.nan else eval(x))\n",
    "    df['query_prediction_keys'] = df['query_prediction_dict'].map(lambda x : list(x.keys()))\n",
    "    df['query_prediction_values'] = df['query_prediction_dict'].map(lambda x : get_float_list(list(x.values())))\n",
    "    df['query_prediction_number'] = df['query_prediction_keys'].map(lambda x : len(x))\n",
    "    df['query_prediction_max'] = df['query_prediction_values'].map(lambda x : np.nan if len(x) == 0 else np.max(x))\n",
    "    df['query_prediction_min'] = df['query_prediction_values'].map(lambda x : np.nan if len(x) == 0 else np.min(x))\n",
    "    df['query_prediction_mean'] = df['query_prediction_values'].map(lambda x : np.nan if len(x) == 0 else np.mean(x))\n",
    "    df['query_prediction_std'] = df['query_prediction_values'].map(lambda x : np.nan if len(x) == 0 else np.std(x))\n",
    "    return df\n",
    "\n",
    "train_df = get_query_prediction_feature(train_df)\n",
    "test_df = get_query_prediction_feature(test_df)\n",
    "print(train_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefix : finish!!!\n",
      "title : finish!!!\n",
      "tag : finish!!!\n",
      "query_prediction : finish!!!\n",
      "  prefix                                   query_prediction  \\\n",
      "0  新生儿游泳  {\"新生儿游泳去黄疸吗\": \"0.013\", \"新生儿游泳时间\": \"0.012\", \"新生...   \n",
      "1    吴奇隆  {\"吴奇隆李小冉\": \"0.011\", \"吴奇隆电视剧\": \"0.021\", \"吴奇隆的综艺...   \n",
      "2      炎  {\"炎琥宁\": \"0.013\", \"炎武战神\": \"0.023\", \"炎帝\": \"0.020...   \n",
      "3     知乎  {\"知乎2018必读书单\": \"0.004\", \"知乎翎春君\": \"0.007\", \"知乎暗...   \n",
      "4    快递查  {\"快递查询询\": \"0.001\", \"快递查单号\": \"0.001\", \"快递查询自动识别...   \n",
      "\n",
      "               title tag  label  \\\n",
      "0       新生儿游泳到底好不好呀？  健康    NaN   \n",
      "1  吴奇隆刘诗诗公开恋情 马苏怎么办呢  影视    NaN   \n",
      "2                炎亚纶  百科    NaN   \n",
      "3                 知乎  应用    NaN   \n",
      "4               快递查询  应用    NaN   \n",
      "\n",
      "                               query_prediction_dict  \\\n",
      "0  {'新生儿游泳去黄疸吗': '0.013', '新生儿游泳时间': '0.012', '新生...   \n",
      "1  {'吴奇隆李小冉': '0.011', '吴奇隆电视剧': '0.021', '吴奇隆的综艺...   \n",
      "2  {'炎琥宁': '0.013', '炎武战神': '0.023', '炎帝': '0.020...   \n",
      "3  {'知乎2018必读书单': '0.004', '知乎翎春君': '0.007', '知乎暗...   \n",
      "4  {'快递查询询': '0.001', '快递查单号': '0.001', '快递查询自动识别...   \n",
      "\n",
      "                               query_prediction_keys  \\\n",
      "0  [新生儿游泳去黄疸吗, 新生儿游泳时间, 新生儿游泳的好处, 新生儿游泳视频教程, 新生儿游...   \n",
      "1  [吴奇隆李小冉, 吴奇隆电视剧, 吴奇隆的综艺, 吴奇隆微博, 吴奇隆宣布刘诗诗离婚, 吴奇...   \n",
      "2  [炎琥宁, 炎武战神, 炎帝, 炎凉和梁希城在楼梯做, 炎症, 炎琥宁的作用与功效, 炎龙铠...   \n",
      "3  [知乎2018必读书单, 知乎翎春君, 知乎暗网到底有多恐怖, 知乎网, 知乎好听到爆的名字...   \n",
      "4  [快递查询询, 快递查单号, 快递查询自动识别, 快递查询, 快递查询单号查询追踪, 快递查...   \n",
      "\n",
      "                             query_prediction_values  query_prediction_number  \\\n",
      "0  [0.013, 0.012, 0.263, 0.009, 0.042, 0.032, 0.0...                       10   \n",
      "1  [0.011, 0.021, 0.006, 0.009, 0.163, 0.014, 0.0...                       10   \n",
      "2  [0.013, 0.023, 0.02, 0.026, 0.02, 0.019, 0.02,...                       10   \n",
      "3  [0.004, 0.007, 0.016, 0.016, 0.027, 0.011, 0.0...                        9   \n",
      "4  [0.001, 0.001, 0.003, 0.553, 0.003, 0.292, 0.0...                        9   \n",
      "\n",
      "   query_prediction_max              ...                prefix_click_number  \\\n",
      "0                 0.332              ...                                1.0   \n",
      "1                 0.163              ...                               64.0   \n",
      "2                 0.077              ...                                6.0   \n",
      "3                 0.027              ...                              309.0   \n",
      "4                 0.553              ...                               46.0   \n",
      "\n",
      "   title_count  title_rate  title_click_number  tag_count  tag_rate  \\\n",
      "0          1.0    0.865140                 1.0     143145  0.297238   \n",
      "1         72.0    0.070606                 5.0      28579  0.342797   \n",
      "2          6.0    0.972598                 6.0     740157  0.396272   \n",
      "3       1189.0    0.217026               258.0     338850  0.353991   \n",
      "4        195.0    0.891579               174.0     338850  0.353991   \n",
      "\n",
      "   tag_click_number  query_prediction_count  query_prediction_rate  \\\n",
      "0             42548                     2.0               0.457637   \n",
      "1              9797                   171.0               0.374472   \n",
      "2            293304                    13.0               0.455281   \n",
      "3            119950                   834.0               0.370552   \n",
      "4            119950                   139.0               0.331627   \n",
      "\n",
      "   query_prediction_click_number  \n",
      "0                            1.0  \n",
      "1                           64.0  \n",
      "2                            6.0  \n",
      "3                          309.0  \n",
      "4                           46.0  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "def getBayesSmoothParam(origion_rate):\n",
    "    origion_rate_mean = origion_rate.mean()\n",
    "    origion_rate_var = origion_rate.var()\n",
    "    alpha = origion_rate_mean / origion_rate_var * (origion_rate_mean * (1 - origion_rate_mean) - origion_rate_var)\n",
    "    beta = (1 - origion_rate_mean) / origion_rate_var * (origion_rate_mean * (1 - origion_rate_mean) - origion_rate_var)\n",
    "#     print('origion_rate_mean : ', origion_rate_mean)\n",
    "#     print('origion_rate_var : ', origion_rate_var)\n",
    "#     print('alpha : ', alpha)\n",
    "#     print('beta : ', beta)\n",
    "    return alpha, beta\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, random_state=2018, shuffle=True)\n",
    "\n",
    "# 统计单维度的转化率特征\n",
    "def get_single_dimension_rate_feature(train_df, valid_df, fea_set):\n",
    "    for fea in fea_set:\n",
    "        temp_df = train_df[[fea, 'label']].copy()\n",
    "        temp_pivot_table = pd.pivot_table(temp_df, index=fea, values='label', aggfunc={len, np.mean, np.sum})\n",
    "        temp_pivot_table.reset_index(inplace=True)\n",
    "        temp_pivot_table.rename(columns={'len':fea + '_count', 'mean':fea + '_rate', 'sum':fea + '_click_number'}, inplace=True)\n",
    "        alpha, beta = getBayesSmoothParam(temp_pivot_table[fea + '_rate'])\n",
    "        temp_pivot_table[fea + '_rate'] = (temp_pivot_table[fea + '_click_number'] + alpha) / (temp_pivot_table[fea + '_count'] + alpha + beta)\n",
    "#             del temp_pivot_table[fea + '_click_number']\n",
    "        valid_df = pd.merge(valid_df, temp_pivot_table, on=fea, how='left')\n",
    "        print(fea + ' : finish!!!')\n",
    "    return valid_df\n",
    "    \n",
    "fea_set = ['prefix', 'title', 'tag', 'query_prediction']\n",
    "test_df = get_single_dimension_rate_feature(train_df, test_df, fea_set)\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefix_title : finish!!!\n",
      "prefix_tag : finish!!!\n",
      "title_tag : finish!!!\n",
      "  prefix                                   query_prediction  \\\n",
      "0  新生儿游泳  {\"新生儿游泳去黄疸吗\": \"0.013\", \"新生儿游泳时间\": \"0.012\", \"新生...   \n",
      "1    吴奇隆  {\"吴奇隆李小冉\": \"0.011\", \"吴奇隆电视剧\": \"0.021\", \"吴奇隆的综艺...   \n",
      "2      炎  {\"炎琥宁\": \"0.013\", \"炎武战神\": \"0.023\", \"炎帝\": \"0.020...   \n",
      "3     知乎  {\"知乎2018必读书单\": \"0.004\", \"知乎翎春君\": \"0.007\", \"知乎暗...   \n",
      "4    快递查  {\"快递查询询\": \"0.001\", \"快递查单号\": \"0.001\", \"快递查询自动识别...   \n",
      "\n",
      "               title tag  label  \\\n",
      "0       新生儿游泳到底好不好呀？  健康    NaN   \n",
      "1  吴奇隆刘诗诗公开恋情 马苏怎么办呢  影视    NaN   \n",
      "2                炎亚纶  百科    NaN   \n",
      "3                 知乎  应用    NaN   \n",
      "4               快递查询  应用    NaN   \n",
      "\n",
      "                               query_prediction_dict  \\\n",
      "0  {'新生儿游泳去黄疸吗': '0.013', '新生儿游泳时间': '0.012', '新生...   \n",
      "1  {'吴奇隆李小冉': '0.011', '吴奇隆电视剧': '0.021', '吴奇隆的综艺...   \n",
      "2  {'炎琥宁': '0.013', '炎武战神': '0.023', '炎帝': '0.020...   \n",
      "3  {'知乎2018必读书单': '0.004', '知乎翎春君': '0.007', '知乎暗...   \n",
      "4  {'快递查询询': '0.001', '快递查单号': '0.001', '快递查询自动识别...   \n",
      "\n",
      "                               query_prediction_keys  \\\n",
      "0  [新生儿游泳去黄疸吗, 新生儿游泳时间, 新生儿游泳的好处, 新生儿游泳视频教程, 新生儿游...   \n",
      "1  [吴奇隆李小冉, 吴奇隆电视剧, 吴奇隆的综艺, 吴奇隆微博, 吴奇隆宣布刘诗诗离婚, 吴奇...   \n",
      "2  [炎琥宁, 炎武战神, 炎帝, 炎凉和梁希城在楼梯做, 炎症, 炎琥宁的作用与功效, 炎龙铠...   \n",
      "3  [知乎2018必读书单, 知乎翎春君, 知乎暗网到底有多恐怖, 知乎网, 知乎好听到爆的名字...   \n",
      "4  [快递查询询, 快递查单号, 快递查询自动识别, 快递查询, 快递查询单号查询追踪, 快递查...   \n",
      "\n",
      "                             query_prediction_values  query_prediction_number  \\\n",
      "0  [0.013, 0.012, 0.263, 0.009, 0.042, 0.032, 0.0...                       10   \n",
      "1  [0.011, 0.021, 0.006, 0.009, 0.163, 0.014, 0.0...                       10   \n",
      "2  [0.013, 0.023, 0.02, 0.026, 0.02, 0.019, 0.02,...                       10   \n",
      "3  [0.004, 0.007, 0.016, 0.016, 0.027, 0.011, 0.0...                        9   \n",
      "4  [0.001, 0.001, 0.003, 0.553, 0.003, 0.292, 0.0...                        9   \n",
      "\n",
      "   query_prediction_max           ...            \\\n",
      "0                 0.332           ...             \n",
      "1                 0.163           ...             \n",
      "2                 0.077           ...             \n",
      "3                 0.027           ...             \n",
      "4                 0.553           ...             \n",
      "\n",
      "   query_prediction_click_number  prefix_title_count  prefix_title_rate  \\\n",
      "0                            1.0                 1.0           0.878841   \n",
      "1                           64.0                40.0           0.051930   \n",
      "2                            6.0                 6.0           0.975926   \n",
      "3                          309.0               540.0           0.311139   \n",
      "4                           46.0                49.0           0.834479   \n",
      "\n",
      "   prefix_title_click_number  prefix_tag_count  prefix_tag_rate  \\\n",
      "0                        1.0               1.0         0.807343   \n",
      "1                        2.0              40.0         0.053687   \n",
      "2                        6.0               6.0         0.956659   \n",
      "3                      168.0             275.0         0.490728   \n",
      "4                       41.0              49.0         0.832571   \n",
      "\n",
      "   prefix_tag_click_number  title_tag_count  title_tag_rate  \\\n",
      "0                      1.0              1.0        0.879012   \n",
      "1                      2.0             72.0        0.070465   \n",
      "2                      6.0              6.0        0.975949   \n",
      "3                    135.0            825.0        0.257004   \n",
      "4                     41.0            195.0        0.891672   \n",
      "\n",
      "   title_tag_click_number  \n",
      "0                     1.0  \n",
      "1                     5.0  \n",
      "2                     6.0  \n",
      "3                   212.0  \n",
      "4                   174.0  \n",
      "\n",
      "[5 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "# 统计双维度交叉转化率\n",
    "def get_jiaocha_dimension_rate_feature(train_df, valid_df, fea_set):\n",
    "    for i in range(len(fea_set)):\n",
    "        for j in range((i+1), len(fea_set)):\n",
    "            fea1 = fea_set[i]\n",
    "            fea2 = fea_set[j]\n",
    "            temp_df = train_df[[fea1, fea2, 'label']].copy()\n",
    "            temp_pivot_table = pd.pivot_table(temp_df, index=[fea1, fea2], values='label', aggfunc={len, np.mean, np.sum})\n",
    "            temp_pivot_table.reset_index(inplace=True)\n",
    "            temp_pivot_table.rename(columns={'len':fea1 + '_' + fea2 + '_count', 'mean':fea1 + '_' + fea2 + '_rate', 'sum':fea1 + '_' + fea2 + '_click_number'}, inplace=True)\n",
    "            alpha, beta = getBayesSmoothParam(temp_pivot_table[fea1 + '_' + fea2 + '_rate'])\n",
    "            temp_pivot_table[fea1 + '_' + fea2 + '_rate'] = (temp_pivot_table[fea1 + '_' + fea2 + '_click_number'] + alpha) / (temp_pivot_table[fea1 + '_' + fea2 + '_count'] + alpha + beta)\n",
    "#             del temp_pivot_table[fea1 + '_' + fea2 + '_click_number']\n",
    "            print(fea1 + '_' + fea2 + ' : finish!!!')\n",
    "            valid_df = pd.merge(valid_df, temp_pivot_table, on=[fea1, fea2], how='left')\n",
    "    return valid_df\n",
    "\n",
    "jiaocha_fea_set = ['prefix', 'title', 'tag']\n",
    "test_df = get_jiaocha_dimension_rate_feature(train_df, test_df, jiaocha_fea_set)\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 统计一些是否交叉的特征\n",
    "def get_is_title_in_query_feature(df):\n",
    "    x = df['title']\n",
    "    y = df['query_prediction_keys']\n",
    "    is_title_in_query = np.nan\n",
    "    if len(y) > 0:\n",
    "        if x in y:\n",
    "            is_title_in_query = 1\n",
    "        else:\n",
    "            is_title_in_query = 0\n",
    "    return is_title_in_query\n",
    "\n",
    "def get_is_prefix_in_title_feature(df):\n",
    "    x = df['prefix']\n",
    "    y = df['title']\n",
    "    is_prefix_in_title = np.nan\n",
    "    if x in y:\n",
    "        is_prefix_in_title = 1\n",
    "    else:\n",
    "        is_prefix_in_title = 0\n",
    "    return is_prefix_in_title\n",
    "\n",
    "test_df['is_title_in_query'] = test_df[['title', 'query_prediction_keys']].apply(get_is_title_in_query_feature, axis = 1)\n",
    "\n",
    "test_df['is_prefix_in_title'] = test_df[['prefix', 'title']].apply(get_is_prefix_in_title_feature, axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  prefix                                   query_prediction  \\\n",
      "0  新生儿游泳  {\"新生儿游泳去黄疸吗\": \"0.013\", \"新生儿游泳时间\": \"0.012\", \"新生...   \n",
      "1    吴奇隆  {\"吴奇隆李小冉\": \"0.011\", \"吴奇隆电视剧\": \"0.021\", \"吴奇隆的综艺...   \n",
      "2      炎  {\"炎琥宁\": \"0.013\", \"炎武战神\": \"0.023\", \"炎帝\": \"0.020...   \n",
      "3     知乎  {\"知乎2018必读书单\": \"0.004\", \"知乎翎春君\": \"0.007\", \"知乎暗...   \n",
      "4    快递查  {\"快递查询询\": \"0.001\", \"快递查单号\": \"0.001\", \"快递查询自动识别...   \n",
      "\n",
      "               title tag  label  \\\n",
      "0       新生儿游泳到底好不好呀？  健康    NaN   \n",
      "1  吴奇隆刘诗诗公开恋情 马苏怎么办呢  影视    NaN   \n",
      "2                炎亚纶  百科    NaN   \n",
      "3                 知乎  应用    NaN   \n",
      "4               快递查询  应用    NaN   \n",
      "\n",
      "                               query_prediction_dict  \\\n",
      "0  {'新生儿游泳去黄疸吗': '0.013', '新生儿游泳时间': '0.012', '新生...   \n",
      "1  {'吴奇隆李小冉': '0.011', '吴奇隆电视剧': '0.021', '吴奇隆的综艺...   \n",
      "2  {'炎琥宁': '0.013', '炎武战神': '0.023', '炎帝': '0.020...   \n",
      "3  {'知乎2018必读书单': '0.004', '知乎翎春君': '0.007', '知乎暗...   \n",
      "4  {'快递查询询': '0.001', '快递查单号': '0.001', '快递查询自动识别...   \n",
      "\n",
      "                               query_prediction_keys  \\\n",
      "0  [新生儿游泳去黄疸吗, 新生儿游泳时间, 新生儿游泳的好处, 新生儿游泳视频教程, 新生儿游...   \n",
      "1  [吴奇隆李小冉, 吴奇隆电视剧, 吴奇隆的综艺, 吴奇隆微博, 吴奇隆宣布刘诗诗离婚, 吴奇...   \n",
      "2  [炎琥宁, 炎武战神, 炎帝, 炎凉和梁希城在楼梯做, 炎症, 炎琥宁的作用与功效, 炎龙铠...   \n",
      "3  [知乎2018必读书单, 知乎翎春君, 知乎暗网到底有多恐怖, 知乎网, 知乎好听到爆的名字...   \n",
      "4  [快递查询询, 快递查单号, 快递查询自动识别, 快递查询, 快递查询单号查询追踪, 快递查...   \n",
      "\n",
      "                             query_prediction_values  query_prediction_number  \\\n",
      "0  [0.013, 0.012, 0.263, 0.009, 0.042, 0.032, 0.0...                       10   \n",
      "1  [0.011, 0.021, 0.006, 0.009, 0.163, 0.014, 0.0...                       10   \n",
      "2  [0.013, 0.023, 0.02, 0.026, 0.02, 0.019, 0.02,...                       10   \n",
      "3  [0.004, 0.007, 0.016, 0.016, 0.027, 0.011, 0.0...                        9   \n",
      "4  [0.001, 0.001, 0.003, 0.553, 0.003, 0.292, 0.0...                        9   \n",
      "\n",
      "   query_prediction_max              ...               is_title_in_query  \\\n",
      "0                 0.332              ...                             0.0   \n",
      "1                 0.163              ...                             0.0   \n",
      "2                 0.077              ...                             1.0   \n",
      "3                 0.027              ...                             0.0   \n",
      "4                 0.553              ...                             1.0   \n",
      "\n",
      "   is_prefix_in_title  title_tag_types  prefix_tag_types  tag_title_types  \\\n",
      "0                   1                1                 2            48260   \n",
      "1                   1                1                 4             3387   \n",
      "2                   1                1                 2           135803   \n",
      "3                   1                2                 3             5057   \n",
      "4                   1                1                 3             5057   \n",
      "\n",
      "   tag_prefix_types  title_prefix_types  prefix_title_types  \\\n",
      "0             41747                   1                   2   \n",
      "1              4139                   2                   3   \n",
      "2            117029                   2                   2   \n",
      "3             18063                  92                   2   \n",
      "4             18063                   2                   3   \n",
      "\n",
      "   tag_query_prediction_types  title_query_prediction_types  \n",
      "0                       39748                             1  \n",
      "1                        4115                             2  \n",
      "2                      114791                             2  \n",
      "3                       17885                            92  \n",
      "4                       17885                             2  \n",
      "\n",
      "[5 rows x 44 columns]\n"
     ]
    }
   ],
   "source": [
    "# 统计一些交叉种类特征\n",
    "def get_jiaocha_type_feature(train_df, valid_df, jiaocha_type_list):\n",
    "    for jiaocha_type in jiaocha_type_list:\n",
    "        fea1 = jiaocha_type[0]\n",
    "        fea2 = jiaocha_type[1]\n",
    "        temp_df = pd.concat([train_df, valid_df])\n",
    "        temp_pivot_table = pd.pivot_table(temp_df[[fea1, fea2, 'label']], index=[fea1, fea2], values='label', aggfunc=len)\n",
    "        temp_pivot_table.reset_index(inplace=True)\n",
    "        final_pivot_table = pd.pivot_table(temp_pivot_table, index=fea1, values=fea2, aggfunc=len)\n",
    "        final_pivot_table.reset_index(inplace=True)\n",
    "        final_pivot_table.rename(columns={fea2 : fea1 + '_' + fea2 + '_types'}, inplace=True)\n",
    "        train_df = pd.merge(train_df, final_pivot_table[[fea1, fea1 + '_' + fea2 + '_types']], on=fea1, how='left')\n",
    "        valid_df = pd.merge(valid_df, final_pivot_table[[fea1, fea1 + '_' + fea2 + '_types']], on=fea1, how='left')\n",
    "    return train_df, valid_df\n",
    "\n",
    "jiaocha_type_list = [['title', 'tag'], ['prefix', 'tag'], ['tag', 'title'], ['tag', 'prefix'], \n",
    "                     ['title', 'prefix'], ['prefix', 'title'], ['tag', 'query_prediction'], ['title', 'query_prediction']]\n",
    "train_df, test_df = get_jiaocha_type_feature(train_df, test_df, jiaocha_type_list)\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  prefix                                   query_prediction  \\\n",
      "0  新生儿游泳  {\"新生儿游泳去黄疸吗\": \"0.013\", \"新生儿游泳时间\": \"0.012\", \"新生...   \n",
      "1    吴奇隆  {\"吴奇隆李小冉\": \"0.011\", \"吴奇隆电视剧\": \"0.021\", \"吴奇隆的综艺...   \n",
      "2      炎  {\"炎琥宁\": \"0.013\", \"炎武战神\": \"0.023\", \"炎帝\": \"0.020...   \n",
      "3     知乎  {\"知乎2018必读书单\": \"0.004\", \"知乎翎春君\": \"0.007\", \"知乎暗...   \n",
      "4    快递查  {\"快递查询询\": \"0.001\", \"快递查单号\": \"0.001\", \"快递查询自动识别...   \n",
      "\n",
      "               title tag  label  \\\n",
      "0       新生儿游泳到底好不好呀？  健康    NaN   \n",
      "1  吴奇隆刘诗诗公开恋情 马苏怎么办呢  影视    NaN   \n",
      "2                炎亚纶  百科    NaN   \n",
      "3                 知乎  应用    NaN   \n",
      "4               快递查询  应用    NaN   \n",
      "\n",
      "                               query_prediction_dict  \\\n",
      "0  {'新生儿游泳去黄疸吗': '0.013', '新生儿游泳时间': '0.012', '新生...   \n",
      "1  {'吴奇隆李小冉': '0.011', '吴奇隆电视剧': '0.021', '吴奇隆的综艺...   \n",
      "2  {'炎琥宁': '0.013', '炎武战神': '0.023', '炎帝': '0.020...   \n",
      "3  {'知乎2018必读书单': '0.004', '知乎翎春君': '0.007', '知乎暗...   \n",
      "4  {'快递查询询': '0.001', '快递查单号': '0.001', '快递查询自动识别...   \n",
      "\n",
      "                               query_prediction_keys  \\\n",
      "0  [新生儿游泳去黄疸吗, 新生儿游泳时间, 新生儿游泳的好处, 新生儿游泳视频教程, 新生儿游...   \n",
      "1  [吴奇隆李小冉, 吴奇隆电视剧, 吴奇隆的综艺, 吴奇隆微博, 吴奇隆宣布刘诗诗离婚, 吴奇...   \n",
      "2  [炎琥宁, 炎武战神, 炎帝, 炎凉和梁希城在楼梯做, 炎症, 炎琥宁的作用与功效, 炎龙铠...   \n",
      "3  [知乎2018必读书单, 知乎翎春君, 知乎暗网到底有多恐怖, 知乎网, 知乎好听到爆的名字...   \n",
      "4  [快递查询询, 快递查单号, 快递查询自动识别, 快递查询, 快递查询单号查询追踪, 快递查...   \n",
      "\n",
      "                             query_prediction_values  query_prediction_number  \\\n",
      "0  [0.013, 0.012, 0.263, 0.009, 0.042, 0.032, 0.0...                       10   \n",
      "1  [0.011, 0.021, 0.006, 0.009, 0.163, 0.014, 0.0...                       10   \n",
      "2  [0.013, 0.023, 0.02, 0.026, 0.02, 0.019, 0.02,...                       10   \n",
      "3  [0.004, 0.007, 0.016, 0.016, 0.027, 0.011, 0.0...                        9   \n",
      "4  [0.001, 0.001, 0.003, 0.553, 0.003, 0.292, 0.0...                        9   \n",
      "\n",
      "   query_prediction_max       ...        prefix_len  title_len  \\\n",
      "0                 0.332       ...                 5         12   \n",
      "1                 0.163       ...                 3         17   \n",
      "2                 0.077       ...                 1          3   \n",
      "3                 0.027       ...                 2          2   \n",
      "4                 0.553       ...                 3          4   \n",
      "\n",
      "   query_prediction_key_len_max  query_prediction_key_len_min  \\\n",
      "0                          10.0                           7.0   \n",
      "1                          10.0                           5.0   \n",
      "2                          10.0                           2.0   \n",
      "3                          10.0                           3.0   \n",
      "4                          10.0                           4.0   \n",
      "\n",
      "   query_prediction_key_len_mean  query_prediction_key_len_std  \\\n",
      "0                       8.500000                      1.118034   \n",
      "1                       6.200000                      1.536229   \n",
      "2                       5.500000                      3.170173   \n",
      "3                       6.666667                      2.449490   \n",
      "4                       6.444444                      1.832491   \n",
      "\n",
      "   len_title-prefix  len_prefix/title  len_mean-title  len_mean/title  \n",
      "0                 7          0.416667       -3.500000        0.708333  \n",
      "1                14          0.176471      -10.800000        0.364706  \n",
      "2                 2          0.333333        2.500000        1.833333  \n",
      "3                 0          1.000000        4.666667        3.333333  \n",
      "4                 1          0.750000        2.444444        1.611111  \n",
      "\n",
      "[5 rows x 54 columns]\n"
     ]
    }
   ],
   "source": [
    "def get_key_len_list(x):\n",
    "    return_list = []\n",
    "    for temp in x:\n",
    "        return_list.append(len(temp))\n",
    "    return return_list\n",
    "\n",
    "# 统计一些跟字符串长度相关的特征\n",
    "def get_string_len_feature(df):\n",
    "    df['prefix_len'] = df['prefix'].map(lambda x : len(x))\n",
    "    df['title_len'] = df['title'].map(lambda x : len(x))\n",
    "    df['query_prediction_key_len_list'] = df['query_prediction_keys'].map(lambda x : get_key_len_list(x))\n",
    "    df['query_prediction_key_len_max'] = df['query_prediction_key_len_list'].map(lambda x : np.nan if len(x) == 0 else np.max(x))\n",
    "    df['query_prediction_key_len_min'] = df['query_prediction_key_len_list'].map(lambda x : np.nan if len(x) == 0 else np.min(x))\n",
    "    df['query_prediction_key_len_mean'] = df['query_prediction_key_len_list'].map(lambda x : np.nan if len(x) == 0 else np.mean(x))\n",
    "    df['query_prediction_key_len_std'] = df['query_prediction_key_len_list'].map(lambda x : np.nan if len(x) == 0 else np.std(x))\n",
    "    df['len_title-prefix'] = df['title_len'] - df['prefix_len']\n",
    "    df['len_prefix/title'] = df['prefix_len'] / df['title_len']\n",
    "    df['len_mean-title'] = df['query_prediction_key_len_mean'] - df['title_len']\n",
    "    df['len_mean/title'] = df['query_prediction_key_len_mean'] / df['title_len']\n",
    "    del df['query_prediction_key_len_list']\n",
    "    return df\n",
    "\n",
    "test_df = get_string_len_feature(test_df)\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  prefix                                   query_prediction  \\\n",
      "0  新生儿游泳  {\"新生儿游泳去黄疸吗\": \"0.013\", \"新生儿游泳时间\": \"0.012\", \"新生...   \n",
      "1    吴奇隆  {\"吴奇隆李小冉\": \"0.011\", \"吴奇隆电视剧\": \"0.021\", \"吴奇隆的综艺...   \n",
      "2      炎  {\"炎琥宁\": \"0.013\", \"炎武战神\": \"0.023\", \"炎帝\": \"0.020...   \n",
      "3     知乎  {\"知乎2018必读书单\": \"0.004\", \"知乎翎春君\": \"0.007\", \"知乎暗...   \n",
      "4    快递查  {\"快递查询询\": \"0.001\", \"快递查单号\": \"0.001\", \"快递查询自动识别...   \n",
      "\n",
      "               title tag  label  \\\n",
      "0       新生儿游泳到底好不好呀？  健康    NaN   \n",
      "1  吴奇隆刘诗诗公开恋情 马苏怎么办呢  影视    NaN   \n",
      "2                炎亚纶  百科    NaN   \n",
      "3                 知乎  应用    NaN   \n",
      "4               快递查询  应用    NaN   \n",
      "\n",
      "                               query_prediction_dict  \\\n",
      "0  {'新生儿游泳去黄疸吗': '0.013', '新生儿游泳时间': '0.012', '新生...   \n",
      "1  {'吴奇隆李小冉': '0.011', '吴奇隆电视剧': '0.021', '吴奇隆的综艺...   \n",
      "2  {'炎琥宁': '0.013', '炎武战神': '0.023', '炎帝': '0.020...   \n",
      "3  {'知乎2018必读书单': '0.004', '知乎翎春君': '0.007', '知乎暗...   \n",
      "4  {'快递查询询': '0.001', '快递查单号': '0.001', '快递查询自动识别...   \n",
      "\n",
      "                               query_prediction_keys  \\\n",
      "0  [新生儿游泳去黄疸吗, 新生儿游泳时间, 新生儿游泳的好处, 新生儿游泳视频教程, 新生儿游...   \n",
      "1  [吴奇隆李小冉, 吴奇隆电视剧, 吴奇隆的综艺, 吴奇隆微博, 吴奇隆宣布刘诗诗离婚, 吴奇...   \n",
      "2  [炎琥宁, 炎武战神, 炎帝, 炎凉和梁希城在楼梯做, 炎症, 炎琥宁的作用与功效, 炎龙铠...   \n",
      "3  [知乎2018必读书单, 知乎翎春君, 知乎暗网到底有多恐怖, 知乎网, 知乎好听到爆的名字...   \n",
      "4  [快递查询询, 快递查单号, 快递查询自动识别, 快递查询, 快递查询单号查询追踪, 快递查...   \n",
      "\n",
      "                             query_prediction_values  query_prediction_number  \\\n",
      "0  [0.013, 0.012, 0.263, 0.009, 0.042, 0.032, 0.0...                       10   \n",
      "1  [0.011, 0.021, 0.006, 0.009, 0.163, 0.014, 0.0...                       10   \n",
      "2  [0.013, 0.023, 0.02, 0.026, 0.02, 0.019, 0.02,...                       10   \n",
      "3  [0.004, 0.007, 0.016, 0.016, 0.027, 0.011, 0.0...                        9   \n",
      "4  [0.001, 0.001, 0.003, 0.553, 0.003, 0.292, 0.0...                        9   \n",
      "\n",
      "   query_prediction_max           ...             \\\n",
      "0                 0.332           ...              \n",
      "1                 0.163           ...              \n",
      "2                 0.077           ...              \n",
      "3                 0.027           ...              \n",
      "4                 0.553           ...              \n",
      "\n",
      "   query_prediction_key_len_max  query_prediction_key_len_min  \\\n",
      "0                          10.0                           7.0   \n",
      "1                          10.0                           5.0   \n",
      "2                          10.0                           2.0   \n",
      "3                          10.0                           3.0   \n",
      "4                          10.0                           4.0   \n",
      "\n",
      "   query_prediction_key_len_mean  query_prediction_key_len_std  \\\n",
      "0                       8.500000                      1.118034   \n",
      "1                       6.200000                      1.536229   \n",
      "2                       5.500000                      3.170173   \n",
      "3                       6.666667                      2.449490   \n",
      "4                       6.444444                      1.832491   \n",
      "\n",
      "   len_title-prefix  len_prefix/title  len_mean-title  len_mean/title  \\\n",
      "0                 7          0.416667       -3.500000        0.708333   \n",
      "1                14          0.176471      -10.800000        0.364706   \n",
      "2                 2          0.333333        2.500000        1.833333   \n",
      "3                 0          1.000000        4.666667        3.333333   \n",
      "4                 1          0.750000        2.444444        1.611111   \n",
      "\n",
      "   title_prefix_leven  title_prefix_leven_rate  \n",
      "0                   7                 0.466667  \n",
      "1                  14                 0.700000  \n",
      "2                   2                 0.333333  \n",
      "3                   0                 0.000000  \n",
      "4                   1                 0.142857  \n",
      "\n",
      "[5 rows x 56 columns]\n"
     ]
    }
   ],
   "source": [
    "# 统计title跟prefix的编辑距离\n",
    "def get_title_prefix_levenshtein_distance(df):\n",
    "    title = df['title']\n",
    "    prefix = df['prefix']\n",
    "    return Levenshtein.distance(title, prefix)\n",
    "\n",
    "def get_title_prefix_levenshtein_distance_rate(df):\n",
    "    title_prefix_leven = df['title_prefix_leven']\n",
    "    title = df['title']\n",
    "    return (title_prefix_leven / (len(title) + 3))\n",
    "\n",
    "test_df['title_prefix_leven'] = test_df[['title', 'prefix']].apply(get_title_prefix_levenshtein_distance, axis=1)\n",
    "\n",
    "test_df['title_prefix_leven_rate'] = test_df[['title', 'title_prefix_leven']].apply(get_title_prefix_levenshtein_distance_rate, axis=1)\n",
    "\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  prefix                                   query_prediction  \\\n",
      "0  新生儿游泳  {\"新生儿游泳去黄疸吗\": \"0.013\", \"新生儿游泳时间\": \"0.012\", \"新生...   \n",
      "1    吴奇隆  {\"吴奇隆李小冉\": \"0.011\", \"吴奇隆电视剧\": \"0.021\", \"吴奇隆的综艺...   \n",
      "2      炎  {\"炎琥宁\": \"0.013\", \"炎武战神\": \"0.023\", \"炎帝\": \"0.020...   \n",
      "3     知乎  {\"知乎2018必读书单\": \"0.004\", \"知乎翎春君\": \"0.007\", \"知乎暗...   \n",
      "4    快递查  {\"快递查询询\": \"0.001\", \"快递查单号\": \"0.001\", \"快递查询自动识别...   \n",
      "\n",
      "               title tag  label  \\\n",
      "0       新生儿游泳到底好不好呀？  健康    NaN   \n",
      "1  吴奇隆刘诗诗公开恋情 马苏怎么办呢  影视    NaN   \n",
      "2                炎亚纶  百科    NaN   \n",
      "3                 知乎  应用    NaN   \n",
      "4               快递查询  应用    NaN   \n",
      "\n",
      "                               query_prediction_dict  \\\n",
      "0  {'新生儿游泳去黄疸吗': '0.013', '新生儿游泳时间': '0.012', '新生...   \n",
      "1  {'吴奇隆李小冉': '0.011', '吴奇隆电视剧': '0.021', '吴奇隆的综艺...   \n",
      "2  {'炎琥宁': '0.013', '炎武战神': '0.023', '炎帝': '0.020...   \n",
      "3  {'知乎2018必读书单': '0.004', '知乎翎春君': '0.007', '知乎暗...   \n",
      "4  {'快递查询询': '0.001', '快递查单号': '0.001', '快递查询自动识别...   \n",
      "\n",
      "                               query_prediction_keys  \\\n",
      "0  [新生儿游泳去黄疸吗, 新生儿游泳时间, 新生儿游泳的好处, 新生儿游泳视频教程, 新生儿游...   \n",
      "1  [吴奇隆李小冉, 吴奇隆电视剧, 吴奇隆的综艺, 吴奇隆微博, 吴奇隆宣布刘诗诗离婚, 吴奇...   \n",
      "2  [炎琥宁, 炎武战神, 炎帝, 炎凉和梁希城在楼梯做, 炎症, 炎琥宁的作用与功效, 炎龙铠...   \n",
      "3  [知乎2018必读书单, 知乎翎春君, 知乎暗网到底有多恐怖, 知乎网, 知乎好听到爆的名字...   \n",
      "4  [快递查询询, 快递查单号, 快递查询自动识别, 快递查询, 快递查询单号查询追踪, 快递查...   \n",
      "\n",
      "                             query_prediction_values  query_prediction_number  \\\n",
      "0  [0.013, 0.012, 0.263, 0.009, 0.042, 0.032, 0.0...                       10   \n",
      "1  [0.011, 0.021, 0.006, 0.009, 0.163, 0.014, 0.0...                       10   \n",
      "2  [0.013, 0.023, 0.02, 0.026, 0.02, 0.019, 0.02,...                       10   \n",
      "3  [0.004, 0.007, 0.016, 0.016, 0.027, 0.011, 0.0...                        9   \n",
      "4  [0.001, 0.001, 0.003, 0.553, 0.003, 0.292, 0.0...                        9   \n",
      "\n",
      "   query_prediction_max          ...            len_mean-title  \\\n",
      "0                 0.332          ...                 -3.500000   \n",
      "1                 0.163          ...                -10.800000   \n",
      "2                 0.077          ...                  2.500000   \n",
      "3                 0.027          ...                  4.666667   \n",
      "4                 0.553          ...                  2.444444   \n",
      "\n",
      "   len_mean/title  title_prefix_leven  title_prefix_leven_rate  \\\n",
      "0        0.708333                   7                 0.466667   \n",
      "1        0.364706                  14                 0.700000   \n",
      "2        1.833333                   2                 0.333333   \n",
      "3        3.333333                   0                 0.000000   \n",
      "4        1.611111                   1                 0.142857   \n",
      "\n",
      "                              title_query_leven_list  title_query_leven_sum  \\\n",
      "0  [0.091, 0.084, 1.578, 0.063, 0.252, 0.224, 0.0...                  5.024   \n",
      "1  [0.154, 0.29400000000000004, 0.084, 0.126, 2.1...                  3.534   \n",
      "2  [0.026, 0.069, 0.04, 0.23399999999999999, 0.04...                  0.906   \n",
      "3  [0.032, 0.021, 0.128, 0.016, 0.189, 0.05499999...                  0.567   \n",
      "4  [0.001, 0.002, 0.012, 0.0, 0.01800000000000000...                  0.830   \n",
      "\n",
      "   title_query_leven_max  title_query_leven_min  title_query_leven_mean  \\\n",
      "0                  2.324                  0.063                0.502400   \n",
      "1                  2.119                  0.084                0.353400   \n",
      "2                  0.234                  0.000                0.090600   \n",
      "3                  0.189                  0.016                0.063000   \n",
      "4                  0.584                  0.000                0.092222   \n",
      "\n",
      "   title_query_leven_std  \n",
      "0               0.746708  \n",
      "1               0.592326  \n",
      "2               0.070659  \n",
      "3               0.054422  \n",
      "4               0.178594  \n",
      "\n",
      "[5 rows x 62 columns]\n"
     ]
    }
   ],
   "source": [
    "# 统计title跟query_prediction编辑距离相关的特征\n",
    "def get_title_query_levenshtein_distance_list(df):\n",
    "    query_keys_list = df['query_prediction_keys']\n",
    "    query_values_list = df['query_prediction_values']\n",
    "    title = df['title']\n",
    "    return_list = list()\n",
    "    for i in range(len(query_keys_list)):\n",
    "        distance = Levenshtein.distance(title, query_keys_list[i])\n",
    "        return_list.append(distance * query_values_list[i])\n",
    "    return return_list\n",
    "\n",
    "def get_title_query_levenshtein_distance_feature(df):\n",
    "    df['title_query_leven_list'] = df[['query_prediction_keys', 'query_prediction_values', 'title']].apply(get_title_query_levenshtein_distance_list, axis=1)\n",
    "    df['title_query_leven_sum'] = df['title_query_leven_list'].map(lambda x : np.nan if len(x) == 0 else np.sum(x))\n",
    "    df['title_query_leven_max'] = df['title_query_leven_list'].map(lambda x : np.nan if len(x) == 0 else np.max(x))\n",
    "    df['title_query_leven_min'] = df['title_query_leven_list'].map(lambda x : np.nan if len(x) == 0 else np.min(x))\n",
    "    df['title_query_leven_mean'] = df['title_query_leven_list'].map(lambda x : np.nan if len(x) == 0 else np.mean(x))\n",
    "    df['title_query_leven_std'] = df['title_query_leven_list'].map(lambda x : np.nan if len(x) == 0 else np.std(x))\n",
    "    return df\n",
    "\n",
    "test_df = get_title_query_levenshtein_distance_feature(test_df)\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Dump cache file failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/jieba/__init__.py\", line 152, in initialize\n",
      "    _replace_file(fpath, cache_file)\n",
      "PermissionError: [Errno 1] Operation not permitted: '/tmp/tmp_by0f_to' -> '/tmp/jieba.cache'\n",
      "Loading model cost 1.206 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  prefix                                   query_prediction  \\\n",
      "0  新生儿游泳  {\"新生儿游泳去黄疸吗\": \"0.013\", \"新生儿游泳时间\": \"0.012\", \"新生...   \n",
      "1    吴奇隆  {\"吴奇隆李小冉\": \"0.011\", \"吴奇隆电视剧\": \"0.021\", \"吴奇隆的综艺...   \n",
      "2      炎  {\"炎琥宁\": \"0.013\", \"炎武战神\": \"0.023\", \"炎帝\": \"0.020...   \n",
      "3     知乎  {\"知乎2018必读书单\": \"0.004\", \"知乎翎春君\": \"0.007\", \"知乎暗...   \n",
      "4    快递查  {\"快递查询询\": \"0.001\", \"快递查单号\": \"0.001\", \"快递查询自动识别...   \n",
      "\n",
      "               title tag  label  \\\n",
      "0       新生儿游泳到底好不好呀？  健康    NaN   \n",
      "1  吴奇隆刘诗诗公开恋情 马苏怎么办呢  影视    NaN   \n",
      "2                炎亚纶  百科    NaN   \n",
      "3                 知乎  应用    NaN   \n",
      "4               快递查询  应用    NaN   \n",
      "\n",
      "                               query_prediction_dict  \\\n",
      "0  {'新生儿游泳去黄疸吗': '0.013', '新生儿游泳时间': '0.012', '新生...   \n",
      "1  {'吴奇隆李小冉': '0.011', '吴奇隆电视剧': '0.021', '吴奇隆的综艺...   \n",
      "2  {'炎琥宁': '0.013', '炎武战神': '0.023', '炎帝': '0.020...   \n",
      "3  {'知乎2018必读书单': '0.004', '知乎翎春君': '0.007', '知乎暗...   \n",
      "4  {'快递查询询': '0.001', '快递查单号': '0.001', '快递查询自动识别...   \n",
      "\n",
      "                               query_prediction_keys  \\\n",
      "0  [新生儿游泳去黄疸吗, 新生儿游泳时间, 新生儿游泳的好处, 新生儿游泳视频教程, 新生儿游...   \n",
      "1  [吴奇隆李小冉, 吴奇隆电视剧, 吴奇隆的综艺, 吴奇隆微博, 吴奇隆宣布刘诗诗离婚, 吴奇...   \n",
      "2  [炎琥宁, 炎武战神, 炎帝, 炎凉和梁希城在楼梯做, 炎症, 炎琥宁的作用与功效, 炎龙铠...   \n",
      "3  [知乎2018必读书单, 知乎翎春君, 知乎暗网到底有多恐怖, 知乎网, 知乎好听到爆的名字...   \n",
      "4  [快递查询询, 快递查单号, 快递查询自动识别, 快递查询, 快递查询单号查询追踪, 快递查...   \n",
      "\n",
      "                             query_prediction_values  query_prediction_number  \\\n",
      "0  [0.013, 0.012, 0.263, 0.009, 0.042, 0.032, 0.0...                       10   \n",
      "1  [0.011, 0.021, 0.006, 0.009, 0.163, 0.014, 0.0...                       10   \n",
      "2  [0.013, 0.023, 0.02, 0.026, 0.02, 0.019, 0.02,...                       10   \n",
      "3  [0.004, 0.007, 0.016, 0.016, 0.027, 0.011, 0.0...                        9   \n",
      "4  [0.001, 0.001, 0.003, 0.553, 0.003, 0.292, 0.0...                        9   \n",
      "\n",
      "   query_prediction_max         ...          title_query_leven_sum  \\\n",
      "0                 0.332         ...                          5.024   \n",
      "1                 0.163         ...                          3.534   \n",
      "2                 0.077         ...                          0.906   \n",
      "3                 0.027         ...                          0.567   \n",
      "4                 0.553         ...                          0.830   \n",
      "\n",
      "   title_query_leven_max  title_query_leven_min  title_query_leven_mean  \\\n",
      "0                  2.324                  0.063                0.502400   \n",
      "1                  2.119                  0.084                0.353400   \n",
      "2                  0.234                  0.000                0.090600   \n",
      "3                  0.189                  0.016                0.063000   \n",
      "4                  0.584                  0.000                0.092222   \n",
      "\n",
      "   title_query_leven_std                     query_prediction_key_sentences  \\\n",
      "0               0.746708  新生儿游泳去黄疸吗新生儿游泳时间新生儿游泳的好处新生儿游泳视频教程新生儿游泳好吗新生儿游泳视...   \n",
      "1               0.592326  吴奇隆李小冉吴奇隆电视剧吴奇隆的综艺吴奇隆微博吴奇隆宣布刘诗诗离婚吴奇隆前妻吴奇隆年龄吴奇隆...   \n",
      "2               0.070659  炎琥宁炎武战神炎帝炎凉和梁希城在楼梯做炎症炎琥宁的作用与功效炎龙铠甲炎亚纶炎景熙陆沐擎免费阅...   \n",
      "3               0.054422  知乎2018必读书单知乎翎春君知乎暗网到底有多恐怖知乎网知乎好听到爆的名字知乎害了多少人知乎...   \n",
      "4               0.178594  快递查询询快递查单号快递查询自动识别快递查询快递查询单号查询追踪快递查询单号快递查询单号查询...   \n",
      "\n",
      "                    query_prediction_key_jieba_words  \\\n",
      "0  [新生儿, 游泳, 去, 黄疸, 吗, 新生儿, 游泳, 时间, 新生儿, 游泳, 的, 好...   \n",
      "1  [吴奇隆, 李小冉, 吴奇隆, 电视剧, 吴奇隆, 的, 综艺, 吴奇隆, 微博, 吴奇隆,...   \n",
      "2  [炎琥宁, 炎武, 战神, 炎帝, 炎凉, 和, 梁希城, 在, 楼梯, 做, 炎症, 炎琥...   \n",
      "3  [知乎, 2018, 必读书, 单知乎, 翎春君, 知乎, 暗网, 到底, 有, 多, 恐怖...   \n",
      "4  [快递, 查询, 询, 快递, 查单, 号, 快递, 查询, 自动识别, 快递, 查询, 快...   \n",
      "\n",
      "                              query_prediction_words  \\\n",
      "0  [[新生儿, 游泳, 去, 黄疸, 吗], [新生儿, 游泳, 时间], [新生儿, 游泳,...   \n",
      "1  [[吴奇隆, 李小冉], [吴奇隆, 电视剧], [吴奇隆, 的, 综艺], [吴奇隆, 微...   \n",
      "2  [[炎琥宁], [炎武, 战神], [炎帝], [炎凉, 和, 梁希城, 在, 楼梯, 做]...   \n",
      "3  [[知乎, 2018, 必读书, 单], [知乎, 翎春君], [知乎, 暗网, 到底, 有...   \n",
      "4  [[快递, 查询, 询], [快递, 查单, 号], [快递, 查询, 自动识别], [快递...   \n",
      "\n",
      "                     title_jieba_words  prefix_jieba_words  \n",
      "0             [新生儿, 游泳, 到底, 好不好, 呀, ？]           [新生儿, 游泳]  \n",
      "1  [吴奇隆, 刘诗, 诗, 公开, 恋情,  , 马苏, 怎么办, 呢]               [吴奇隆]  \n",
      "2                                [炎亚纶]                 [炎]  \n",
      "3                                 [知乎]                [知乎]  \n",
      "4                             [快递, 查询]             [快递, 查]  \n",
      "\n",
      "[5 rows x 67 columns]\n"
     ]
    }
   ],
   "source": [
    "#分词方法，调用结巴接口\n",
    "def jieba_seg_to_list(sentence, pos=False):\n",
    "    if not pos:\n",
    "        #不进行词性标注的分词方法\n",
    "        seg_list = jieba.cut(sentence)\n",
    "    else:\n",
    "        #进行词性标注的分词方法\n",
    "        seg_list = psg.cut(sentence)\n",
    "    return seg_list\n",
    "\n",
    "#去除干扰词\n",
    "def jieba_word_filter(seg_list, pos=False):\n",
    "    \n",
    "    filter_list = []\n",
    "    #根据pos参数选择是否词性过滤\n",
    "    #不进行词性过滤，则将词性都标记为n，表示全部保留\n",
    "    for seg in seg_list:\n",
    "        if not pos:\n",
    "            word = seg\n",
    "            flag = 'n'\n",
    "        else:\n",
    "            word = seg.word\n",
    "            flag = seg.flag\n",
    "        if not flag.startswith('n'):\n",
    "            continue\n",
    "        filter_list.append(word)\n",
    "    return filter_list\n",
    "\n",
    "def jieba_word_deal(sentence, pos=False):\n",
    "    #调用上面方式对数据集进行处理，处理后的每条数据仅保留非干扰词\n",
    "    seg_list = jieba_seg_to_list(sentence, pos)\n",
    "    filter_list = jieba_word_filter(seg_list, pos)\n",
    "    return filter_list\n",
    "\n",
    "def get_prefix_prediction_key_sentences(x):\n",
    "    prefix_prediction_key_sentences = \"\"\n",
    "    for temp in x:\n",
    "        if len(prefix_prediction_key_sentences) > 0:\n",
    "            prefix_prediction_key_sentences = prefix_prediction_key_sentences + temp\n",
    "        else:\n",
    "            prefix_prediction_key_sentences = temp\n",
    "    return prefix_prediction_key_sentences\n",
    "\n",
    "def get_max_query_key_sentences(x):\n",
    "    if len(x) == 0:\n",
    "        return \"\"\n",
    "    else:\n",
    "        return max(x, key=x.get)\n",
    "\n",
    "def get_jieba_word(df):\n",
    "    df['query_prediction_key_sentences'] = df['query_prediction_keys'].map(lambda x : get_prefix_prediction_key_sentences(x))\n",
    "#     df['query_prediction_key_sentences'] = df['query_prediction_dict'].map(lambda x : get_max_query_key_sentences(x))\n",
    "    df['query_prediction_key_jieba_words'] = df['query_prediction_key_sentences'].map(lambda x : jieba_word_deal(x, False))\n",
    "    df['query_prediction_words'] = df['query_prediction_keys'].map(lambda x : [jieba_word_deal(j, False) for j in x] if len(x) > 0 else np.nan)\n",
    "    df['title_jieba_words'] = df['title'].map(lambda x : jieba_word_deal(x, False))\n",
    "    df['prefix_jieba_words'] = df['prefix'].map(lambda x : jieba_word_deal(x, False))\n",
    "#     del df['query_prediction_key_sentences']\n",
    "    return df\n",
    "\n",
    "test_df = get_jieba_word(test_df)\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_t_word_match finish!!!\n",
      "q_t_jaccard finish!!!\n",
      "q_t_common_words finish!!!\n",
      "q_t_total_unique_words finish!!!\n",
      "q_t_wc_diff finish!!!\n",
      "q_t_wc_ratio finish!!!\n",
      "q_t_wc_diff_unique finish!!!\n",
      "q_t_wc_ratio_unique finish!!!\n",
      "p_t_word_match finish!!!\n",
      "p_t_jaccard finish!!!\n",
      "p_t_common_words finish!!!\n",
      "p_t_total_unique_words finish!!!\n",
      "p_t_wc_diff finish!!!\n",
      "p_t_wc_ratio finish!!!\n",
      "p_t_wc_diff_unique finish!!!\n",
      "p_t_wc_ratio_unique finish!!!\n",
      "p_q_word_match finish!!!\n",
      "p_q_jaccard finish!!!\n",
      "p_q_common_words finish!!!\n",
      "p_q_total_unique_words finish!!!\n",
      "p_q_wc_diff finish!!!\n",
      "p_q_wc_ratio finish!!!\n",
      "p_q_wc_diff_unique finish!!!\n",
      "p_q_wc_ratio_unique finish!!!\n",
      "  prefix                                   query_prediction  \\\n",
      "0  新生儿游泳  {\"新生儿游泳去黄疸吗\": \"0.013\", \"新生儿游泳时间\": \"0.012\", \"新生...   \n",
      "1    吴奇隆  {\"吴奇隆李小冉\": \"0.011\", \"吴奇隆电视剧\": \"0.021\", \"吴奇隆的综艺...   \n",
      "2      炎  {\"炎琥宁\": \"0.013\", \"炎武战神\": \"0.023\", \"炎帝\": \"0.020...   \n",
      "3     知乎  {\"知乎2018必读书单\": \"0.004\", \"知乎翎春君\": \"0.007\", \"知乎暗...   \n",
      "4    快递查  {\"快递查询询\": \"0.001\", \"快递查单号\": \"0.001\", \"快递查询自动识别...   \n",
      "\n",
      "               title tag  label  \\\n",
      "0       新生儿游泳到底好不好呀？  健康    NaN   \n",
      "1  吴奇隆刘诗诗公开恋情 马苏怎么办呢  影视    NaN   \n",
      "2                炎亚纶  百科    NaN   \n",
      "3                 知乎  应用    NaN   \n",
      "4               快递查询  应用    NaN   \n",
      "\n",
      "                               query_prediction_dict  \\\n",
      "0  {'新生儿游泳去黄疸吗': '0.013', '新生儿游泳时间': '0.012', '新生...   \n",
      "1  {'吴奇隆李小冉': '0.011', '吴奇隆电视剧': '0.021', '吴奇隆的综艺...   \n",
      "2  {'炎琥宁': '0.013', '炎武战神': '0.023', '炎帝': '0.020...   \n",
      "3  {'知乎2018必读书单': '0.004', '知乎翎春君': '0.007', '知乎暗...   \n",
      "4  {'快递查询询': '0.001', '快递查单号': '0.001', '快递查询自动识别...   \n",
      "\n",
      "                               query_prediction_keys  \\\n",
      "0  [新生儿游泳去黄疸吗, 新生儿游泳时间, 新生儿游泳的好处, 新生儿游泳视频教程, 新生儿游...   \n",
      "1  [吴奇隆李小冉, 吴奇隆电视剧, 吴奇隆的综艺, 吴奇隆微博, 吴奇隆宣布刘诗诗离婚, 吴奇...   \n",
      "2  [炎琥宁, 炎武战神, 炎帝, 炎凉和梁希城在楼梯做, 炎症, 炎琥宁的作用与功效, 炎龙铠...   \n",
      "3  [知乎2018必读书单, 知乎翎春君, 知乎暗网到底有多恐怖, 知乎网, 知乎好听到爆的名字...   \n",
      "4  [快递查询询, 快递查单号, 快递查询自动识别, 快递查询, 快递查询单号查询追踪, 快递查...   \n",
      "\n",
      "                             query_prediction_values  query_prediction_number  \\\n",
      "0  [0.013, 0.012, 0.263, 0.009, 0.042, 0.032, 0.0...                       10   \n",
      "1  [0.011, 0.021, 0.006, 0.009, 0.163, 0.014, 0.0...                       10   \n",
      "2  [0.013, 0.023, 0.02, 0.026, 0.02, 0.019, 0.02,...                       10   \n",
      "3  [0.004, 0.007, 0.016, 0.016, 0.027, 0.011, 0.0...                        9   \n",
      "4  [0.001, 0.001, 0.003, 0.553, 0.003, 0.292, 0.0...                        9   \n",
      "\n",
      "   query_prediction_max         ...           p_t_wc_diff_unique  \\\n",
      "0                 0.332         ...                            4   \n",
      "1                 0.163         ...                            8   \n",
      "2                 0.077         ...                            0   \n",
      "3                 0.027         ...                            0   \n",
      "4                 0.553         ...                            0   \n",
      "\n",
      "   p_t_wc_ratio_unique  p_q_word_match  p_q_jaccard  p_q_common_words  \\\n",
      "0                  3.0        0.210526     0.117647                 2   \n",
      "1                  9.0        0.133333     0.071429                 1   \n",
      "2                  1.0        0.000000     0.000000                 0   \n",
      "3                  1.0        0.074074     0.038462                 1   \n",
      "4                  1.0        0.307692     0.181818                 2   \n",
      "\n",
      "   p_q_total_unique_words  p_q_wc_diff  p_q_wc_ratio  p_q_wc_diff_unique  \\\n",
      "0                      17           37          19.5                  15   \n",
      "1                      14           25          26.0                  13   \n",
      "2                      26           27          28.0                  24   \n",
      "3                      26           29          30.0                  25   \n",
      "4                      11           31          16.5                   9   \n",
      "\n",
      "   p_q_wc_ratio_unique  \n",
      "0                  8.5  \n",
      "1                 14.0  \n",
      "2                 25.0  \n",
      "3                 26.0  \n",
      "4                  5.5  \n",
      "\n",
      "[5 rows x 91 columns]\n"
     ]
    }
   ],
   "source": [
    "def word_match_share(df):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in df[0]:\n",
    "        q1words[word] = 1\n",
    "    for word in df[1]:\n",
    "        q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
    "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
    "    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n",
    "    return R\n",
    "\n",
    "def jaccard(df):\n",
    "    wic = set(df[0]).intersection(set(df[1]))\n",
    "    uw = set(df[0]).union(df[1])\n",
    "    if len(uw) == 0:\n",
    "        uw = [1]\n",
    "    return (len(wic) / len(uw))\n",
    "\n",
    "def common_words(df):\n",
    "    return len(set(df[0]).intersection(set(df[1])))\n",
    "\n",
    "def total_unique_words(df):\n",
    "    return len(set(df[0]).union(df[1]))\n",
    "\n",
    "def wc_diff(df):\n",
    "    return abs(len(df[0]) - len(df[1]))\n",
    "\n",
    "def wc_ratio(df):\n",
    "    l1 = len(df[0])*1.0 \n",
    "    l2 = len(df[1])\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "\n",
    "def wc_diff_unique(df):\n",
    "    return abs(len(set(df[0])) - len(set(df[1])))\n",
    "    \n",
    "def wc_ratio_unique(df):\n",
    "    l1 = len(set(df[0])) * 1.0\n",
    "    l2 = len(set(df[1]))\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "    \n",
    "def tfidf_word_match_share(df, weights=None):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in df[0]:\n",
    "        q1words[word] = 1\n",
    "    for word in df[1]:\n",
    "        q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
    "    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "    R = np.sum(shared_weights) / np.sum(total_weights)\n",
    "    return R\n",
    "\n",
    "def deal_word_for_all(train_df, fea1, fea2, func, colName):\n",
    "    train_df[colName] = train_df[[fea1, fea2]].apply(func, axis=1)\n",
    "#     valid_df[colName] = valid_df[[fea1, fea2]].apply(func, axis=1)\n",
    "    print(colName + ' finish!!!')\n",
    "    return train_df\n",
    "                   \n",
    "def get_weight(count, eps=10000, min_count=2):\n",
    "    if count < min_count:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 / (count + eps)\n",
    "\n",
    "def get_word_statistic_feature(train_df, col_list):\n",
    "#     df = pd.concat([train_df[['query_prediction_key_jieba_words', 'title_jieba_words', 'prefix_jieba_words']], valid_df[['query_prediction_key_jieba_words', 'title_jieba_words', 'prefix_jieba_words']]])\n",
    "#     train_qs = pd.Series(df['query_prediction_key_jieba_words'].tolist() + df['title_jieba_words'].tolist() + df['prefix_jieba_words'].tolist())\n",
    "#     words = [x for y in train_qs for x in y]\n",
    "#     counts = Counter(words)\n",
    "#     weights = {word: get_weight(count) for word, count in counts.items()}\n",
    "    for col in col_list:\n",
    "        fea1 = col[0]\n",
    "        fea2 = col[1]\n",
    "        train_df = deal_word_for_all(train_df, fea1, fea2, word_match_share, fea1[0] + '_' + fea2[0] + '_word_match')\n",
    "        train_df = deal_word_for_all(train_df, fea1, fea2, jaccard, fea1[0] + '_' + fea2[0] + '_jaccard')\n",
    "        train_df = deal_word_for_all(train_df, fea1, fea2, common_words, fea1[0] + '_' + fea2[0] + '_common_words')\n",
    "        train_df = deal_word_for_all(train_df, fea1, fea2, total_unique_words, fea1[0] + '_' + fea2[0] + '_total_unique_words')\n",
    "        train_df = deal_word_for_all(train_df, fea1, fea2, wc_diff, fea1[0] + '_' + fea2[0] + '_wc_diff')\n",
    "        train_df = deal_word_for_all(train_df, fea1, fea2, wc_ratio, fea1[0] + '_' + fea2[0] + '_wc_ratio')\n",
    "        train_df = deal_word_for_all(train_df, fea1, fea2, wc_diff_unique, fea1[0] + '_' + fea2[0] + '_wc_diff_unique')\n",
    "        train_df = deal_word_for_all(train_df, fea1, fea2, wc_ratio_unique, fea1[0] + '_' + fea2[0] + '_wc_ratio_unique')\n",
    "#         f = functools.partial(tfidf_word_match_share, weights=weights)\n",
    "#         train_df, valid_df = deal_word_for_all(train_df, valid_df, fea1, fea2, f, fea1[0] + '_' + fea2[0] + '_tfidf_word_match_share')\n",
    "    return train_df\n",
    "\n",
    "col_list = [['query_prediction_key_jieba_words', 'title_jieba_words'], ['prefix_jieba_words', 'title_jieba_words'], ['prefix_jieba_words', 'query_prediction_key_jieba_words']]\n",
    "test_df = get_word_statistic_feature(test_df, col_list)\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 加载word2vec模型\n",
    "model = word2vec.Word2Vec.load('../temp/B_word2vec.model')\n",
    "word_wv = model.wv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_w2v_array(word_list, word_wv, num_features):\n",
    "    word_vectors = np.zeros((len(word_list), num_features))\n",
    "    for i in range(len(word_list)):\n",
    "        if str(word_list[i]) in word_wv.vocab.keys():\n",
    "            word_vectors[i][:] = word_wv[str(word_list[i])]\n",
    "    mean_array = np.mean(word_vectors, axis=0)\n",
    "    return mean_array\n",
    "\n",
    "num_features = 500\n",
    "test_df['title_jieba_array'] = test_df['title_jieba_words'].map(lambda x : get_w2v_array(x, word_wv, num_features))\n",
    "\n",
    "test_df['prefix_jieba_array'] = test_df['prefix_jieba_words'].map(lambda x : get_w2v_array(x, word_wv, num_features))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot : finish!!!\n",
      "norm : finish!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:67: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:26: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_maximum(a, axis, None, out, keepdims)\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:29: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_minimum(a, axis, None, out, keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine : finish!!!\n"
     ]
    }
   ],
   "source": [
    "def get_title_prefix_similarity(df, f_similarity):\n",
    "    title_array = df['title_jieba_array']\n",
    "    prefix_array = df['prefix_jieba_array']\n",
    "    similarity = 0\n",
    "    if f_similarity == 'dot':\n",
    "        similarity = np.dot(title_array, prefix_array)\n",
    "    elif f_similarity == 'norm':\n",
    "        similarity = np.linalg.norm(title_array - prefix_array)\n",
    "    else:\n",
    "        similarity = np.dot(title_array,prefix_array) / (np.linalg.norm(title_array) * np.linalg.norm(prefix_array))\n",
    "    return similarity\n",
    "\n",
    "# def get_title_query_similarity(df, f_similarity, word_wv, num_features):\n",
    "#     title_array = df['title_jieba_array']\n",
    "#     query_prediction_words = df['query_prediction_words']\n",
    "#     query_prediction_keys = df['query_prediction_keys']\n",
    "#     query_prediction_dict = df['query_prediction_dict']\n",
    "#     if len(query_prediction_keys) <= 0:\n",
    "#         return np.nan\n",
    "#     similarity = 0\n",
    "#     if f_similarity == 'dot':\n",
    "#         i = 0\n",
    "#         for key in query_prediction_keys:\n",
    "#             key_array = get_w2v_array(query_prediction_words[i], word_wv, num_features)\n",
    "#             similarity = similarity + np.dot(title_array, key_array) * float(query_prediction_dict[key])\n",
    "#             i = i + 1\n",
    "#     elif f_similarity == 'norm':\n",
    "#         i = 0\n",
    "#         for key in query_prediction_keys:\n",
    "#             key_array = get_w2v_array(query_prediction_words[i], word_wv, num_features)\n",
    "#             similarity = similarity + np.linalg.norm(title_array - key_array) * float(query_prediction_dict[key])\n",
    "#             i = i + 1\n",
    "#     else:\n",
    "#         i = 0\n",
    "#         for key in query_prediction_keys:\n",
    "#             key_array = get_w2v_array(query_prediction_words[i], word_wv, num_features)\n",
    "#             similarity = similarity + (np.dot(title_array, key_array) / (np.linalg.norm(title_array) * np.linalg.norm(key_array))) * float(query_prediction_dict[key])\n",
    "#             i = i + 1\n",
    "#     return similarity\n",
    "\n",
    "def get_title_query_similarity_list(df, f_similarity, word_wv, num_features):\n",
    "    title_array = df['title_jieba_array']\n",
    "    query_prediction_words = df['query_prediction_words']\n",
    "    query_prediction_keys = df['query_prediction_keys']\n",
    "    query_prediction_dict = df['query_prediction_dict']\n",
    "    similarity_list = list()\n",
    "    if len(query_prediction_keys) <= 0:\n",
    "        return similarity_list\n",
    "    if f_similarity == 'dot':\n",
    "        i = 0\n",
    "        for key in query_prediction_keys:\n",
    "            key_array = get_w2v_array(query_prediction_words[i], word_wv, num_features)\n",
    "            similarity = np.dot(title_array, key_array) * float(query_prediction_dict[key])\n",
    "            similarity_list.append(similarity)\n",
    "            i = i + 1\n",
    "    elif f_similarity == 'norm':\n",
    "        i = 0\n",
    "        for key in query_prediction_keys:\n",
    "            key_array = get_w2v_array(query_prediction_words[i], word_wv, num_features)\n",
    "            similarity = np.linalg.norm(title_array - key_array) * float(query_prediction_dict[key])\n",
    "            similarity_list.append(similarity)\n",
    "            i = i + 1\n",
    "    else:\n",
    "        i = 0\n",
    "        for key in query_prediction_keys:\n",
    "            key_array = get_w2v_array(query_prediction_words[i], word_wv, num_features)\n",
    "            similarity = (np.dot(title_array, key_array) / (np.linalg.norm(title_array) * np.linalg.norm(key_array))) * float(query_prediction_dict[key])\n",
    "            similarity_list.append(similarity)\n",
    "            i = i + 1\n",
    "    return similarity_list\n",
    "\n",
    "def get_similarity_feature(train_df):\n",
    "    f_list = ['dot', 'norm', 'cosine']\n",
    "    for fun in f_list:\n",
    "        f_prefix_similarity = functools.partial(get_title_prefix_similarity, f_similarity=fun)\n",
    "        train_df['title_prefix_' + fun + '_similarity'] = train_df[['title_jieba_array', 'prefix_jieba_array']].apply(f_prefix_similarity, axis=1)\n",
    "#         f_query_similarity = functools.partial(get_title_query_similarity, f_similarity=fun, word_wv=word_wv, num_features=num_features)\n",
    "#         train_df['title_query_' + fun + '_similarity'] = train_df[['title_jieba_array', 'query_prediction_words', 'query_prediction_keys', 'query_prediction_dict']].apply(f_query_similarity, axis=1)\n",
    "#         valid_df['title_query_' + fun + '_similarity'] = valid_df[['title_jieba_array', 'query_prediction_words', 'query_prediction_keys', 'query_prediction_dict']].apply(f_query_similarity, axis=1)\n",
    "        f_query_similarity_list = functools.partial(get_title_query_similarity_list, f_similarity=fun, word_wv=word_wv, num_features=num_features)\n",
    "        train_df['title_query_' + fun + '_similarity_list'] = train_df[['title_jieba_array', 'query_prediction_words', 'query_prediction_keys', 'query_prediction_dict']].apply(f_query_similarity_list, axis=1)\n",
    "        train_df['title_query_' + fun + '_similarity'] = train_df['title_query_' + fun + '_similarity_list'].map(lambda x : np.nan if len(x)==0 else np.sum(x))\n",
    "        train_df['title_query_' + fun + '_similarity_max'] = train_df['title_query_' + fun + '_similarity_list'].map(lambda x : np.nan if len(x)==0 else np.max(x))\n",
    "        train_df['title_query_' + fun + '_similarity_min'] = train_df['title_query_' + fun + '_similarity_list'].map(lambda x : np.nan if len(x)==0 else np.min(x))\n",
    "        train_df['title_query_' + fun + '_similarity_mean'] = train_df['title_query_' + fun + '_similarity_list'].map(lambda x : np.nan if len(x)==0 else np.mean(x))\n",
    "        train_df['title_query_' + fun + '_similarity_std'] = train_df['title_query_' + fun + '_similarity_list'].map(lambda x : np.nan if len(x)==0 else np.std(x))\n",
    "        print(fun + ' : finish!!!')\n",
    "    return train_df\n",
    "\n",
    "test_df = get_similarity_feature(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merge_fea = [\n",
    "#     'query_prediction_number', 'query_prediction_max', 'query_prediction_min', 'query_prediction_mean', 'query_prediction_std',\n",
    "       'prefix_count', 'prefix_rate',\n",
    " 'title_count', 'title_rate', 'tag_count', 'tag_rate',\n",
    " 'query_prediction_count', 'query_prediction_rate', 'prefix_title_count',\n",
    " 'prefix_title_rate',  'prefix_tag_count', 'prefix_tag_rate',\n",
    " 'title_tag_count', 'title_tag_rate',\n",
    "    'prefix_click_number', 'title_click_number', 'query_prediction_click_number', 'prefix_tag_click_number', \n",
    "    'prefix_title_click_number', 'title_tag_click_number',\n",
    "    'is_title_in_query', 'is_prefix_in_title', \n",
    "#     'title_tag_types', 'prefix_tag_types', 'tag_title_types', 'tag_prefix_types',\n",
    "#  'title_prefix_types', 'prefix_title_types', 'tag_query_prediction_types', 'title_query_prediction_types',\n",
    "      'prefix_len', 'title_len',\n",
    " 'query_prediction_key_len_max', 'query_prediction_key_len_min',\n",
    " 'query_prediction_key_len_mean', 'query_prediction_key_len_std',\n",
    " 'len_title-prefix', 'len_prefix/title', 'len_mean-title', 'len_mean/title',\n",
    "    'q_t_word_match', 'q_t_common_words',\n",
    "#      'q_t_jaccard', 'p_t_jaccard', 'p_q_jaccard', \n",
    "#     'q_t_tfidf_word_match_share', 'p_t_tfidf_word_match_share', 'p_q_tfidf_word_match_share', \n",
    " 'q_t_total_unique_words', 'q_t_wc_diff', 'q_t_wc_ratio',\n",
    " 'q_t_wc_diff_unique', 'q_t_wc_ratio_unique',\n",
    " 'p_t_word_match', 'p_t_common_words',\n",
    " 'p_t_total_unique_words', 'p_t_wc_diff', 'p_t_wc_ratio',\n",
    " 'p_t_wc_diff_unique', 'p_t_wc_ratio_unique',\n",
    " 'p_q_word_match', 'p_q_common_words',\n",
    " 'p_q_total_unique_words', 'p_q_wc_diff', 'p_q_wc_ratio',\n",
    " 'p_q_wc_diff_unique', 'p_q_wc_ratio_unique',\n",
    "    'title_prefix_dot_similarity',\n",
    " 'title_query_dot_similarity', 'title_prefix_norm_similarity',\n",
    " 'title_query_norm_similarity', 'title_prefix_cosine_similarity',\n",
    " 'title_query_cosine_similarity',\n",
    "    'title_query_dot_similarity_max', 'title_query_dot_similarity_min',\n",
    " 'title_query_dot_similarity_mean', 'title_query_dot_similarity_std',\n",
    "    'title_query_norm_similarity_min', 'title_query_norm_similarity_mean',\n",
    " 'title_query_norm_similarity_std',\n",
    "    'title_query_cosine_similarity_max', 'title_query_cosine_similarity_min',\n",
    " 'title_query_cosine_similarity_mean', 'title_query_cosine_similarity_std',\n",
    "    'title_prefix_leven', 'title_prefix_leven_rate',\n",
    " 'title_query_leven_sum', 'title_query_leven_max', 'title_query_leven_min',\n",
    " 'title_query_leven_mean', 'title_query_leven_std',\n",
    "#     'prefix', 'query_prediction', 'title', 'tag', 'index', 'label'\n",
    "      ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv('../temp/B_train_online_df.csv')\n",
    "train_dataset = pd.concat([train_df, train_dataset[merge_fea]], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fea = [\n",
    "    'query_prediction_number', 'query_prediction_max', 'query_prediction_min', 'query_prediction_mean', 'query_prediction_std',\n",
    "       'prefix_count', 'prefix_rate',\n",
    " 'title_count', 'title_rate', 'tag_count', 'tag_rate',\n",
    " 'query_prediction_count', 'query_prediction_rate', 'prefix_title_count',\n",
    " 'prefix_title_rate',  'prefix_tag_count', 'prefix_tag_rate',\n",
    " 'title_tag_count', 'title_tag_rate',\n",
    "    'prefix_click_number', 'title_click_number', 'query_prediction_click_number', 'prefix_tag_click_number', \n",
    "    'prefix_title_click_number', 'title_tag_click_number',\n",
    "    'is_title_in_query', 'is_prefix_in_title', \n",
    "    'title_tag_types', 'prefix_tag_types', 'tag_title_types', 'tag_prefix_types',\n",
    " 'title_prefix_types', 'prefix_title_types', 'tag_query_prediction_types', 'title_query_prediction_types',\n",
    "      'prefix_len', 'title_len',\n",
    " 'query_prediction_key_len_max', 'query_prediction_key_len_min',\n",
    " 'query_prediction_key_len_mean', 'query_prediction_key_len_std',\n",
    " 'len_title-prefix', 'len_prefix/title', 'len_mean-title', 'len_mean/title',\n",
    "    'q_t_word_match', 'q_t_common_words',\n",
    " 'q_t_total_unique_words', 'q_t_wc_diff', 'q_t_wc_ratio',\n",
    " 'q_t_wc_diff_unique', 'q_t_wc_ratio_unique',\n",
    " 'p_t_word_match', 'p_t_common_words',\n",
    " 'p_t_total_unique_words', 'p_t_wc_diff', 'p_t_wc_ratio',\n",
    " 'p_t_wc_diff_unique', 'p_t_wc_ratio_unique',\n",
    " 'p_q_word_match', 'p_q_common_words',\n",
    " 'p_q_total_unique_words', 'p_q_wc_diff', 'p_q_wc_ratio',\n",
    " 'p_q_wc_diff_unique', 'p_q_wc_ratio_unique',\n",
    "    'title_prefix_dot_similarity',\n",
    " 'title_query_dot_similarity', 'title_prefix_norm_similarity',\n",
    " 'title_query_norm_similarity', 'title_prefix_cosine_similarity',\n",
    " 'title_query_cosine_similarity',\n",
    "    'title_query_dot_similarity_max', 'title_query_dot_similarity_min',\n",
    " 'title_query_dot_similarity_mean', 'title_query_dot_similarity_std',\n",
    "    'title_query_norm_similarity_min', 'title_query_norm_similarity_mean',\n",
    " 'title_query_norm_similarity_std',\n",
    "    'title_query_cosine_similarity_max', 'title_query_cosine_similarity_min',\n",
    " 'title_query_cosine_similarity_mean', 'title_query_cosine_similarity_std',\n",
    "    'title_prefix_leven', 'title_prefix_leven_rate',\n",
    " 'title_query_leven_sum', 'title_query_leven_max', 'title_query_leven_min',\n",
    " 'title_query_leven_mean', 'title_query_leven_std',\n",
    "      ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py:99: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37952399932839065\n",
      "                                   importance\n",
      "prefix_title_rate                        8312\n",
      "prefix_tag_rate                          6614\n",
      "prefix_rate                              5219\n",
      "title_tag_rate                           3798\n",
      "query_prediction_rate                    2704\n",
      "tag_rate                                 2625\n",
      "title_rate                               2588\n",
      "prefix_click_number                      2325\n",
      "prefix_title_count                       1904\n",
      "prefix_title_click_number                1832\n",
      "title_tag_count                          1715\n",
      "q_t_word_match                           1655\n",
      "tag_count                                1649\n",
      "prefix_title_types                       1525\n",
      "prefix_tag_count                         1368\n",
      "title_query_norm_similarity_std          1332\n",
      "prefix_tag_click_number                  1207\n",
      "query_prediction_click_number            1157\n",
      "title_query_norm_similarity              1033\n",
      "title_tag_click_number                   1009\n",
      "title_query_leven_min                     979\n",
      "title_query_norm_similarity_min           953\n",
      "tag_title_types                           948\n",
      "prefix_tag_types                          927\n",
      "query_prediction_key_len_mean             915\n",
      "prefix_count                              908\n",
      "title_prefix_leven_rate                   877\n",
      "title_query_cosine_similarity             838\n",
      "title_query_leven_sum                     829\n",
      "title_prefix_types                        809\n",
      "...                                       ...\n",
      "title_query_dot_similarity_std            387\n",
      "title_query_leven_std                     380\n",
      "q_t_wc_ratio                              375\n",
      "title_query_dot_similarity_min            371\n",
      "query_prediction_key_len_max              369\n",
      "title_query_cosine_similarity_std         351\n",
      "p_q_wc_ratio                              334\n",
      "p_q_word_match                            277\n",
      "p_t_total_unique_words                    258\n",
      "p_q_wc_diff                               257\n",
      "q_t_wc_diff                               255\n",
      "title_query_prediction_types              240\n",
      "prefix_len                                205\n",
      "len_title-prefix                          193\n",
      "p_t_wc_ratio_unique                       176\n",
      "title_prefix_leven                        167\n",
      "is_title_in_query                         164\n",
      "q_t_total_unique_words                    160\n",
      "p_q_wc_diff_unique                        156\n",
      "query_prediction_key_len_min              155\n",
      "q_t_wc_diff_unique                        155\n",
      "p_t_wc_ratio                              152\n",
      "p_q_total_unique_words                    143\n",
      "tag_prefix_types                          127\n",
      "p_t_wc_diff                               118\n",
      "query_prediction_number                   104\n",
      "p_t_wc_diff_unique                        101\n",
      "p_q_common_words                           90\n",
      "p_t_common_words                           68\n",
      "tag_query_prediction_types                  0\n",
      "\n",
      "[90 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: FutureWarning: by argument to sort_index is deprecated, pls use .sort_values(by=...)\n"
     ]
    }
   ],
   "source": [
    "test_dataset = test_df\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    boosting_type='gbdt', num_leaves=127, max_depth=-1, n_estimators=5000, objective='binary',\n",
    "    subsample=0.8, colsample_bytree=1, subsample_freq=1,\n",
    "    learning_rate=0.01, random_state=2018, n_jobs=-1, num_boost_round=666\n",
    ")\n",
    "\n",
    "test_dataset['predicted_score'] = 0\n",
    "\n",
    "# lgb_model.fit(train_df[fea], train_df['label'], eval_set=[(train_df[fea], train_df['label']),\n",
    "#                             (valid_df[fea], valid_df['label'])], early_stopping_rounds=50, eval_metric='auc')\n",
    "lgb_model.fit(train_dataset[fea], train_dataset['label'], eval_metric='auc')\n",
    "test_pred = lgb_model.predict_proba(test_dataset[fea], num_iteration=666)[:, 1]\n",
    "print(np.mean(test_pred))\n",
    "\n",
    "fscore = lgb_model.booster_.feature_importance()\n",
    "feaNames = lgb_model.booster_.feature_name()\n",
    "scoreDf = pd.DataFrame(index=feaNames, columns=['importance'], data=fscore)\n",
    "print(scoreDf.sort_index(by=['importance'], ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3740921946553892\n",
      "0.4361415969060372\n"
     ]
    }
   ],
   "source": [
    "test_dataset['predicted_score'] = test_pred\n",
    "\n",
    "train_prefix_set = set(train_df['prefix'])\n",
    "\n",
    "test_dataset['is_prefix_in_train'] = test_dataset['prefix'].map(lambda x : 1 if x in train_prefix_set else 0)\n",
    "print(np.mean(test_dataset[test_dataset.is_prefix_in_train == 1]['predicted_score']))\n",
    "print(np.mean(test_dataset[test_dataset.is_prefix_in_train == 0]['predicted_score']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original mean :  0.3740921946553892\n",
      "0.43615756685494683\n"
     ]
    }
   ],
   "source": [
    "test_prefix0_df = test_dataset[test_dataset.is_prefix_in_train == 1].copy()\n",
    "\n",
    "#定义调整函数\n",
    "def resultAdjustment(result_df, t):\n",
    "    result_df_temp = result_df.copy()\n",
    "    result_df_temp['x'] = result_df_temp.predicted_score.map(lambda x: -(math.log(((1 - x) / x), math.e)))\n",
    "    result_df_temp['adjust_result'] = result_df_temp.x.map(lambda x: 1 / (1 + math.exp(-(x + t)))) \n",
    "    print(result_df_temp['adjust_result'].mean())\n",
    "    return result_df_temp['adjust_result']\n",
    "\n",
    "print('original mean : ', test_prefix0_df['predicted_score'].mean())\n",
    "test_df_after = resultAdjustment(test_prefix0_df, 0.45385)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4361415969060372\n",
      "0.43615756685494683\n"
     ]
    }
   ],
   "source": [
    "test_dataset.loc[test_dataset.is_prefix_in_train == 1, 'predicted_score'] = test_df_after\n",
    "print(np.mean(test_dataset['predicted_score'][test_dataset.is_prefix_in_train == 0]))\n",
    "print(np.mean(test_dataset['predicted_score'][test_dataset.is_prefix_in_train == 1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40784\n"
     ]
    }
   ],
   "source": [
    "test_dataset['predicted_label'] = test_dataset['predicted_score'].map(lambda x : 1 if x > 0.49 else 0)\n",
    "print(np.mean(test_dataset['predicted_label']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 导出预测结果\n",
    "def exportResult(df, fileName):\n",
    "    df.to_csv('../result/%s.csv' % fileName, header=False, index=False)\n",
    "\n",
    "exportResult(test_dataset[['predicted_label']], 'result')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
