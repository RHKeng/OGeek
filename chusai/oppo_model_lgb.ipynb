{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import gc\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import chi2, SelectPercentile\n",
    "import math\n",
    "from sklearn.metrics import f1_score\n",
    "import jieba\n",
    "import jieba.posseg as psg\n",
    "from collections import Counter\n",
    "import functools\n",
    "from gensim.models import word2vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 4 columns):\n",
      "prefix              50000 non-null object\n",
      "query_prediction    50000 non-null object\n",
      "title               50000 non-null object\n",
      "tag                 50000 non-null object\n",
      "dtypes: object(4)\n",
      "memory usage: 1.5+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_table('../data/oppo_round1_train_20180929.txt', names=['prefix', 'query_prediction', 'title', 'tag', 'label'], header=None, quoting=3)\n",
    "valid_df = pd.read_table('../data/oppo_round1_vali_20180929.txt', names=['prefix', 'query_prediction', 'title', 'tag', 'label'], header=None, quoting=3)\n",
    "test_df = pd.read_table('../data/oppo_round1_test_A_20180929.txt', names=['prefix', 'query_prediction', 'title', 'tag'], header=None, quoting=3)\n",
    "print(test_df.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  prefix                                   query_prediction            title  \\\n",
      "0     小品  {\"小品大全宋小宝\": \"0.009\", \"小品相亲\": \"0.012\", \"小品剧本\": ...               小品   \n",
      "1   1368  {\"13688cc赛马会\": \"0.059\", \"13685367892\": \"0.124\"...  HCG大于1368%2C正常吗   \n",
      "2   1368  {\"13688cc赛马会\": \"0.059\", \"13685367892\": \"0.124\"...            1368年   \n",
      "3     银耳  {\"银耳汤的功效\": \"0.012\", \"银耳为什么不能天天吃\": \"0.009\", \"银耳...         银耳红枣汤的做法   \n",
      "4   月经量少  {\"月经量少喝红糖水好吗\": \"0.010\", \"月经量少该怎么调理\": \"0.016\", ...         月经量少怎么调理   \n",
      "\n",
      "  tag  label                              query_prediction_dict  \\\n",
      "0  阅读      0  {'小品大全宋小宝': '0.009', '小品相亲': '0.012', '小品剧本': ...   \n",
      "1  健康      0  {'13688cc赛马会': '0.059', '13685367892': '0.124'...   \n",
      "2  百科      1  {'13688cc赛马会': '0.059', '13685367892': '0.124'...   \n",
      "3  菜谱      1  {'银耳汤的功效': '0.012', '银耳为什么不能天天吃': '0.009', '银耳...   \n",
      "4  百科      0  {'月经量少喝红糖水好吗': '0.010', '月经量少该怎么调理': '0.016', ...   \n",
      "\n",
      "                               query_prediction_keys  \\\n",
      "0  [小品大全宋小宝, 小品相亲, 小品剧本, 小品搞笑大全, 小品不差钱, 小品搞笑大全剧本,...   \n",
      "1  [13688cc赛马会, 13685367892, 13688cc, 1368个单词就够了,...   \n",
      "2  [13688cc赛马会, 13685367892, 13688cc, 1368个单词就够了,...   \n",
      "3  [银耳汤的功效, 银耳为什么不能天天吃, 银耳莲子羹, 银耳的做法, 银耳的功效, 银耳莲子...   \n",
      "4  [月经量少喝红糖水好吗, 月经量少该怎么调理, 月经量少怎么, 月经量少发黑, 月经量少是什...   \n",
      "\n",
      "                             query_prediction_values  query_prediction_number  \\\n",
      "0  [0.009, 0.012, 0.02, 0.066, 0.007, 0.01, 0.198...                       10   \n",
      "1  [0.059, 0.124, 0.029, 0.07, 0.022, 0.042, 0.08...                        9   \n",
      "2  [0.059, 0.124, 0.029, 0.07, 0.022, 0.042, 0.08...                        9   \n",
      "3  [0.012, 0.009, 0.05, 0.045, 0.053, 0.014, 0.05...                       10   \n",
      "4  [0.01, 0.016, 0.009, 0.009, 0.569, 0.016, 0.02...                       10   \n",
      "\n",
      "   query_prediction_max  query_prediction_min  query_prediction_mean  \\\n",
      "0                 0.198                 0.007               0.037300   \n",
      "1                 0.124                 0.022               0.057778   \n",
      "2                 0.124                 0.022               0.057778   \n",
      "3                 0.114                 0.009               0.040400   \n",
      "4                 0.569                 0.009               0.074700   \n",
      "\n",
      "   query_prediction_std  \n",
      "0              0.056023  \n",
      "1              0.031538  \n",
      "2              0.031538  \n",
      "3              0.030660  \n",
      "4              0.165089  \n"
     ]
    }
   ],
   "source": [
    "def get_float_list(x):\n",
    "    return_list = []\n",
    "    for temp in x:\n",
    "        return_list.append(float(temp))\n",
    "    return return_list\n",
    "\n",
    "# 处理跟query_prediction相关的统计特征\n",
    "def get_query_prediction_feature(df):\n",
    "    df['query_prediction_dict'] = df['query_prediction'].map(lambda x : eval(x))\n",
    "    df['query_prediction_keys'] = df['query_prediction_dict'].map(lambda x : list(x.keys()))\n",
    "    df['query_prediction_values'] = df['query_prediction_dict'].map(lambda x : get_float_list(list(x.values())))\n",
    "    df['query_prediction_number'] = df['query_prediction_keys'].map(lambda x : len(x))\n",
    "    df['query_prediction_max'] = df['query_prediction_values'].map(lambda x : np.nan if len(x) == 0 else np.max(x))\n",
    "    df['query_prediction_min'] = df['query_prediction_values'].map(lambda x : np.nan if len(x) == 0 else np.min(x))\n",
    "    df['query_prediction_mean'] = df['query_prediction_values'].map(lambda x : np.nan if len(x) == 0 else np.mean(x))\n",
    "    df['query_prediction_std'] = df['query_prediction_values'].map(lambda x : np.nan if len(x) == 0 else np.std(x))\n",
    "    return df\n",
    "\n",
    "train_df = get_query_prediction_feature(train_df)\n",
    "valid_df = get_query_prediction_feature(valid_df)\n",
    "test_df = get_query_prediction_feature(test_df)\n",
    "print(train_df.head())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 增加一个冷启动特征，根据prefix\n",
    "prefix_pivot_table = pd.pivot_table(train_df, index='prefix', values='label', aggfunc=len)\n",
    "prefix_pivot_table.reset_index(inplace=True)\n",
    "prefix_pivot_table.rename(columns={'label' : 'prefix_number'}, inplace=True)\n",
    "prefix_repeat_set = set(prefix_pivot_table['prefix'][prefix_pivot_table.prefix_number > 1])\n",
    "train_df['is_repeat_prefix'] = train_df['prefix'].map(lambda x : 1 if x in prefix_repeat_set else 0)\n",
    "valid_df['is_repeat_prefix'] = valid_df['prefix'].map(lambda x : 1 if x in prefix_repeat_set else 0)\n",
    "test_df['is_repeat_prefix'] = test_df['prefix'].map(lambda x : 1 if x in prefix_repeat_set else 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origion_rate_mean :  0.40002767378326565\n",
      "origion_rate_var :  0.09707128998059503\n",
      "alpha :  0.5890274369157756\n",
      "beta :  0.8834392835614426\n",
      "origion_rate_mean :  0.37554872005889733\n",
      "origion_rate_var :  0.18375012255029244\n",
      "alpha :  0.10374693833333963\n",
      "beta :  0.17250733386087524\n",
      "origion_rate_mean :  0.3155117577588836\n",
      "origion_rate_var :  0.02654243870294424\n",
      "alpha :  2.2516679174047067\n",
      "beta :  4.884889950988419\n",
      "origion_rate_mean :  0.39952573312087963\n",
      "origion_rate_var :  0.09550917645024579\n",
      "alpha :  0.6040238029749967\n",
      "beta :  0.9078282578589535\n",
      "  prefix                                   query_prediction            title  \\\n",
      "0     小品  {\"小品大全宋小宝\": \"0.009\", \"小品相亲\": \"0.012\", \"小品剧本\": ...               小品   \n",
      "1   1368  {\"13688cc赛马会\": \"0.059\", \"13685367892\": \"0.124\"...  HCG大于1368%2C正常吗   \n",
      "2   1368  {\"13688cc赛马会\": \"0.059\", \"13685367892\": \"0.124\"...            1368年   \n",
      "3     银耳  {\"银耳汤的功效\": \"0.012\", \"银耳为什么不能天天吃\": \"0.009\", \"银耳...         银耳红枣汤的做法   \n",
      "4   月经量少  {\"月经量少喝红糖水好吗\": \"0.010\", \"月经量少该怎么调理\": \"0.016\", ...         月经量少怎么调理   \n",
      "\n",
      "  tag  label                              query_prediction_dict  \\\n",
      "0  阅读      0  {'小品大全宋小宝': '0.009', '小品相亲': '0.012', '小品剧本': ...   \n",
      "1  健康      0  {'13688cc赛马会': '0.059', '13685367892': '0.124'...   \n",
      "2  百科      1  {'13688cc赛马会': '0.059', '13685367892': '0.124'...   \n",
      "3  菜谱      1  {'银耳汤的功效': '0.012', '银耳为什么不能天天吃': '0.009', '银耳...   \n",
      "4  百科      0  {'月经量少喝红糖水好吗': '0.010', '月经量少该怎么调理': '0.016', ...   \n",
      "\n",
      "                               query_prediction_keys  \\\n",
      "0  [小品大全宋小宝, 小品相亲, 小品剧本, 小品搞笑大全, 小品不差钱, 小品搞笑大全剧本,...   \n",
      "1  [13688cc赛马会, 13685367892, 13688cc, 1368个单词就够了,...   \n",
      "2  [13688cc赛马会, 13685367892, 13688cc, 1368个单词就够了,...   \n",
      "3  [银耳汤的功效, 银耳为什么不能天天吃, 银耳莲子羹, 银耳的做法, 银耳的功效, 银耳莲子...   \n",
      "4  [月经量少喝红糖水好吗, 月经量少该怎么调理, 月经量少怎么, 月经量少发黑, 月经量少是什...   \n",
      "\n",
      "                             query_prediction_values  query_prediction_number  \\\n",
      "0  [0.009, 0.012, 0.02, 0.066, 0.007, 0.01, 0.198...                       10   \n",
      "1  [0.059, 0.124, 0.029, 0.07, 0.022, 0.042, 0.08...                        9   \n",
      "2  [0.059, 0.124, 0.029, 0.07, 0.022, 0.042, 0.08...                        9   \n",
      "3  [0.012, 0.009, 0.05, 0.045, 0.053, 0.014, 0.05...                       10   \n",
      "4  [0.01, 0.016, 0.009, 0.009, 0.569, 0.016, 0.02...                       10   \n",
      "\n",
      "   query_prediction_max              ...                prefix_click_number  \\\n",
      "0                 0.198              ...                                 58   \n",
      "1                 0.124              ...                                  1   \n",
      "2                 0.124              ...                                  1   \n",
      "3                 0.114              ...                                205   \n",
      "4                 0.569              ...                                 68   \n",
      "\n",
      "   title_count  title_rate  title_click_number  tag_count  tag_rate  \\\n",
      "0           94    0.382957                  36      78409  0.263163   \n",
      "1            1    0.081290                   0     139704  0.297380   \n",
      "2            1    0.864833                   1     722096  0.396153   \n",
      "3          203    0.620357                 126      95444  0.413911   \n",
      "4           75    0.492901                  37     722096  0.396153   \n",
      "\n",
      "   tag_click_number  query_prediction_count  query_prediction_rate  \\\n",
      "0             20634                     150               0.386795   \n",
      "1             41545                       2               0.456746   \n",
      "2            286061                       2               0.456746   \n",
      "3             39506                     568               0.361018   \n",
      "4            286061                     185               0.367827   \n",
      "\n",
      "   query_prediction_click_number  \n",
      "0                             58  \n",
      "1                              1  \n",
      "2                              1  \n",
      "3                            205  \n",
      "4                             68  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "def getBayesSmoothParam(origion_rate):\n",
    "    origion_rate_mean = origion_rate.mean()\n",
    "    origion_rate_var = origion_rate.var()\n",
    "    alpha = origion_rate_mean / origion_rate_var * (origion_rate_mean * (1 - origion_rate_mean) - origion_rate_var)\n",
    "    beta = (1 - origion_rate_mean) / origion_rate_var * (origion_rate_mean * (1 - origion_rate_mean) - origion_rate_var)\n",
    "    print('origion_rate_mean : ', origion_rate_mean)\n",
    "    print('origion_rate_var : ', origion_rate_var)\n",
    "    print('alpha : ', alpha)\n",
    "    print('beta : ', beta)\n",
    "    return alpha, beta\n",
    "\n",
    "# 统计单维度的转化率特征\n",
    "def get_single_dimension_rate_feature(train_df, valid_df, test_df, fea_set):\n",
    "    for fea in fea_set:\n",
    "        temp_df = train_df[[fea, 'label']].copy()\n",
    "#         temp_df = train_df[[fea, 'label']][train_df.is_repeat_prefix == 1].copy()\n",
    "        temp_pivot_table = pd.pivot_table(temp_df, index=fea, values='label', aggfunc={len, np.mean, np.sum})\n",
    "        temp_pivot_table.reset_index(inplace=True)\n",
    "        temp_pivot_table.rename(columns={'len':fea + '_count', 'mean':fea + '_rate', 'sum':fea + '_click_number'}, inplace=True)\n",
    "        alpha, beta = getBayesSmoothParam(temp_pivot_table[fea + '_rate'])\n",
    "        temp_pivot_table[fea + '_rate'] = (temp_pivot_table[fea + '_click_number'] + alpha) / (temp_pivot_table[fea + '_count'] + alpha + beta)\n",
    "#         del temp_pivot_table[fea + '_click_number']\n",
    "        train_df = pd.merge(train_df, temp_pivot_table, on=fea, how='left')\n",
    "        valid_df = pd.merge(valid_df, temp_pivot_table, on=fea, how='left')\n",
    "        test_df = pd.merge(test_df, temp_pivot_table, on=fea, how='left')\n",
    "    return train_df, valid_df, test_df\n",
    "    \n",
    "fea_set = ['prefix', 'title', 'tag', 'query_prediction']\n",
    "train_df, valid_df, test_df = get_single_dimension_rate_feature(train_df, valid_df, test_df, fea_set)\n",
    "print(train_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origion_rate_mean :  0.3739739185109122\n",
      "origion_rate_var :  0.18828707562599176\n",
      "alpha :  0.09102778803323584\n",
      "beta :  0.15237899390409795\n",
      "origion_rate_mean :  0.38081147231232\n",
      "origion_rate_var :  0.1613523282902859\n",
      "alpha :  0.17569178599550558\n",
      "beta :  0.2856698030571823\n",
      "origion_rate_mean :  0.37681270456893634\n",
      "origion_rate_var :  0.18916100599546115\n",
      "alpha :  0.09096341835574066\n",
      "beta :  0.15043878823864787\n",
      "  prefix                                   query_prediction            title  \\\n",
      "0     小品  {\"小品大全宋小宝\": \"0.009\", \"小品相亲\": \"0.012\", \"小品剧本\": ...               小品   \n",
      "1   1368  {\"13688cc赛马会\": \"0.059\", \"13685367892\": \"0.124\"...  HCG大于1368%2C正常吗   \n",
      "2   1368  {\"13688cc赛马会\": \"0.059\", \"13685367892\": \"0.124\"...            1368年   \n",
      "3     银耳  {\"银耳汤的功效\": \"0.012\", \"银耳为什么不能天天吃\": \"0.009\", \"银耳...         银耳红枣汤的做法   \n",
      "4   月经量少  {\"月经量少喝红糖水好吗\": \"0.010\", \"月经量少该怎么调理\": \"0.016\", ...         月经量少怎么调理   \n",
      "\n",
      "  tag  label                              query_prediction_dict  \\\n",
      "0  阅读      0  {'小品大全宋小宝': '0.009', '小品相亲': '0.012', '小品剧本': ...   \n",
      "1  健康      0  {'13688cc赛马会': '0.059', '13685367892': '0.124'...   \n",
      "2  百科      1  {'13688cc赛马会': '0.059', '13685367892': '0.124'...   \n",
      "3  菜谱      1  {'银耳汤的功效': '0.012', '银耳为什么不能天天吃': '0.009', '银耳...   \n",
      "4  百科      0  {'月经量少喝红糖水好吗': '0.010', '月经量少该怎么调理': '0.016', ...   \n",
      "\n",
      "                               query_prediction_keys  \\\n",
      "0  [小品大全宋小宝, 小品相亲, 小品剧本, 小品搞笑大全, 小品不差钱, 小品搞笑大全剧本,...   \n",
      "1  [13688cc赛马会, 13685367892, 13688cc, 1368个单词就够了,...   \n",
      "2  [13688cc赛马会, 13685367892, 13688cc, 1368个单词就够了,...   \n",
      "3  [银耳汤的功效, 银耳为什么不能天天吃, 银耳莲子羹, 银耳的做法, 银耳的功效, 银耳莲子...   \n",
      "4  [月经量少喝红糖水好吗, 月经量少该怎么调理, 月经量少怎么, 月经量少发黑, 月经量少是什...   \n",
      "\n",
      "                             query_prediction_values  query_prediction_number  \\\n",
      "0  [0.009, 0.012, 0.02, 0.066, 0.007, 0.01, 0.198...                       10   \n",
      "1  [0.059, 0.124, 0.029, 0.07, 0.022, 0.042, 0.08...                        9   \n",
      "2  [0.059, 0.124, 0.029, 0.07, 0.022, 0.042, 0.08...                        9   \n",
      "3  [0.012, 0.009, 0.05, 0.045, 0.053, 0.014, 0.05...                       10   \n",
      "4  [0.01, 0.016, 0.009, 0.009, 0.569, 0.016, 0.02...                       10   \n",
      "\n",
      "   query_prediction_max           ...            \\\n",
      "0                 0.198           ...             \n",
      "1                 0.124           ...             \n",
      "2                 0.124           ...             \n",
      "3                 0.114           ...             \n",
      "4                 0.569           ...             \n",
      "\n",
      "   query_prediction_click_number  prefix_title_count  prefix_title_rate  \\\n",
      "0                             58                  94           0.382955   \n",
      "1                              1                   1           0.073208   \n",
      "2                              1                   1           0.877450   \n",
      "3                            205                 194           0.633695   \n",
      "4                             68                  66           0.469345   \n",
      "\n",
      "   prefix_title_click_number  prefix_tag_count  prefix_tag_rate  \\\n",
      "0                         36                46         0.068351   \n",
      "1                          0                 1         0.120225   \n",
      "2                          1                 1         0.804518   \n",
      "3                        123               194         0.633420   \n",
      "4                         31                66         0.469080   \n",
      "\n",
      "   prefix_tag_click_number  title_tag_count  title_tag_rate  \\\n",
      "0                        3               46        0.066844   \n",
      "1                        0                1        0.073275   \n",
      "2                        1                1        0.878815   \n",
      "3                      123              203        0.620400   \n",
      "4                       31               73        0.506421   \n",
      "\n",
      "   title_tag_click_number  \n",
      "0                       3  \n",
      "1                       0  \n",
      "2                       1  \n",
      "3                     126  \n",
      "4                      37  \n",
      "\n",
      "[5 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "# 统计双维度交叉转化率\n",
    "def get_jiaocha_dimension_rate_feature(train_df, valid_df, test_df, fea_set):\n",
    "    for i in range(len(fea_set)):\n",
    "        for j in range((i+1), len(fea_set)):\n",
    "            fea1 = fea_set[i]\n",
    "            fea2 = fea_set[j]\n",
    "            temp_df = train_df[[fea1, fea2, 'label']].copy()\n",
    "            temp_pivot_table = pd.pivot_table(temp_df, index=[fea1, fea2], values='label', aggfunc={len, np.mean, np.sum})\n",
    "            temp_pivot_table.reset_index(inplace=True)\n",
    "            temp_pivot_table.rename(columns={'len':fea1 + '_' + fea2 + '_count', 'mean':fea1 + '_' + fea2 + '_rate', 'sum':fea1 + '_' + fea2 + '_click_number'}, inplace=True)\n",
    "            alpha, beta = getBayesSmoothParam(temp_pivot_table[fea1 + '_' + fea2 + '_rate'])\n",
    "            temp_pivot_table[fea1 + '_' + fea2 + '_rate'] = (temp_pivot_table[fea1 + '_' + fea2 + '_click_number'] + alpha) / (temp_pivot_table[fea1 + '_' + fea2 + '_count'] + alpha + beta)\n",
    "#             del temp_pivot_table[fea1 + '_' + fea2 + '_click_number']\n",
    "            train_df = pd.merge(train_df, temp_pivot_table, on=[fea1, fea2], how='left')\n",
    "            valid_df = pd.merge(valid_df, temp_pivot_table, on=[fea1, fea2], how='left')\n",
    "            test_df = pd.merge(test_df, temp_pivot_table, on=[fea1, fea2], how='left')\n",
    "    return train_df, valid_df, test_df\n",
    "\n",
    "jiaocha_fea_set = ['prefix', 'title', 'tag']\n",
    "train_df, valid_df, test_df = get_jiaocha_dimension_rate_feature(train_df, valid_df, test_df, jiaocha_fea_set)\n",
    "print(train_df.head())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 统计一些是否交叉的特征\n",
    "def get_is_title_in_query_feature(df):\n",
    "    x = df['title']\n",
    "    y = df['query_prediction_keys']\n",
    "    is_title_in_query = np.nan\n",
    "    if len(y) > 0:\n",
    "        if x in y:\n",
    "            is_title_in_query = 1\n",
    "        else:\n",
    "            is_title_in_query = 0\n",
    "    return is_title_in_query\n",
    "\n",
    "def get_is_prefix_in_title_feature(df):\n",
    "    x = df['prefix']\n",
    "    y = df['title']\n",
    "    is_prefix_in_title = np.nan\n",
    "    if x in y:\n",
    "        is_prefix_in_title = 1\n",
    "    else:\n",
    "        is_prefix_in_title = 0\n",
    "    return is_prefix_in_title\n",
    "\n",
    "train_df['is_title_in_query'] = train_df[['title', 'query_prediction_keys']].apply(get_is_title_in_query_feature, axis = 1)\n",
    "valid_df['is_title_in_query'] = valid_df[['title', 'query_prediction_keys']].apply(get_is_title_in_query_feature, axis = 1)\n",
    "test_df['is_title_in_query'] = test_df[['title', 'query_prediction_keys']].apply(get_is_title_in_query_feature, axis = 1)\n",
    "\n",
    "train_df['is_prefix_in_title'] = train_df[['prefix', 'title']].apply(get_is_prefix_in_title_feature, axis = 1)\n",
    "valid_df['is_prefix_in_title'] = valid_df[['prefix', 'title']].apply(get_is_prefix_in_title_feature, axis = 1)\n",
    "test_df['is_prefix_in_title'] = test_df[['prefix', 'title']].apply(get_is_prefix_in_title_feature, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  prefix                                   query_prediction            title  \\\n",
      "0     小品  {\"小品大全宋小宝\": \"0.009\", \"小品相亲\": \"0.012\", \"小品剧本\": ...               小品   \n",
      "1   1368  {\"13688cc赛马会\": \"0.059\", \"13685367892\": \"0.124\"...  HCG大于1368%2C正常吗   \n",
      "2   1368  {\"13688cc赛马会\": \"0.059\", \"13685367892\": \"0.124\"...            1368年   \n",
      "3     银耳  {\"银耳汤的功效\": \"0.012\", \"银耳为什么不能天天吃\": \"0.009\", \"银耳...         银耳红枣汤的做法   \n",
      "4   月经量少  {\"月经量少喝红糖水好吗\": \"0.010\", \"月经量少该怎么调理\": \"0.016\", ...         月经量少怎么调理   \n",
      "\n",
      "  tag  label                              query_prediction_dict  \\\n",
      "0  阅读      0  {'小品大全宋小宝': '0.009', '小品相亲': '0.012', '小品剧本': ...   \n",
      "1  健康      0  {'13688cc赛马会': '0.059', '13685367892': '0.124'...   \n",
      "2  百科      1  {'13688cc赛马会': '0.059', '13685367892': '0.124'...   \n",
      "3  菜谱      1  {'银耳汤的功效': '0.012', '银耳为什么不能天天吃': '0.009', '银耳...   \n",
      "4  百科      0  {'月经量少喝红糖水好吗': '0.010', '月经量少该怎么调理': '0.016', ...   \n",
      "\n",
      "                               query_prediction_keys  \\\n",
      "0  [小品大全宋小宝, 小品相亲, 小品剧本, 小品搞笑大全, 小品不差钱, 小品搞笑大全剧本,...   \n",
      "1  [13688cc赛马会, 13685367892, 13688cc, 1368个单词就够了,...   \n",
      "2  [13688cc赛马会, 13685367892, 13688cc, 1368个单词就够了,...   \n",
      "3  [银耳汤的功效, 银耳为什么不能天天吃, 银耳莲子羹, 银耳的做法, 银耳的功效, 银耳莲子...   \n",
      "4  [月经量少喝红糖水好吗, 月经量少该怎么调理, 月经量少怎么, 月经量少发黑, 月经量少是什...   \n",
      "\n",
      "                             query_prediction_values  query_prediction_number  \\\n",
      "0  [0.009, 0.012, 0.02, 0.066, 0.007, 0.01, 0.198...                       10   \n",
      "1  [0.059, 0.124, 0.029, 0.07, 0.022, 0.042, 0.08...                        9   \n",
      "2  [0.059, 0.124, 0.029, 0.07, 0.022, 0.042, 0.08...                        9   \n",
      "3  [0.012, 0.009, 0.05, 0.045, 0.053, 0.014, 0.05...                       10   \n",
      "4  [0.01, 0.016, 0.009, 0.009, 0.569, 0.016, 0.02...                       10   \n",
      "\n",
      "   query_prediction_max              ...               is_title_in_query  \\\n",
      "0                 0.198              ...                             0.0   \n",
      "1                 0.124              ...                             0.0   \n",
      "2                 0.124              ...                             1.0   \n",
      "3                 0.114              ...                             0.0   \n",
      "4                 0.569              ...                             1.0   \n",
      "\n",
      "   is_prefix_in_title  title_tag_types  prefix_tag_types  tag_title_types  \\\n",
      "0                   1                2                 3            17530   \n",
      "1                   1                1                 2            44527   \n",
      "2                   1                1                 2           124967   \n",
      "3                   1                1                 4            12325   \n",
      "4                   1                2                 3           124967   \n",
      "\n",
      "   tag_prefix_types  title_prefix_types  prefix_title_types  \\\n",
      "0             16742                   1                   2   \n",
      "1             37967                   1                   2   \n",
      "2            104512                   1                   2   \n",
      "3             10842                   3                   4   \n",
      "4            104512                   4                   3   \n",
      "\n",
      "   tag_query_prediction_types  title_query_prediction_types  \n",
      "0                       16570                             1  \n",
      "1                       36137                             1  \n",
      "2                      102472                             1  \n",
      "3                       10716                             3  \n",
      "4                      102472                             4  \n",
      "\n",
      "[5 rows x 45 columns]\n"
     ]
    }
   ],
   "source": [
    "# 统计一些交叉种类特征\n",
    "def get_jiaocha_type_feature(train_df, valid_df, test_df, jiaocha_type_list):\n",
    "    for jiaocha_type in jiaocha_type_list:\n",
    "        fea1 = jiaocha_type[0]\n",
    "        fea2 = jiaocha_type[1]\n",
    "        temp_pivot_table = pd.pivot_table(train_df[[fea1, fea2, 'label']], index=[fea1, fea2], values='label', aggfunc=len)\n",
    "        temp_pivot_table.reset_index(inplace=True)\n",
    "        final_pivot_table = pd.pivot_table(temp_pivot_table, index=fea1, values=fea2, aggfunc=len)\n",
    "        final_pivot_table.reset_index(inplace=True)\n",
    "        final_pivot_table.rename(columns={fea2 : fea1 + '_' + fea2 + '_types'}, inplace=True)\n",
    "        train_df = pd.merge(train_df, final_pivot_table[[fea1, fea1 + '_' + fea2 + '_types']], on=fea1, how='left')\n",
    "        valid_df = pd.merge(valid_df, final_pivot_table[[fea1, fea1 + '_' + fea2 + '_types']], on=fea1, how='left')\n",
    "        test_df = pd.merge(test_df, final_pivot_table[[fea1, fea1 + '_' + fea2 + '_types']], on=fea1, how='left')\n",
    "    return train_df, valid_df, test_df\n",
    "\n",
    "jiaocha_type_list = [['title', 'tag'], ['prefix', 'tag'], ['tag', 'title'], ['tag', 'prefix'], \n",
    "                     ['title', 'prefix'], ['prefix', 'title'], ['tag', 'query_prediction'], ['title', 'query_prediction']]\n",
    "train_df, valid_df, test_df = get_jiaocha_type_feature(train_df, valid_df, test_df, jiaocha_type_list)\n",
    "print(train_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  prefix                                   query_prediction            title  \\\n",
      "0     小品  {\"小品大全宋小宝\": \"0.009\", \"小品相亲\": \"0.012\", \"小品剧本\": ...               小品   \n",
      "1   1368  {\"13688cc赛马会\": \"0.059\", \"13685367892\": \"0.124\"...  HCG大于1368%2C正常吗   \n",
      "2   1368  {\"13688cc赛马会\": \"0.059\", \"13685367892\": \"0.124\"...            1368年   \n",
      "3     银耳  {\"银耳汤的功效\": \"0.012\", \"银耳为什么不能天天吃\": \"0.009\", \"银耳...         银耳红枣汤的做法   \n",
      "4   月经量少  {\"月经量少喝红糖水好吗\": \"0.010\", \"月经量少该怎么调理\": \"0.016\", ...         月经量少怎么调理   \n",
      "\n",
      "  tag  label                              query_prediction_dict  \\\n",
      "0  阅读      0  {'小品大全宋小宝': '0.009', '小品相亲': '0.012', '小品剧本': ...   \n",
      "1  健康      0  {'13688cc赛马会': '0.059', '13685367892': '0.124'...   \n",
      "2  百科      1  {'13688cc赛马会': '0.059', '13685367892': '0.124'...   \n",
      "3  菜谱      1  {'银耳汤的功效': '0.012', '银耳为什么不能天天吃': '0.009', '银耳...   \n",
      "4  百科      0  {'月经量少喝红糖水好吗': '0.010', '月经量少该怎么调理': '0.016', ...   \n",
      "\n",
      "                               query_prediction_keys  \\\n",
      "0  [小品大全宋小宝, 小品相亲, 小品剧本, 小品搞笑大全, 小品不差钱, 小品搞笑大全剧本,...   \n",
      "1  [13688cc赛马会, 13685367892, 13688cc, 1368个单词就够了,...   \n",
      "2  [13688cc赛马会, 13685367892, 13688cc, 1368个单词就够了,...   \n",
      "3  [银耳汤的功效, 银耳为什么不能天天吃, 银耳莲子羹, 银耳的做法, 银耳的功效, 银耳莲子...   \n",
      "4  [月经量少喝红糖水好吗, 月经量少该怎么调理, 月经量少怎么, 月经量少发黑, 月经量少是什...   \n",
      "\n",
      "                             query_prediction_values  query_prediction_number  \\\n",
      "0  [0.009, 0.012, 0.02, 0.066, 0.007, 0.01, 0.198...                       10   \n",
      "1  [0.059, 0.124, 0.029, 0.07, 0.022, 0.042, 0.08...                        9   \n",
      "2  [0.059, 0.124, 0.029, 0.07, 0.022, 0.042, 0.08...                        9   \n",
      "3  [0.012, 0.009, 0.05, 0.045, 0.053, 0.014, 0.05...                       10   \n",
      "4  [0.01, 0.016, 0.009, 0.009, 0.569, 0.016, 0.02...                       10   \n",
      "\n",
      "   query_prediction_max       ...        prefix_len  title_len  \\\n",
      "0                 0.198       ...                 2          2   \n",
      "1                 0.124       ...                 4         15   \n",
      "2                 0.124       ...                 4          5   \n",
      "3                 0.114       ...                 2          8   \n",
      "4                 0.569       ...                 4          8   \n",
      "\n",
      "   query_prediction_key_len_max  query_prediction_key_len_min  \\\n",
      "0                          10.0                           4.0   \n",
      "1                          15.0                           5.0   \n",
      "2                          15.0                           5.0   \n",
      "3                          10.0                           3.0   \n",
      "4                          10.0                           6.0   \n",
      "\n",
      "   query_prediction_key_len_mean  query_prediction_key_len_std  \\\n",
      "0                       5.600000                      2.009975   \n",
      "1                       9.222222                      3.010270   \n",
      "2                       9.222222                      3.010270   \n",
      "3                       5.700000                      1.734935   \n",
      "4                       7.900000                      1.300000   \n",
      "\n",
      "   len_title-prefix  len_prefix/title  len_mean-title  len_mean/title  \n",
      "0                 0          1.000000        3.600000        2.800000  \n",
      "1                11          0.266667       -5.777778        0.614815  \n",
      "2                 1          0.800000        4.222222        1.844444  \n",
      "3                 6          0.250000       -2.300000        0.712500  \n",
      "4                 4          0.500000       -0.100000        0.987500  \n",
      "\n",
      "[5 rows x 55 columns]\n"
     ]
    }
   ],
   "source": [
    "def get_key_len_list(x):\n",
    "    return_list = []\n",
    "    for temp in x:\n",
    "        return_list.append(len(temp))\n",
    "    return return_list\n",
    "\n",
    "# 统计一些跟字符串长度相关的特征\n",
    "def get_string_len_feature(df):\n",
    "    df['prefix_len'] = df['prefix'].map(lambda x : len(x))\n",
    "    df['title_len'] = df['title'].map(lambda x : len(x))\n",
    "    df['query_prediction_key_len_list'] = df['query_prediction_keys'].map(lambda x : get_key_len_list(x))\n",
    "    df['query_prediction_key_len_max'] = df['query_prediction_key_len_list'].map(lambda x : np.nan if len(x) == 0 else np.max(x))\n",
    "    df['query_prediction_key_len_min'] = df['query_prediction_key_len_list'].map(lambda x : np.nan if len(x) == 0 else np.min(x))\n",
    "    df['query_prediction_key_len_mean'] = df['query_prediction_key_len_list'].map(lambda x : np.nan if len(x) == 0 else np.mean(x))\n",
    "    df['query_prediction_key_len_std'] = df['query_prediction_key_len_list'].map(lambda x : np.nan if len(x) == 0 else np.std(x))\n",
    "    df['len_title-prefix'] = df['title_len'] - df['prefix_len']\n",
    "    df['len_prefix/title'] = df['prefix_len'] / df['title_len']\n",
    "    df['len_mean-title'] = df['query_prediction_key_len_mean'] - df['title_len']\n",
    "    df['len_mean/title'] = df['query_prediction_key_len_mean'] / df['title_len']\n",
    "    del df['query_prediction_key_len_list']\n",
    "    return df\n",
    "\n",
    "train_df = get_string_len_feature(train_df)\n",
    "valid_df = get_string_len_feature(valid_df)\n",
    "test_df = get_string_len_feature(test_df)\n",
    "print(train_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Dump cache file failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/jieba/__init__.py\", line 152, in initialize\n",
      "    _replace_file(fpath, cache_file)\n",
      "PermissionError: [Errno 1] Operation not permitted: '/tmp/tmp1srf1tvs' -> '/tmp/jieba.cache'\n",
      "Loading model cost 2.023 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  prefix                                   query_prediction            title  \\\n",
      "0     小品  {\"小品大全宋小宝\": \"0.009\", \"小品相亲\": \"0.012\", \"小品剧本\": ...               小品   \n",
      "1   1368  {\"13688cc赛马会\": \"0.059\", \"13685367892\": \"0.124\"...  HCG大于1368%2C正常吗   \n",
      "2   1368  {\"13688cc赛马会\": \"0.059\", \"13685367892\": \"0.124\"...            1368年   \n",
      "3     银耳  {\"银耳汤的功效\": \"0.012\", \"银耳为什么不能天天吃\": \"0.009\", \"银耳...         银耳红枣汤的做法   \n",
      "4   月经量少  {\"月经量少喝红糖水好吗\": \"0.010\", \"月经量少该怎么调理\": \"0.016\", ...         月经量少怎么调理   \n",
      "\n",
      "  tag  label                              query_prediction_dict  \\\n",
      "0  阅读      0  {'小品大全宋小宝': '0.009', '小品相亲': '0.012', '小品剧本': ...   \n",
      "1  健康      0  {'13688cc赛马会': '0.059', '13685367892': '0.124'...   \n",
      "2  百科      1  {'13688cc赛马会': '0.059', '13685367892': '0.124'...   \n",
      "3  菜谱      1  {'银耳汤的功效': '0.012', '银耳为什么不能天天吃': '0.009', '银耳...   \n",
      "4  百科      0  {'月经量少喝红糖水好吗': '0.010', '月经量少该怎么调理': '0.016', ...   \n",
      "\n",
      "                               query_prediction_keys  \\\n",
      "0  [小品大全宋小宝, 小品相亲, 小品剧本, 小品搞笑大全, 小品不差钱, 小品搞笑大全剧本,...   \n",
      "1  [13688cc赛马会, 13685367892, 13688cc, 1368个单词就够了,...   \n",
      "2  [13688cc赛马会, 13685367892, 13688cc, 1368个单词就够了,...   \n",
      "3  [银耳汤的功效, 银耳为什么不能天天吃, 银耳莲子羹, 银耳的做法, 银耳的功效, 银耳莲子...   \n",
      "4  [月经量少喝红糖水好吗, 月经量少该怎么调理, 月经量少怎么, 月经量少发黑, 月经量少是什...   \n",
      "\n",
      "                             query_prediction_values  query_prediction_number  \\\n",
      "0  [0.009, 0.012, 0.02, 0.066, 0.007, 0.01, 0.198...                       10   \n",
      "1  [0.059, 0.124, 0.029, 0.07, 0.022, 0.042, 0.08...                        9   \n",
      "2  [0.059, 0.124, 0.029, 0.07, 0.022, 0.042, 0.08...                        9   \n",
      "3  [0.012, 0.009, 0.05, 0.045, 0.053, 0.014, 0.05...                       10   \n",
      "4  [0.01, 0.016, 0.009, 0.009, 0.569, 0.016, 0.02...                       10   \n",
      "\n",
      "   query_prediction_max         ...          query_prediction_key_len_min  \\\n",
      "0                 0.198         ...                                   4.0   \n",
      "1                 0.124         ...                                   5.0   \n",
      "2                 0.124         ...                                   5.0   \n",
      "3                 0.114         ...                                   3.0   \n",
      "4                 0.569         ...                                   6.0   \n",
      "\n",
      "   query_prediction_key_len_mean  query_prediction_key_len_std  \\\n",
      "0                       5.600000                      2.009975   \n",
      "1                       9.222222                      3.010270   \n",
      "2                       9.222222                      3.010270   \n",
      "3                       5.700000                      1.734935   \n",
      "4                       7.900000                      1.300000   \n",
      "\n",
      "   len_title-prefix  len_prefix/title  len_mean-title  len_mean/title  \\\n",
      "0                 0          1.000000        3.600000        2.800000   \n",
      "1                11          0.266667       -5.777778        0.614815   \n",
      "2                 1          0.800000        4.222222        1.844444   \n",
      "3                 6          0.250000       -2.300000        0.712500   \n",
      "4                 4          0.500000       -0.100000        0.987500   \n",
      "\n",
      "                              query_prediction_words  \\\n",
      "0  [[小品, 大全, 宋, 小宝], [小品, 相亲], [小品, 剧本], [小品, 搞笑,...   \n",
      "1  [[13688cc, 赛马会], [13685367892], [13688cc], [13...   \n",
      "2  [[13688cc, 赛马会], [13685367892], [13688cc], [13...   \n",
      "3  [[银耳汤, 的, 功效], [银耳, 为什么, 不能, 天天, 吃], [银耳, 莲子, ...   \n",
      "4  [[月经, 量少, 喝, 红糖, 水好, 吗], [月经, 量少, 该, 怎么, 调理], ...   \n",
      "\n",
      "             title_jieba_words  prefix_jieba_words  \n",
      "0                         [小品]                [小品]  \n",
      "1  [HCG, 大于, 1368%, 2C, 正常, 吗]              [1368]  \n",
      "2                    [1368, 年]              [1368]  \n",
      "3             [银耳, 红枣汤, 的, 做法]                [银耳]  \n",
      "4             [月经, 量少, 怎么, 调理]            [月经, 量少]  \n",
      "\n",
      "[5 rows x 58 columns]\n"
     ]
    }
   ],
   "source": [
    "#分词方法，调用结巴接口\n",
    "def jieba_seg_to_list(sentence, pos=False):\n",
    "    if not pos:\n",
    "        #不进行词性标注的分词方法\n",
    "        seg_list = jieba.cut(sentence)\n",
    "    else:\n",
    "        #进行词性标注的分词方法\n",
    "        seg_list = psg.cut(sentence)\n",
    "    return seg_list\n",
    "\n",
    "#去除干扰词\n",
    "def jieba_word_filter(seg_list, pos=False):\n",
    "    \n",
    "    filter_list = []\n",
    "    #根据pos参数选择是否词性过滤\n",
    "    #不进行词性过滤，则将词性都标记为n，表示全部保留\n",
    "    for seg in seg_list:\n",
    "        if not pos:\n",
    "            word = seg\n",
    "            flag = 'n'\n",
    "        else:\n",
    "            word = seg.word\n",
    "            flag = seg.flag\n",
    "        if not flag.startswith('n'):\n",
    "            continue\n",
    "        filter_list.append(word)\n",
    "    return filter_list\n",
    "\n",
    "def jieba_word_deal(sentence, pos=False):\n",
    "    #调用上面方式对数据集进行处理，处理后的每条数据仅保留非干扰词\n",
    "    seg_list = jieba_seg_to_list(sentence, pos)\n",
    "    filter_list = jieba_word_filter(seg_list, pos)\n",
    "    return filter_list\n",
    "\n",
    "def get_prefix_prediction_key_sentences(x):\n",
    "    prefix_prediction_key_sentences = \"\"\n",
    "    for temp in x:\n",
    "        if len(prefix_prediction_key_sentences) > 0:\n",
    "            prefix_prediction_key_sentences = prefix_prediction_key_sentences + temp\n",
    "        else:\n",
    "            prefix_prediction_key_sentences = temp\n",
    "    return prefix_prediction_key_sentences\n",
    "\n",
    "def get_max_query_key_sentences(x):\n",
    "    if len(x) == 0:\n",
    "        return \"\"\n",
    "    else:\n",
    "        return max(x, key=x.get)\n",
    "\n",
    "def get_jieba_word(df):\n",
    "#     df['query_prediction_key_sentences'] = df['query_prediction_keys'].map(lambda x : get_prefix_prediction_key_sentences(x))\n",
    "#     df['query_prediction_key_sentences'] = df['query_prediction_dict'].map(lambda x : get_max_query_key_sentences(x))\n",
    "#     df['query_prediction_key_jieba_words'] = df['query_prediction_key_sentences'].map(lambda x : jieba_word_deal(x, False))\n",
    "    df['query_prediction_words'] = df['query_prediction_keys'].map(lambda x : [jieba_word_deal(j, False) for j in x] if len(x) > 0 else np.nan)\n",
    "    df['title_jieba_words'] = df['title'].map(lambda x : jieba_word_deal(x, False))\n",
    "    df['prefix_jieba_words'] = df['prefix'].map(lambda x : jieba_word_deal(x, False))\n",
    "#     del df['query_prediction_key_sentences']\n",
    "    return df\n",
    "\n",
    "train_df = get_jieba_word(train_df)\n",
    "valid_df = get_jieba_word(valid_df)\n",
    "test_df = get_jieba_word(test_df)\n",
    "print(train_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['query_prediction_key_jieba_words'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-25d4c1a9beba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0mcol_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'query_prediction_key_jieba_words'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'title_jieba_words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'prefix_jieba_words'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'title_jieba_words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'prefix_jieba_words'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'query_prediction_key_jieba_words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_word_statistic_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-25d4c1a9beba>\u001b[0m in \u001b[0;36mget_word_statistic_feature\u001b[0;34m(train_df, valid_df, test_df, col_list)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_word_statistic_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'query_prediction_key_jieba_words'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'title_jieba_words'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'prefix_jieba_words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'query_prediction_key_jieba_words'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'title_jieba_words'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'prefix_jieba_words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'query_prediction_key_jieba_words'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'title_jieba_words'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'prefix_jieba_words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mtrain_qs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'query_prediction_key_jieba_words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title_jieba_words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'prefix_jieba_words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_qs\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2054\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2055\u001b[0m             \u001b[0;31m# either boolean or fancy integer index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2056\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2057\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2098\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2099\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2100\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2101\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[1;32m   1229\u001b[0m                 \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1231\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s not in index'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mobjarr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['query_prediction_key_jieba_words'] not in index\""
     ]
    }
   ],
   "source": [
    "def word_match_share(df):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in df[0]:\n",
    "        q1words[word] = 1\n",
    "    for word in df[1]:\n",
    "        q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
    "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
    "    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n",
    "    return R\n",
    "\n",
    "def jaccard(df):\n",
    "    wic = set(df[0]).intersection(set(df[1]))\n",
    "    uw = set(df[0]).union(df[1])\n",
    "    if len(uw) == 0:\n",
    "        uw = [1]\n",
    "    return (len(wic) / len(uw))\n",
    "\n",
    "def common_words(df):\n",
    "    return len(set(df[0]).intersection(set(df[1])))\n",
    "\n",
    "def total_unique_words(df):\n",
    "    return len(set(df[0]).union(df[1]))\n",
    "\n",
    "def wc_diff(df):\n",
    "    return abs(len(df[0]) - len(df[1]))\n",
    "\n",
    "def wc_ratio(df):\n",
    "    l1 = len(df[0])*1.0 \n",
    "    l2 = len(df[1])\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "\n",
    "def wc_diff_unique(df):\n",
    "    return abs(len(set(df[0])) - len(set(df[1])))\n",
    "    \n",
    "def wc_ratio_unique(df):\n",
    "    l1 = len(set(df[0])) * 1.0\n",
    "    l2 = len(set(df[1]))\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "    \n",
    "def tfidf_word_match_share(df, weights=None):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in df[0]:\n",
    "        q1words[word] = 1\n",
    "    for word in df[1]:\n",
    "        q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
    "    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "    R = np.sum(shared_weights) / np.sum(total_weights)\n",
    "    return R\n",
    "\n",
    "def deal_word_for_all(train_df, valid_df, test_df, fea1, fea2, func, colName):\n",
    "    train_df[colName] = train_df[[fea1, fea2]].apply(func, axis=1)\n",
    "    valid_df[colName] = valid_df[[fea1, fea2]].apply(func, axis=1)\n",
    "    test_df[colName] = test_df[[fea1, fea2]].apply(func, axis=1)\n",
    "    print(colName + ' finish!!!')\n",
    "    return train_df, valid_df, test_df\n",
    "                   \n",
    "def get_weight(count, eps=10000, min_count=2):\n",
    "    if count < min_count:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 / (count + eps)\n",
    "\n",
    "def get_word_statistic_feature(train_df, valid_df, test_df, col_list):\n",
    "    df = pd.concat([train_df[['query_prediction_key_jieba_words', 'title_jieba_words', 'prefix_jieba_words']], valid_df[['query_prediction_key_jieba_words', 'title_jieba_words', 'prefix_jieba_words']], test_df[['query_prediction_key_jieba_words', 'title_jieba_words', 'prefix_jieba_words']]])\n",
    "    train_qs = pd.Series(df['query_prediction_key_jieba_words'].tolist() + df['title_jieba_words'].tolist() + df['prefix_jieba_words'].tolist())\n",
    "    words = [x for y in train_qs for x in y]\n",
    "    counts = Counter(words)\n",
    "    weights = {word: get_weight(count) for word, count in counts.items()}\n",
    "    for col in col_list:\n",
    "        fea1 = col[0]\n",
    "        fea2 = col[1]\n",
    "        train_df, valid_df, test_df = deal_word_for_all(train_df, valid_df, test_df, fea1, fea2, word_match_share, fea1[0] + '_' + fea2[0] + '_word_match')\n",
    "        train_df, valid_df, test_df = deal_word_for_all(train_df, valid_df, test_df, fea1, fea2, jaccard, fea1[0] + '_' + fea2[0] + '_jaccard')\n",
    "        train_df, valid_df, test_df = deal_word_for_all(train_df, valid_df, test_df, fea1, fea2, common_words, fea1[0] + '_' + fea2[0] + '_common_words')\n",
    "        train_df, valid_df, test_df = deal_word_for_all(train_df, valid_df, test_df, fea1, fea2, total_unique_words, fea1[0] + '_' + fea2[0] + '_total_unique_words')\n",
    "        train_df, valid_df, test_df = deal_word_for_all(train_df, valid_df, test_df, fea1, fea2, wc_diff, fea1[0] + '_' + fea2[0] + '_wc_diff')\n",
    "        train_df, valid_df, test_df = deal_word_for_all(train_df, valid_df, test_df, fea1, fea2, wc_ratio, fea1[0] + '_' + fea2[0] + '_wc_ratio')\n",
    "        train_df, valid_df, test_df = deal_word_for_all(train_df, valid_df, test_df, fea1, fea2, wc_diff_unique, fea1[0] + '_' + fea2[0] + '_wc_diff_unique')\n",
    "        train_df, valid_df, test_df = deal_word_for_all(train_df, valid_df, test_df, fea1, fea2, wc_ratio_unique, fea1[0] + '_' + fea2[0] + '_wc_ratio_unique')\n",
    "        f = functools.partial(tfidf_word_match_share, weights=weights)\n",
    "        train_df, valid_df, test_df = deal_word_for_all(train_df, valid_df, test_df, fea1, fea2, f, fea1[0] + '_' + fea2[0] + '_tfidf_word_match_share')\n",
    "    return train_df, valid_df, test_df\n",
    "\n",
    "col_list = [['query_prediction_key_jieba_words', 'title_jieba_words'], ['prefix_jieba_words', 'title_jieba_words'], ['prefix_jieba_words', 'query_prediction_key_jieba_words']]\n",
    "train_df, valid_df, test_df = get_word_statistic_feature(train_df, valid_df, test_df, col_list)\n",
    "print(train_df.head())\n",
    "                   \n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set values for various parameters\n",
    "num_features = 500  # Word vector dimensionality                      \n",
    "min_word_count = 1  # Minimum word count                        \n",
    "num_workers = 20       # Number of threads to run in parallel\n",
    "context = 5          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "word2vec_df = pd.concat([train_df[['query_prediction_words', 'title_jieba_words', 'prefix_jieba_words', 'query_prediction_number']], valid_df[['query_prediction_words', 'title_jieba_words', 'prefix_jieba_words', 'query_prediction_number']], test_df[['query_prediction_words', 'title_jieba_words', 'prefix_jieba_words', 'query_prediction_number']]])\n",
    "word2vec_df.reset_index(inplace=True)\n",
    "word2vec_list = word2vec_df['title_jieba_words'].tolist() + word2vec_df['prefix_jieba_words'].tolist() + [y for x in word2vec_df['query_prediction_words'][word2vec_df.query_prediction_number > 0] for y in x]\n",
    "model = word2vec.Word2Vec(word2vec_list, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "word_wv = model.wv\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_w2v_array(word_list, word_wv, num_features):\n",
    "    word_vectors = np.zeros((len(word_list), num_features))\n",
    "    for i in range(len(word_list)):\n",
    "        word_vectors[i][:] = word_wv[str(word_list[i])]\n",
    "    mean_array = np.mean(word_vectors, axis=0)\n",
    "    return mean_array\n",
    "\n",
    "train_df['title_jieba_array'] = train_df['title_jieba_words'].map(lambda x : get_w2v_array(x, word_wv, num_features))\n",
    "valid_df['title_jieba_array'] = valid_df['title_jieba_words'].map(lambda x : get_w2v_array(x, word_wv, num_features))\n",
    "test_df['title_jieba_array'] = test_df['title_jieba_words'].map(lambda x : get_w2v_array(x, word_wv, num_features))\n",
    "\n",
    "train_df['prefix_jieba_array'] = train_df['prefix_jieba_words'].map(lambda x : get_w2v_array(x, word_wv, num_features))\n",
    "valid_df['prefix_jieba_array'] = valid_df['prefix_jieba_words'].map(lambda x : get_w2v_array(x, word_wv, num_features))\n",
    "test_df['prefix_jieba_array'] = test_df['prefix_jieba_words'].map(lambda x : get_w2v_array(x, word_wv, num_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot : finish!!!\n",
      "norm : finish!!!\n",
      "cosine : finish!!!\n"
     ]
    }
   ],
   "source": [
    "# def get_query_jieba_array_dict(df, word_wv, num_features):\n",
    "#     word_array_dict = {}\n",
    "#     query_prediction_words = df['query_prediction_words']\n",
    "#     query_prediction_keys = df['query_prediction_keys']\n",
    "#     if len(query_prediction_keys) > 0:\n",
    "#         for i in range(len(query_prediction_keys)):\n",
    "#             word_array_dict[query_prediction_keys[i]] = get_w2v_array(query_prediction_words[i], word_wv, num_features)\n",
    "#     return word_array_dict\n",
    "\n",
    "def get_title_prefix_similarity(df, f_similarity):\n",
    "    title_array = df['title_jieba_array']\n",
    "    prefix_array = df['prefix_jieba_array']\n",
    "    similarity = 0\n",
    "    if f_similarity == 'dot':\n",
    "        similarity = np.dot(title_array, prefix_array)\n",
    "    elif f_similarity == 'norm':\n",
    "        similarity = np.linalg.norm(title_array - prefix_array)\n",
    "    else:\n",
    "        similarity = np.dot(title_array,prefix_array) / (np.linalg.norm(title_array) * np.linalg.norm(prefix_array))\n",
    "    return similarity\n",
    "\n",
    "def get_title_query_similarity(df, f_similarity, word_wv, num_features):\n",
    "    title_array = df['title_jieba_array']\n",
    "    query_prediction_words = df['query_prediction_words']\n",
    "    query_prediction_keys = df['query_prediction_keys']\n",
    "    query_prediction_dict = df['query_prediction_dict']\n",
    "    if len(query_prediction_keys) <= 0:\n",
    "        return np.nan\n",
    "    similarity = 0\n",
    "    if f_similarity == 'dot':\n",
    "        i = 0\n",
    "        for key in query_prediction_keys:\n",
    "            key_array = get_w2v_array(query_prediction_words[i], word_wv, num_features)\n",
    "            similarity = similarity + np.dot(title_array, key_array) * float(query_prediction_dict[key])\n",
    "            i = i + 1\n",
    "    elif f_similarity == 'norm':\n",
    "        i = 0\n",
    "        for key in query_prediction_keys:\n",
    "            key_array = get_w2v_array(query_prediction_words[i], word_wv, num_features)\n",
    "            similarity = similarity + np.linalg.norm(title_array - key_array) * float(query_prediction_dict[key])\n",
    "            i = i + 1\n",
    "    else:\n",
    "        i = 0\n",
    "        for key in query_prediction_keys:\n",
    "            key_array = get_w2v_array(query_prediction_words[i], word_wv, num_features)\n",
    "            similarity = similarity + (np.dot(title_array, key_array) / (np.linalg.norm(title_array) * np.linalg.norm(key_array))) * float(query_prediction_dict[key])\n",
    "            i = i + 1\n",
    "    return similarity\n",
    "\n",
    "# f_query_jieba_array_dict = functools.partial(get_query_jieba_array_dict, word_wv=word_wv, num_features=num_features)\n",
    "# train_df['query_jieba_array_dict'] = train_df[['query_prediction_words', 'query_prediction_keys']].apply(f_query_jieba_array_dict, axis=1)\n",
    "# valid_df['query_jieba_array_dict'] = valid_df[['query_prediction_words', 'query_prediction_keys']].apply(f_query_jieba_array_dict, axis=1)\n",
    "# test_df['query_jieba_array_dict'] = test_df[['query_prediction_words', 'query_prediction_keys']].apply(f_query_jieba_array_dict, axis=1)\n",
    "# print(train_df.head())\n",
    "\n",
    "def get_similarity_feature(train_df, valid_df, test_df):\n",
    "    f_list = ['dot', 'norm', 'cosine']\n",
    "    for fun in f_list:\n",
    "        f_prefix_similarity = functools.partial(get_title_prefix_similarity, f_similarity=fun)\n",
    "        train_df['title_prefix_' + fun + '_similarity'] = train_df[['title_jieba_array', 'prefix_jieba_array']].apply(f_prefix_similarity, axis=1)\n",
    "        valid_df['title_prefix_' + fun + '_similarity'] = valid_df[['title_jieba_array', 'prefix_jieba_array']].apply(f_prefix_similarity, axis=1)\n",
    "        test_df['title_prefix_' + fun + '_similarity'] = test_df[['title_jieba_array', 'prefix_jieba_array']].apply(f_prefix_similarity, axis=1)\n",
    "        f_query_similarity = functools.partial(get_title_query_similarity, f_similarity=fun, word_wv=word_wv, num_features=num_features)\n",
    "        train_df['title_query_' + fun + '_similarity'] = train_df[['title_jieba_array', 'query_prediction_words', 'query_prediction_keys', 'query_prediction_dict']].apply(f_query_similarity, axis=1)\n",
    "        valid_df['title_query_' + fun + '_similarity'] = valid_df[['title_jieba_array', 'query_prediction_words', 'query_prediction_keys', 'query_prediction_dict']].apply(f_query_similarity, axis=1)\n",
    "        test_df['title_query_' + fun + '_similarity'] = test_df[['title_jieba_array', 'query_prediction_words', 'query_prediction_keys', 'query_prediction_dict']].apply(f_query_similarity, axis=1)\n",
    "        print(fun + ' : finish!!!')\n",
    "    return train_df, valid_df, test_df\n",
    "\n",
    "train_df, valid_df, test_df = get_similarity_feature(train_df, valid_df, test_df)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['prefix' 'query_prediction' 'title' 'tag' 'label' 'query_prediction_dict'\n",
      " 'query_prediction_keys' 'query_prediction_values'\n",
      " 'query_prediction_number' 'query_prediction_max' 'query_prediction_min'\n",
      " 'query_prediction_mean' 'query_prediction_std' 'is_repeat_prefix'\n",
      " 'prefix_count' 'prefix_rate' 'prefix_click_number' 'title_count'\n",
      " 'title_rate' 'title_click_number' 'tag_count' 'tag_rate'\n",
      " 'tag_click_number' 'query_prediction_count' 'query_prediction_rate'\n",
      " 'query_prediction_click_number' 'prefix_title_count' 'prefix_title_rate'\n",
      " 'prefix_title_click_number' 'prefix_tag_count' 'prefix_tag_rate'\n",
      " 'prefix_tag_click_number' 'title_tag_count' 'title_tag_rate'\n",
      " 'title_tag_click_number' 'is_title_in_query' 'is_prefix_in_title'\n",
      " 'title_tag_types' 'prefix_tag_types' 'tag_title_types' 'tag_prefix_types'\n",
      " 'title_prefix_types' 'prefix_title_types' 'tag_query_prediction_types'\n",
      " 'title_query_prediction_types' 'prefix_len' 'title_len'\n",
      " 'query_prediction_key_len_max' 'query_prediction_key_len_min'\n",
      " 'query_prediction_key_len_mean' 'query_prediction_key_len_std'\n",
      " 'len_title-prefix' 'len_prefix/title' 'len_mean-title' 'len_mean/title'\n",
      " 'query_prediction_words' 'title_jieba_words' 'prefix_jieba_words'\n",
      " 'title_jieba_array' 'prefix_jieba_array' 'title_prefix_dot_similarity'\n",
      " 'title_query_dot_similarity' 'title_prefix_norm_similarity'\n",
      " 'title_query_norm_similarity' 'title_prefix_cosine_similarity'\n",
      " 'title_query_cosine_similarity']\n"
     ]
    }
   ],
   "source": [
    "print(train_df.columns.values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  pearsonr      p_values\n",
      "query_prediction_number         -0.0162134  2.29532e-116\n",
      "query_prediction_max            0.00897021   9.35332e-37\n",
      "query_prediction_min             0.0079775    2.0096e-29\n",
      "query_prediction_mean            0.0117999   2.59087e-62\n",
      "query_prediction_std             0.0078163   2.59257e-28\n",
      "is_repeat_prefix                -0.0173927  1.30175e-133\n",
      "prefix_count                   -0.00603852   1.34422e-17\n",
      "prefix_rate                       0.237067             0\n",
      "title_count                    -0.00321917   5.29863e-06\n",
      "title_rate                        0.636893             0\n",
      "tag_count                         0.040607             0\n",
      "tag_rate                          0.171706             0\n",
      "query_prediction_count         -0.00522041   1.54971e-13\n",
      "query_prediction_rate             0.233632             0\n",
      "prefix_title_count               0.0030172   1.98149e-05\n",
      "prefix_title_rate                 0.681393             0\n",
      "prefix_tag_count               -0.00485272   6.75152e-12\n",
      "prefix_tag_rate                   0.627692             0\n",
      "title_tag_count                 -0.0112642   3.89695e-57\n",
      "title_tag_rate                     0.70046             0\n",
      "prefix_click_number            -0.00413289   5.07144e-09\n",
      "title_click_number               0.0399513             0\n",
      "query_prediction_click_number  -0.00321168    5.5722e-06\n",
      "prefix_tag_click_number           0.121323             0\n",
      "prefix_title_click_number        0.0643776             0\n",
      "title_tag_click_number            0.156369             0\n",
      "is_title_in_query                 0.203549             0\n",
      "is_prefix_in_title               0.0967345             0\n",
      "tag_title_types                  0.0223193  1.00793e-218\n",
      "tag_prefix_types                 0.0199181  1.31916e-174\n",
      "title_prefix_types              -0.0703192             0\n",
      "prefix_title_types              -0.0212836  4.42779e-199\n",
      "prefix_len                      0.00739949   1.25604e-25\n",
      "title_len                       -0.0795811             0\n",
      "query_prediction_key_len_max    0.00218719     0.0020165\n",
      "query_prediction_key_len_min    0.00679422   8.64543e-22\n",
      "query_prediction_key_len_mean   0.00686835   3.11761e-22\n",
      "query_prediction_key_len_std   -0.00409501   7.41682e-09\n",
      "len_title-prefix                -0.0847127             0\n",
      "len_prefix/title                 0.0467998             0\n",
      "len_mean-title                   0.0859241             0\n",
      "len_mean/title                   0.0520751             0\n",
      "title_prefix_dot_similarity      0.0752081             0\n",
      "title_query_dot_similarity        0.117176             0\n",
      "title_prefix_norm_similarity    -0.0683231             0\n",
      "title_query_norm_similarity      -0.158303             0\n",
      "title_prefix_cosine_similarity   0.0844364             0\n",
      "title_query_cosine_similarity     0.105214             0\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import mode, pearsonr\n",
    "\n",
    "# 计算转化皮尔森系数\n",
    "def getFeaPearsonr(df, cols):\n",
    "    resultDf = pd.DataFrame(index=cols, columns=['pearsonr','p_values'])\n",
    "    for c in cols:\n",
    "        tempDf = df.dropna(subset=[c])\n",
    "        result = pearsonr(tempDf[c].values, tempDf['label'].values)\n",
    "        resultDf.loc[c,:] = result\n",
    "    return resultDf\n",
    "\n",
    "cols = ['query_prediction_number', 'query_prediction_max', 'query_prediction_min', 'query_prediction_mean', 'query_prediction_std',\n",
    "       'is_repeat_prefix', 'prefix_count', 'prefix_rate',\n",
    " 'title_count', 'title_rate', 'tag_count', 'tag_rate',\n",
    " 'query_prediction_count', 'query_prediction_rate', 'prefix_title_count',\n",
    " 'prefix_title_rate',  'prefix_tag_count', 'prefix_tag_rate',\n",
    " 'title_tag_count', 'title_tag_rate',\n",
    "    'prefix_click_number', 'title_click_number', 'query_prediction_click_number', 'prefix_tag_click_number', \n",
    "    'prefix_title_click_number', 'title_tag_click_number',\n",
    "    'is_title_in_query', 'is_prefix_in_title', \n",
    "    'tag_title_types', 'tag_prefix_types', 'title_prefix_types', 'prefix_title_types',\n",
    "       'prefix_len', 'title_len',\n",
    " 'query_prediction_key_len_max', 'query_prediction_key_len_min',\n",
    " 'query_prediction_key_len_mean', 'query_prediction_key_len_std',\n",
    " 'len_title-prefix', 'len_prefix/title', 'len_mean-title', 'len_mean/title',\n",
    "#        'q_t_word_match', 'q_t_jaccard', 'q_t_common_words',\n",
    "#  'q_t_total_unique_words', 'q_t_wc_diff', 'q_t_wc_ratio',\n",
    "#  'q_t_wc_diff_unique', 'q_t_wc_ratio_unique', 'q_t_tfidf_word_match_share',\n",
    "#  'p_t_word_match', 'p_t_jaccard', 'p_t_common_words',\n",
    "#  'p_t_total_unique_words', 'p_t_wc_diff', 'p_t_wc_ratio',\n",
    "#  'p_t_wc_diff_unique', 'p_t_wc_ratio_unique', 'p_t_tfidf_word_match_share',\n",
    "#  'p_q_word_match', 'p_q_jaccard', 'p_q_common_words',\n",
    "#  'p_q_total_unique_words', 'p_q_wc_diff', 'p_q_wc_ratio',\n",
    "#  'p_q_wc_diff_unique', 'p_q_wc_ratio_unique', 'p_q_tfidf_word_match_share',\n",
    "        'title_prefix_dot_similarity',\n",
    " 'title_query_dot_similarity', 'title_prefix_norm_similarity',\n",
    " 'title_query_norm_similarity', 'title_prefix_cosine_similarity',\n",
    " 'title_query_cosine_similarity',\n",
    "       ]\n",
    "resultDf = getFeaPearsonr(train_df, cols)\n",
    "print(resultDf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fea = [\n",
    "    'query_prediction_number', 'query_prediction_max', 'query_prediction_min', 'query_prediction_mean', 'query_prediction_std',\n",
    "       'is_repeat_prefix', 'prefix_count', 'prefix_rate',\n",
    " 'title_count', 'title_rate', 'tag_count', 'tag_rate',\n",
    " 'query_prediction_count', 'query_prediction_rate', 'prefix_title_count',\n",
    " 'prefix_title_rate',  'prefix_tag_count', 'prefix_tag_rate',\n",
    " 'title_tag_count', 'title_tag_rate',\n",
    "    'prefix_click_number', 'title_click_number', 'query_prediction_click_number', 'prefix_tag_click_number', \n",
    "    'prefix_title_click_number', 'title_tag_click_number',\n",
    "    'is_title_in_query', 'is_prefix_in_title', \n",
    "    'title_tag_types', 'prefix_tag_types', 'tag_title_types', 'tag_prefix_types',\n",
    " 'title_prefix_types', 'prefix_title_types', 'tag_query_prediction_types', 'title_query_prediction_types',\n",
    "      'prefix_len', 'title_len',\n",
    " 'query_prediction_key_len_max', 'query_prediction_key_len_min',\n",
    " 'query_prediction_key_len_mean', 'query_prediction_key_len_std',\n",
    " 'len_title-prefix', 'len_prefix/title', 'len_mean-title', 'len_mean/title',\n",
    "#     'q_t_word_match', 'q_t_jaccard', 'q_t_common_words',\n",
    "#  'q_t_total_unique_words', 'q_t_wc_diff', 'q_t_wc_ratio',\n",
    "#  'q_t_wc_diff_unique', 'q_t_wc_ratio_unique', 'q_t_tfidf_word_match_share',\n",
    "#  'p_t_word_match', 'p_t_jaccard', 'p_t_common_words',\n",
    "#  'p_t_total_unique_words', 'p_t_wc_diff', 'p_t_wc_ratio',\n",
    "#  'p_t_wc_diff_unique', 'p_t_wc_ratio_unique', 'p_t_tfidf_word_match_share',\n",
    "#  'p_q_word_match', 'p_q_jaccard', 'p_q_common_words',\n",
    "#  'p_q_total_unique_words', 'p_q_wc_diff', 'p_q_wc_ratio',\n",
    "#  'p_q_wc_diff_unique', 'p_q_wc_ratio_unique', 'p_q_tfidf_word_match_share',\n",
    "    'title_prefix_dot_similarity',\n",
    " 'title_query_dot_similarity', 'title_prefix_norm_similarity',\n",
    " 'title_query_norm_similarity', 'title_prefix_cosine_similarity',\n",
    " 'title_query_cosine_similarity',\n",
    "      ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's auc: 0.921196\tvalid_1's auc: 0.789941\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[2]\tvalid_0's auc: 0.92168\tvalid_1's auc: 0.791152\n",
      "[3]\tvalid_0's auc: 0.921731\tvalid_1's auc: 0.791153\n",
      "[4]\tvalid_0's auc: 0.921759\tvalid_1's auc: 0.791219\n",
      "[5]\tvalid_0's auc: 0.921895\tvalid_1's auc: 0.791868\n",
      "[6]\tvalid_0's auc: 0.921915\tvalid_1's auc: 0.791853\n",
      "[7]\tvalid_0's auc: 0.923303\tvalid_1's auc: 0.784389\n",
      "[8]\tvalid_0's auc: 0.923334\tvalid_1's auc: 0.783834\n",
      "[9]\tvalid_0's auc: 0.92346\tvalid_1's auc: 0.783601\n",
      "[10]\tvalid_0's auc: 0.923568\tvalid_1's auc: 0.783928\n",
      "[11]\tvalid_0's auc: 0.92359\tvalid_1's auc: 0.783646\n",
      "[12]\tvalid_0's auc: 0.923612\tvalid_1's auc: 0.783826\n",
      "[13]\tvalid_0's auc: 0.923622\tvalid_1's auc: 0.784327\n",
      "[14]\tvalid_0's auc: 0.923635\tvalid_1's auc: 0.783861\n",
      "[15]\tvalid_0's auc: 0.923662\tvalid_1's auc: 0.783813\n",
      "[16]\tvalid_0's auc: 0.923678\tvalid_1's auc: 0.783823\n",
      "[17]\tvalid_0's auc: 0.923704\tvalid_1's auc: 0.783858\n",
      "[18]\tvalid_0's auc: 0.923738\tvalid_1's auc: 0.783648\n",
      "[19]\tvalid_0's auc: 0.923717\tvalid_1's auc: 0.783637\n",
      "[20]\tvalid_0's auc: 0.923728\tvalid_1's auc: 0.783666\n",
      "[21]\tvalid_0's auc: 0.923754\tvalid_1's auc: 0.78366\n",
      "[22]\tvalid_0's auc: 0.923912\tvalid_1's auc: 0.783621\n",
      "[23]\tvalid_0's auc: 0.924291\tvalid_1's auc: 0.783769\n",
      "[24]\tvalid_0's auc: 0.924274\tvalid_1's auc: 0.783719\n",
      "[25]\tvalid_0's auc: 0.924325\tvalid_1's auc: 0.783785\n",
      "[26]\tvalid_0's auc: 0.924346\tvalid_1's auc: 0.784339\n",
      "[27]\tvalid_0's auc: 0.924386\tvalid_1's auc: 0.785002\n",
      "[28]\tvalid_0's auc: 0.924411\tvalid_1's auc: 0.785164\n",
      "[29]\tvalid_0's auc: 0.924456\tvalid_1's auc: 0.785723\n",
      "[30]\tvalid_0's auc: 0.924477\tvalid_1's auc: 0.785706\n",
      "[31]\tvalid_0's auc: 0.924493\tvalid_1's auc: 0.786158\n",
      "[32]\tvalid_0's auc: 0.9245\tvalid_1's auc: 0.786271\n",
      "[33]\tvalid_0's auc: 0.924549\tvalid_1's auc: 0.787464\n",
      "[34]\tvalid_0's auc: 0.924565\tvalid_1's auc: 0.787606\n",
      "[35]\tvalid_0's auc: 0.924608\tvalid_1's auc: 0.787635\n",
      "[36]\tvalid_0's auc: 0.924614\tvalid_1's auc: 0.787897\n",
      "[37]\tvalid_0's auc: 0.924654\tvalid_1's auc: 0.78781\n",
      "[38]\tvalid_0's auc: 0.924644\tvalid_1's auc: 0.787859\n",
      "[39]\tvalid_0's auc: 0.92467\tvalid_1's auc: 0.78779\n",
      "[40]\tvalid_0's auc: 0.924673\tvalid_1's auc: 0.787981\n",
      "[41]\tvalid_0's auc: 0.924673\tvalid_1's auc: 0.788679\n",
      "[42]\tvalid_0's auc: 0.924693\tvalid_1's auc: 0.788732\n",
      "[43]\tvalid_0's auc: 0.924734\tvalid_1's auc: 0.789775\n",
      "[44]\tvalid_0's auc: 0.924745\tvalid_1's auc: 0.789624\n",
      "[45]\tvalid_0's auc: 0.92475\tvalid_1's auc: 0.790236\n",
      "[46]\tvalid_0's auc: 0.92479\tvalid_1's auc: 0.790367\n",
      "[47]\tvalid_0's auc: 0.924814\tvalid_1's auc: 0.790745\n",
      "[48]\tvalid_0's auc: 0.924839\tvalid_1's auc: 0.790782\n",
      "[49]\tvalid_0's auc: 0.924954\tvalid_1's auc: 0.789055\n",
      "[50]\tvalid_0's auc: 0.924968\tvalid_1's auc: 0.789213\n",
      "[51]\tvalid_0's auc: 0.924982\tvalid_1's auc: 0.789161\n",
      "[52]\tvalid_0's auc: 0.924996\tvalid_1's auc: 0.789164\n",
      "[53]\tvalid_0's auc: 0.925014\tvalid_1's auc: 0.78927\n",
      "[54]\tvalid_0's auc: 0.925033\tvalid_1's auc: 0.789197\n",
      "[55]\tvalid_0's auc: 0.92504\tvalid_1's auc: 0.789218\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's auc: 0.921895\tvalid_1's auc: 0.791868\n",
      "0.48984255266738697\n",
      "                                importance\n",
      "prefix_title_rate                       86\n",
      "prefix_tag_rate                         35\n",
      "title_tag_rate                          25\n",
      "title_tag_count                          5\n",
      "prefix_tag_count                         3\n",
      "prefix_title_count                       1\n",
      "tag_prefix_types                         0\n",
      "title_prefix_types                       0\n",
      "prefix_title_types                       0\n",
      "tag_query_prediction_types               0\n",
      "title_query_prediction_types             0\n",
      "prefix_len                               0\n",
      "title_len                                0\n",
      "query_prediction_key_len_max             0\n",
      "query_prediction_number                  0\n",
      "prefix_tag_types                         0\n",
      "query_prediction_key_len_min             0\n",
      "query_prediction_key_len_mean            0\n",
      "query_prediction_key_len_std             0\n",
      "len_title-prefix                         0\n",
      "len_prefix/title                         0\n",
      "len_mean-title                           0\n",
      "len_mean/title                           0\n",
      "title_prefix_dot_similarity              0\n",
      "title_query_dot_similarity               0\n",
      "title_prefix_norm_similarity             0\n",
      "title_query_norm_similarity              0\n",
      "title_prefix_cosine_similarity           0\n",
      "tag_title_types                          0\n",
      "is_title_in_query                        0\n",
      "title_tag_types                          0\n",
      "is_prefix_in_title                       0\n",
      "query_prediction_min                     0\n",
      "query_prediction_mean                    0\n",
      "query_prediction_std                     0\n",
      "is_repeat_prefix                         0\n",
      "prefix_count                             0\n",
      "prefix_rate                              0\n",
      "title_count                              0\n",
      "title_rate                               0\n",
      "tag_count                                0\n",
      "tag_rate                                 0\n",
      "query_prediction_count                   0\n",
      "query_prediction_rate                    0\n",
      "prefix_click_number                      0\n",
      "title_click_number                       0\n",
      "query_prediction_click_number            0\n",
      "prefix_tag_click_number                  0\n",
      "prefix_title_click_number                0\n",
      "title_tag_click_number                   0\n",
      "query_prediction_max                     0\n",
      "title_query_cosine_similarity            0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: FutureWarning: by argument to sort_index is deprecated, pls use .sort_values(by=...)\n"
     ]
    }
   ],
   "source": [
    "lgb_model = lgb.LGBMClassifier(\n",
    "    boosting_type='gbdt', num_leaves=32, max_depth=-1, n_estimators=5000, objective='binary',\n",
    "    subsample=0.8, colsample_bytree=1, subsample_freq=1,\n",
    "    learning_rate=0.01, random_state=2018, n_jobs=-1\n",
    ")\n",
    "\n",
    "valid_df['predicted_score'] = 0\n",
    "\n",
    "lgb_model.fit(train_df[fea], train_df['label'], eval_set=[(train_df[fea], train_df['label']),\n",
    "                            (valid_df[fea], valid_df['label'])], early_stopping_rounds=50, eval_metric='auc')\n",
    "valid_pred = lgb_model.predict_proba(valid_df[fea], num_iteration=lgb_model.best_iteration_)[:, 1]\n",
    "print(np.mean(valid_pred))\n",
    "\n",
    "fscore = lgb_model.booster_.feature_importance()\n",
    "feaNames = lgb_model.booster_.feature_name()\n",
    "scoreDf = pd.DataFrame(index=feaNames, columns=['importance'], data=fscore)\n",
    "print(scoreDf.sort_index(by=['importance'], ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1795042485057676\n",
      "0.4037811010487482\n",
      "0.3771786726195587\n",
      "0.37003624227570203\n"
     ]
    }
   ],
   "source": [
    "valid_df['predicted_score'] = valid_pred\n",
    "print(np.mean(valid_df['predicted_score'][valid_df.is_repeat_prefix == 0]))\n",
    "print(np.mean(valid_df['predicted_score'][valid_df.is_repeat_prefix == 1]))\n",
    "print(np.mean(valid_df['label'][valid_df.is_repeat_prefix == 0]))\n",
    "print(np.mean(valid_df['label'][valid_df.is_repeat_prefix == 1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original mean :  0.1795042485057676\n",
      "0.4034150988840599\n"
     ]
    }
   ],
   "source": [
    "valid_prefix0_df = valid_df[valid_df.is_repeat_prefix == 0].copy()\n",
    "\n",
    "#定义调整函数\n",
    "def resultAdjustment(result_df, t):\n",
    "    result_df_temp = result_df.copy()\n",
    "    result_df_temp['x'] = result_df_temp.predicted_score.map(lambda x: -(math.log(((1 - x) / x), math.e)))\n",
    "    result_df_temp['adjust_result'] = result_df_temp.x.map(lambda x: 1 / (1 + math.exp(-(x + t)))) \n",
    "    print(result_df_temp['adjust_result'].mean())\n",
    "    return result_df_temp['adjust_result']\n",
    "\n",
    "print('original mean : ', valid_prefix0_df['predicted_score'].mean())\n",
    "valid_df_after = resultAdjustment(valid_prefix0_df, 1.22085)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4034150988840599\n",
      "0.4037811010487482\n",
      "0.3771786726195587\n",
      "0.37003624227570203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "valid_df['predicted_score'][valid_df.is_repeat_prefix == 0] = valid_df_after\n",
    "print(np.mean(valid_df['predicted_score'][valid_df.is_repeat_prefix == 0]))\n",
    "print(np.mean(valid_df['predicted_score'][valid_df.is_repeat_prefix == 1]))\n",
    "print(np.mean(valid_df['label'][valid_df.is_repeat_prefix == 0]))\n",
    "print(np.mean(valid_df['label'][valid_df.is_repeat_prefix == 1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0.35': 0.6884588053337792, '0.352': 0.6883901867692885, '0.354': 0.6886324123918645, '0.356': 0.6890555100497764, '0.358': 0.7003655252020008, '0.36': 0.7006958322297931, '0.362': 0.7008946873417417, '0.364': 0.7009248750332037, '0.366': 0.7012892769889456, '0.368': 0.708543946606468, '0.37': 0.7087867547716722, '0.372': 0.709702853200611, '0.374': 0.7130736017672457, '0.376': 0.7146073099785001, '0.378': 0.7210792928770805, '0.38': 0.722459169874129, '0.382': 0.7220522475876677, '0.384': 0.7224778481844715, '0.386': 0.7228833192923337, '0.388': 0.7223571654873799, '0.39': 0.7218458933107536, '0.392': 0.7211862383617602, '0.394': 0.7210414452709885, '0.396': 0.7080971995687393, '0.398': 0.7077681191517634, '0.4': 0.7075327843854834, '0.402': 0.7073644486375492, '0.404': 0.7069751919012126, '0.406': 0.706478747702078, '0.408': 0.706401272996287, '0.41': 0.7063729743345742, '0.412': 0.7060144256405939, '0.414': 0.7056010352491068, '0.416': 0.7056405057587789, '0.418': 0.7014527295426172, '0.42': 0.7013725100167656, '0.422': 0.7015677013685379, '0.424': 0.7011592713151733, '0.426': 0.7010721944245889, '0.428': 0.7009021910353717, '0.43': 0.7007136101796922, '0.432': 0.7006943248981465, '0.434': 0.7001378676470588, '0.436': 0.6990201729106628, '0.438': 0.6958235515752254, '0.44': 0.6952660145705745, '0.442': 0.6945318876063892, '0.444': 0.6943074723543314, '0.446': 0.6944744446714731, '0.448': 0.691832371804258}\n"
     ]
    }
   ],
   "source": [
    "yuzhi_dict = {}\n",
    "# 定义搜索方法获取最佳F1对应的阈值\n",
    "for yuzhi in range(350, 450, 2):\n",
    "    real_yuzhi = yuzhi / 1000\n",
    "    valid_df['predicted_label'] = valid_df['predicted_score'].map(lambda x : 1 if x > real_yuzhi else 0)\n",
    "    f1 = f1_score(valid_df['label'], valid_df['predicted_label'])\n",
    "    yuzhi_dict[str(real_yuzhi)] = f1\n",
    "print(yuzhi_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original mean :  0.18111353307202657\n",
      "0.4024094698755495\n"
     ]
    }
   ],
   "source": [
    "test_pred = lgb_model.predict_proba(test_df[fea], num_iteration=lgb_model.best_iteration_)[:, 1]\n",
    "test_df['predicted_score'] = test_pred\n",
    "\n",
    "test_prefix0_df = test_df[test_df.is_repeat_prefix == 0].copy()\n",
    "\n",
    "#定义调整函数\n",
    "def resultAdjustment(result_df, t):\n",
    "    result_df_temp = result_df.copy()\n",
    "    result_df_temp['x'] = result_df_temp.predicted_score.map(lambda x: -(math.log(((1 - x) / x), math.e)))\n",
    "    result_df_temp['adjust_result'] = result_df_temp.x.map(lambda x: 1 / (1 + math.exp(-(x + t)))) \n",
    "    print(result_df_temp['adjust_result'].mean())\n",
    "    return result_df_temp['adjust_result']\n",
    "\n",
    "print('original mean : ', test_prefix0_df['predicted_score'].mean())\n",
    "test_df_after = resultAdjustment(test_prefix0_df, 1.21085)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4024094698755495\n",
      "0.40235523040784643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "test_df['predicted_score'][test_df.is_repeat_prefix == 0] = test_df_after\n",
    "print(np.mean(test_df['predicted_score'][test_df.is_repeat_prefix == 0]))\n",
    "print(np.mean(test_df['predicted_score'][test_df.is_repeat_prefix == 1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3717\n",
      "0.3898\n"
     ]
    }
   ],
   "source": [
    "test_df['predicted_label'] = test_df['predicted_score'].map(lambda x : 1 if x > 0.382 else 0)\n",
    "\n",
    "print(np.mean(valid_df['label']))\n",
    "print(np.mean(test_df['predicted_label']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 导出预测结果\n",
    "def exportResult(df, fileName):\n",
    "    df.to_csv('../result/%s.csv' % fileName, header=False, index=False)\n",
    "\n",
    "exportResult(test_df[['predicted_label']], 'lgb_keng_10_28')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
