{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import gc\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import chi2, SelectPercentile\n",
    "import math\n",
    "from sklearn.metrics import f1_score\n",
    "import jieba\n",
    "import jieba.posseg as psg\n",
    "from collections import Counter\n",
    "import functools\n",
    "from gensim.models import word2vec\n",
    "import Levenshtein\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_table('../data/oppo_round1_train_20180929.txt', names=['prefix', 'query_prediction', 'title', 'tag', 'label'], header=None, quoting=3)\n",
    "train_df['index'] = train_df.index\n",
    "valid_df = pd.read_table('../data/oppo_round1_vali_20180929.txt', names=['prefix', 'query_prediction', 'title', 'tag', 'label'], header=None, quoting=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_float_list(x):\n",
    "    return_list = []\n",
    "    for temp in x:\n",
    "        return_list.append(float(temp))\n",
    "    return return_list\n",
    "\n",
    "# 处理跟query_prediction相关的统计特征\n",
    "def get_query_prediction_feature(df):\n",
    "    df['query_prediction_dict'] = df['query_prediction'].map(lambda x : eval(x))\n",
    "    df['query_prediction_keys'] = df['query_prediction_dict'].map(lambda x : list(x.keys()))\n",
    "    df['query_prediction_values'] = df['query_prediction_dict'].map(lambda x : get_float_list(list(x.values())))\n",
    "    df['query_prediction_number'] = df['query_prediction_keys'].map(lambda x : len(x))\n",
    "    df['query_prediction_max'] = df['query_prediction_values'].map(lambda x : np.nan if len(x) == 0 else np.max(x))\n",
    "    df['query_prediction_min'] = df['query_prediction_values'].map(lambda x : np.nan if len(x) == 0 else np.min(x))\n",
    "    df['query_prediction_mean'] = df['query_prediction_values'].map(lambda x : np.nan if len(x) == 0 else np.mean(x))\n",
    "    df['query_prediction_std'] = df['query_prediction_values'].map(lambda x : np.nan if len(x) == 0 else np.std(x))\n",
    "    return df\n",
    "\n",
    "train_df = get_query_prediction_feature(train_df)\n",
    "valid_df = get_query_prediction_feature(valid_df)\n",
    "print(test_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getBayesSmoothParam(origion_rate):\n",
    "    origion_rate_mean = origion_rate.mean()\n",
    "    origion_rate_var = origion_rate.var()\n",
    "    alpha = origion_rate_mean / origion_rate_var * (origion_rate_mean * (1 - origion_rate_mean) - origion_rate_var)\n",
    "    beta = (1 - origion_rate_mean) / origion_rate_var * (origion_rate_mean * (1 - origion_rate_mean) - origion_rate_var)\n",
    "#     print('origion_rate_mean : ', origion_rate_mean)\n",
    "#     print('origion_rate_var : ', origion_rate_var)\n",
    "#     print('alpha : ', alpha)\n",
    "#     print('beta : ', beta)\n",
    "    return alpha, beta\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, random_state=2018, shuffle=True)\n",
    "\n",
    "# 统计单维度的转化率特征\n",
    "def get_single_dimension_rate_feature(train_df, valid_df, fea_set):\n",
    "    for fea in fea_set:\n",
    "        train_temp_df = pd.DataFrame()\n",
    "        for index, (train_index, test_index) in enumerate(skf.split(train_df, train_df['label'])):\n",
    "            temp_df = train_df[[fea, 'label']].iloc[train_index].copy()\n",
    "            temp_pivot_table = pd.pivot_table(temp_df, index=fea, values='label', aggfunc={len, np.mean, np.sum})\n",
    "            temp_pivot_table.reset_index(inplace=True)\n",
    "            temp_pivot_table.rename(columns={'len':fea + '_count', 'mean':fea + '_rate', 'sum':fea + '_click_number'}, inplace=True)\n",
    "            alpha, beta = getBayesSmoothParam(temp_pivot_table[fea + '_rate'])\n",
    "            temp_pivot_table[fea + '_rate'] = (temp_pivot_table[fea + '_click_number'] + alpha) / (temp_pivot_table[fea + '_count'] + alpha + beta)\n",
    "#             del temp_pivot_table[fea + '_click_number']\n",
    "            fea_df = train_df.iloc[test_index].copy()\n",
    "            fea_df = pd.merge(fea_df, temp_pivot_table, on=fea, how='left')\n",
    "#             print(fea_df.head())\n",
    "            train_temp_df = pd.concat([train_temp_df, fea_df])\n",
    "        temp_df = train_df[[fea, 'label']].copy()\n",
    "        temp_pivot_table = pd.pivot_table(temp_df, index=fea, values='label', aggfunc={len, np.mean, np.sum})\n",
    "        temp_pivot_table.reset_index(inplace=True)\n",
    "        temp_pivot_table.rename(columns={'len':fea + '_count', 'mean':fea + '_rate', 'sum':fea + '_click_number'}, inplace=True)\n",
    "        alpha, beta = getBayesSmoothParam(temp_pivot_table[fea + '_rate'])\n",
    "        temp_pivot_table[fea + '_rate'] = (temp_pivot_table[fea + '_click_number'] + alpha) / (temp_pivot_table[fea + '_count'] + alpha + beta)\n",
    "#             del temp_pivot_table[fea + '_click_number']\n",
    "        valid_df = pd.merge(valid_df, temp_pivot_table, on=fea, how='left')\n",
    "        print(fea + ' : finish!!!')\n",
    "        train_df = train_temp_df\n",
    "        train_df.sort_index(by='index', ascending=True, inplace=True)\n",
    "    return train_df, valid_df\n",
    "    \n",
    "fea_set = ['prefix', 'title', 'tag', 'query_prediction']\n",
    "train_df, valid_df = get_single_dimension_rate_feature(train_df, valid_df, fea_set)\n",
    "print(train_df.head())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 统计双维度交叉转化率\n",
    "def get_jiaocha_dimension_rate_feature(train_df, valid_df, fea_set):\n",
    "    for i in range(len(fea_set)):\n",
    "        for j in range((i+1), len(fea_set)):\n",
    "            fea1 = fea_set[i]\n",
    "            fea2 = fea_set[j]\n",
    "            train_temp_df = pd.DataFrame()\n",
    "            for index, (train_index, test_index) in enumerate(skf.split(train_df, train_df['label'])):\n",
    "                temp_df = train_df[[fea1, fea2, 'label']].iloc[train_index].copy()\n",
    "                temp_pivot_table = pd.pivot_table(temp_df, index=[fea1, fea2], values='label', aggfunc={len, np.mean, np.sum})\n",
    "                temp_pivot_table.reset_index(inplace=True)\n",
    "                temp_pivot_table.rename(columns={'len':fea1 + '_' + fea2 + '_count', 'mean':fea1 + '_' + fea2 + '_rate', 'sum':fea1 + '_' + fea2 + '_click_number'}, inplace=True)\n",
    "                alpha, beta = getBayesSmoothParam(temp_pivot_table[fea1 + '_' + fea2 + '_rate'])\n",
    "                temp_pivot_table[fea1 + '_' + fea2 + '_rate'] = (temp_pivot_table[fea1 + '_' + fea2 + '_click_number'] + alpha) / (temp_pivot_table[fea1 + '_' + fea2 + '_count'] + alpha + beta)\n",
    "#                 del temp_pivot_table[fea1 + '_' + fea2 + '_click_number']\n",
    "                fea_df = train_df.iloc[test_index].copy()\n",
    "                fea_df = pd.merge(fea_df, temp_pivot_table, on=[fea1, fea2], how='left')\n",
    "                train_temp_df = pd.concat([train_temp_df, fea_df])\n",
    "            temp_df = train_df[[fea1, fea2, 'label']].copy()\n",
    "            temp_pivot_table = pd.pivot_table(temp_df, index=[fea1, fea2], values='label', aggfunc={len, np.mean, np.sum})\n",
    "            temp_pivot_table.reset_index(inplace=True)\n",
    "            temp_pivot_table.rename(columns={'len':fea1 + '_' + fea2 + '_count', 'mean':fea1 + '_' + fea2 + '_rate', 'sum':fea1 + '_' + fea2 + '_click_number'}, inplace=True)\n",
    "            alpha, beta = getBayesSmoothParam(temp_pivot_table[fea1 + '_' + fea2 + '_rate'])\n",
    "            temp_pivot_table[fea1 + '_' + fea2 + '_rate'] = (temp_pivot_table[fea1 + '_' + fea2 + '_click_number'] + alpha) / (temp_pivot_table[fea1 + '_' + fea2 + '_count'] + alpha + beta)\n",
    "#             del temp_pivot_table[fea1 + '_' + fea2 + '_click_number']\n",
    "            print(fea1 + '_' + fea2 + ' : finish!!!')\n",
    "            valid_df = pd.merge(valid_df, temp_pivot_table, on=[fea1, fea2], how='left')\n",
    "            train_df = train_temp_df\n",
    "            train_df.sort_index(by='index', ascending=True, inplace=True)\n",
    "    return train_df, valid_df\n",
    "\n",
    "jiaocha_fea_set = ['prefix', 'title', 'tag']\n",
    "train_df, valid_df = get_jiaocha_dimension_rate_feature(train_df, valid_df, jiaocha_fea_set)\n",
    "print(train_df.head())\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 统计一些是否交叉的特征\n",
    "def get_is_title_in_query_feature(df):\n",
    "    x = df['title']\n",
    "    y = df['query_prediction_keys']\n",
    "    is_title_in_query = np.nan\n",
    "    if len(y) > 0:\n",
    "        if x in y:\n",
    "            is_title_in_query = 1\n",
    "        else:\n",
    "            is_title_in_query = 0\n",
    "    return is_title_in_query\n",
    "\n",
    "def get_is_prefix_in_title_feature(df):\n",
    "    x = df['prefix']\n",
    "    y = df['title']\n",
    "    is_prefix_in_title = np.nan\n",
    "    if x in y:\n",
    "        is_prefix_in_title = 1\n",
    "    else:\n",
    "        is_prefix_in_title = 0\n",
    "    return is_prefix_in_title\n",
    "\n",
    "train_df['is_title_in_query'] = train_df[['title', 'query_prediction_keys']].apply(get_is_title_in_query_feature, axis = 1)\n",
    "valid_df['is_title_in_query'] = valid_df[['title', 'query_prediction_keys']].apply(get_is_title_in_query_feature, axis = 1)\n",
    "\n",
    "train_df['is_prefix_in_title'] = train_df[['prefix', 'title']].apply(get_is_prefix_in_title_feature, axis = 1)\n",
    "valid_df['is_prefix_in_title'] = valid_df[['prefix', 'title']].apply(get_is_prefix_in_title_feature, axis = 1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 统计一些交叉种类特征\n",
    "def get_jiaocha_type_feature(train_df, valid_df, jiaocha_type_list):\n",
    "    for jiaocha_type in jiaocha_type_list:\n",
    "        fea1 = jiaocha_type[0]\n",
    "        fea2 = jiaocha_type[1]\n",
    "        temp_df = pd.concat([train_df, valid_df])\n",
    "        temp_pivot_table = pd.pivot_table(temp_df[[fea1, fea2, 'label']], index=[fea1, fea2], values='label', aggfunc=len)\n",
    "        temp_pivot_table.reset_index(inplace=True)\n",
    "        final_pivot_table = pd.pivot_table(temp_pivot_table, index=fea1, values=fea2, aggfunc=len)\n",
    "        final_pivot_table.reset_index(inplace=True)\n",
    "        final_pivot_table.rename(columns={fea2 : fea1 + '_' + fea2 + '_types'}, inplace=True)\n",
    "        train_df = pd.merge(train_df, final_pivot_table[[fea1, fea1 + '_' + fea2 + '_types']], on=fea1, how='left')\n",
    "        valid_df = pd.merge(valid_df, final_pivot_table[[fea1, fea1 + '_' + fea2 + '_types']], on=fea1, how='left')\n",
    "    return train_df, valid_df\n",
    "\n",
    "jiaocha_type_list = [['title', 'tag'], ['prefix', 'tag'], ['tag', 'title'], ['tag', 'prefix'], \n",
    "                     ['title', 'prefix'], ['prefix', 'title'], ['tag', 'query_prediction'], ['title', 'query_prediction']]\n",
    "train_df, valid_df = get_jiaocha_type_feature(train_df, valid_df, jiaocha_type_list)\n",
    "print(train_df.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_key_len_list(x):\n",
    "    return_list = []\n",
    "    for temp in x:\n",
    "        return_list.append(len(temp))\n",
    "    return return_list\n",
    "\n",
    "# 统计一些跟字符串长度相关的特征\n",
    "def get_string_len_feature(df):\n",
    "    df['prefix_len'] = df['prefix'].map(lambda x : len(x))\n",
    "    df['title_len'] = df['title'].map(lambda x : len(x))\n",
    "    df['query_prediction_key_len_list'] = df['query_prediction_keys'].map(lambda x : get_key_len_list(x))\n",
    "    df['query_prediction_key_len_max'] = df['query_prediction_key_len_list'].map(lambda x : np.nan if len(x) == 0 else np.max(x))\n",
    "    df['query_prediction_key_len_min'] = df['query_prediction_key_len_list'].map(lambda x : np.nan if len(x) == 0 else np.min(x))\n",
    "    df['query_prediction_key_len_mean'] = df['query_prediction_key_len_list'].map(lambda x : np.nan if len(x) == 0 else np.mean(x))\n",
    "    df['query_prediction_key_len_std'] = df['query_prediction_key_len_list'].map(lambda x : np.nan if len(x) == 0 else np.std(x))\n",
    "    df['len_title-prefix'] = df['title_len'] - df['prefix_len']\n",
    "    df['len_prefix/title'] = df['prefix_len'] / df['title_len']\n",
    "    df['len_mean-title'] = df['query_prediction_key_len_mean'] - df['title_len']\n",
    "    df['len_mean/title'] = df['query_prediction_key_len_mean'] / df['title_len']\n",
    "    del df['query_prediction_key_len_list']\n",
    "    return df\n",
    "\n",
    "train_df = get_string_len_feature(train_df)\n",
    "valid_df = get_string_len_feature(valid_df)\n",
    "print(train_df.head())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 统计title跟prefix的编辑距离\n",
    "def get_title_prefix_levenshtein_distance(df):\n",
    "    title = df['title']\n",
    "    prefix = df['prefix']\n",
    "    return Levenshtein.distance(title, prefix)\n",
    "\n",
    "def get_title_prefix_levenshtein_distance_rate(df):\n",
    "    title_prefix_leven = df['title_prefix_leven']\n",
    "    title = df['title']\n",
    "    return (title_prefix_leven / (len(title) + 3))\n",
    "\n",
    "train_df['title_prefix_leven'] = train_df[['title', 'prefix']].apply(get_title_prefix_levenshtein_distance, axis=1)\n",
    "valid_df['title_prefix_leven'] = valid_df[['title', 'prefix']].apply(get_title_prefix_levenshtein_distance, axis=1)\n",
    "\n",
    "train_df['title_prefix_leven_rate'] = train_df[['title', 'title_prefix_leven']].apply(get_title_prefix_levenshtein_distance_rate, axis=1)\n",
    "valid_df['title_prefix_leven_rate'] = valid_df[['title', 'title_prefix_leven']].apply(get_title_prefix_levenshtein_distance_rate, axis=1)\n",
    "\n",
    "print(train_df.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 统计title跟query_prediction编辑距离相关的特征\n",
    "def get_title_query_levenshtein_distance_list(df):\n",
    "    query_keys_list = df['query_prediction_keys']\n",
    "    query_values_list = df['query_prediction_values']\n",
    "    title = df['title']\n",
    "    return_list = list()\n",
    "    for i in range(len(query_keys_list)):\n",
    "        distance = Levenshtein.distance(title, query_keys_list[i])\n",
    "        return_list.append(distance * query_values_list[i])\n",
    "    return return_list\n",
    "\n",
    "def get_title_query_levenshtein_distance_feature(df):\n",
    "    df['title_query_leven_list'] = df[['query_prediction_keys', 'query_prediction_values', 'title']].apply(get_title_query_levenshtein_distance_list, axis=1)\n",
    "    df['title_query_leven_sum'] = df['title_query_leven_list'].map(lambda x : np.nan if len(x) == 0 else np.sum(x))\n",
    "    df['title_query_leven_max'] = df['title_query_leven_list'].map(lambda x : np.nan if len(x) == 0 else np.max(x))\n",
    "    df['title_query_leven_min'] = df['title_query_leven_list'].map(lambda x : np.nan if len(x) == 0 else np.min(x))\n",
    "    df['title_query_leven_mean'] = df['title_query_leven_list'].map(lambda x : np.nan if len(x) == 0 else np.mean(x))\n",
    "    df['title_query_leven_std'] = df['title_query_leven_list'].map(lambda x : np.nan if len(x) == 0 else np.std(x))\n",
    "    return df\n",
    "\n",
    "train_df = get_title_query_levenshtein_distance_feature(train_df)\n",
    "valid_df = get_title_query_levenshtein_distance_feature(valid_df)\n",
    "print(train_df.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#分词方法，调用结巴接口\n",
    "def jieba_seg_to_list(sentence, pos=False):\n",
    "    if not pos:\n",
    "        #不进行词性标注的分词方法\n",
    "        seg_list = jieba.cut(sentence)\n",
    "    else:\n",
    "        #进行词性标注的分词方法\n",
    "        seg_list = psg.cut(sentence)\n",
    "    return seg_list\n",
    "\n",
    "#去除干扰词\n",
    "def jieba_word_filter(seg_list, pos=False):\n",
    "    \n",
    "    filter_list = []\n",
    "    #根据pos参数选择是否词性过滤\n",
    "    #不进行词性过滤，则将词性都标记为n，表示全部保留\n",
    "    for seg in seg_list:\n",
    "        if not pos:\n",
    "            word = seg\n",
    "            flag = 'n'\n",
    "        else:\n",
    "            word = seg.word\n",
    "            flag = seg.flag\n",
    "        if not flag.startswith('n'):\n",
    "            continue\n",
    "        filter_list.append(word)\n",
    "    return filter_list\n",
    "\n",
    "def jieba_word_deal(sentence, pos=False):\n",
    "    #调用上面方式对数据集进行处理，处理后的每条数据仅保留非干扰词\n",
    "    seg_list = jieba_seg_to_list(sentence, pos)\n",
    "    filter_list = jieba_word_filter(seg_list, pos)\n",
    "    return filter_list\n",
    "\n",
    "def get_prefix_prediction_key_sentences(x):\n",
    "    prefix_prediction_key_sentences = \"\"\n",
    "    for temp in x:\n",
    "        if len(prefix_prediction_key_sentences) > 0:\n",
    "            prefix_prediction_key_sentences = prefix_prediction_key_sentences + temp\n",
    "        else:\n",
    "            prefix_prediction_key_sentences = temp\n",
    "    return prefix_prediction_key_sentences\n",
    "\n",
    "def get_max_query_key_sentences(x):\n",
    "    if len(x) == 0:\n",
    "        return \"\"\n",
    "    else:\n",
    "        return max(x, key=x.get)\n",
    "\n",
    "def get_jieba_word(df):\n",
    "    df['query_prediction_key_sentences'] = df['query_prediction_keys'].map(lambda x : get_prefix_prediction_key_sentences(x))\n",
    "#     df['query_prediction_key_sentences'] = df['query_prediction_dict'].map(lambda x : get_max_query_key_sentences(x))\n",
    "    df['query_prediction_key_jieba_words'] = df['query_prediction_key_sentences'].map(lambda x : jieba_word_deal(x, False))\n",
    "    df['query_prediction_words'] = df['query_prediction_keys'].map(lambda x : [jieba_word_deal(j, False) for j in x] if len(x) > 0 else np.nan)\n",
    "    df['title_jieba_words'] = df['title'].map(lambda x : jieba_word_deal(x, False))\n",
    "    df['prefix_jieba_words'] = df['prefix'].map(lambda x : jieba_word_deal(x, False))\n",
    "#     del df['query_prediction_key_sentences']\n",
    "    return df\n",
    "\n",
    "train_df = get_jieba_word(train_df)\n",
    "valid_df = get_jieba_word(valid_df)\n",
    "print(train_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_match_share(df):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in df[0]:\n",
    "        q1words[word] = 1\n",
    "    for word in df[1]:\n",
    "        q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
    "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
    "    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n",
    "    return R\n",
    "\n",
    "def jaccard(df):\n",
    "    wic = set(df[0]).intersection(set(df[1]))\n",
    "    uw = set(df[0]).union(df[1])\n",
    "    if len(uw) == 0:\n",
    "        uw = [1]\n",
    "    return (len(wic) / len(uw))\n",
    "\n",
    "def common_words(df):\n",
    "    return len(set(df[0]).intersection(set(df[1])))\n",
    "\n",
    "def total_unique_words(df):\n",
    "    return len(set(df[0]).union(df[1]))\n",
    "\n",
    "def wc_diff(df):\n",
    "    return abs(len(df[0]) - len(df[1]))\n",
    "\n",
    "def wc_ratio(df):\n",
    "    l1 = len(df[0])*1.0 \n",
    "    l2 = len(df[1])\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "\n",
    "def wc_diff_unique(df):\n",
    "    return abs(len(set(df[0])) - len(set(df[1])))\n",
    "    \n",
    "def wc_ratio_unique(df):\n",
    "    l1 = len(set(df[0])) * 1.0\n",
    "    l2 = len(set(df[1]))\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "    \n",
    "def tfidf_word_match_share(df, weights=None):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in df[0]:\n",
    "        q1words[word] = 1\n",
    "    for word in df[1]:\n",
    "        q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
    "    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "    R = np.sum(shared_weights) / np.sum(total_weights)\n",
    "    return R\n",
    "\n",
    "def deal_word_for_all(train_df, valid_df, fea1, fea2, func, colName):\n",
    "    train_df[colName] = train_df[[fea1, fea2]].apply(func, axis=1)\n",
    "    valid_df[colName] = valid_df[[fea1, fea2]].apply(func, axis=1)\n",
    "    print(colName + ' finish!!!')\n",
    "    return train_df, valid_df\n",
    "                   \n",
    "def get_weight(count, eps=10000, min_count=2):\n",
    "    if count < min_count:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 / (count + eps)\n",
    "\n",
    "def get_word_statistic_feature(train_df, valid_df, col_list):\n",
    "    df = pd.concat([train_df[['query_prediction_key_jieba_words', 'title_jieba_words', 'prefix_jieba_words']], valid_df[['query_prediction_key_jieba_words', 'title_jieba_words', 'prefix_jieba_words']]])\n",
    "    train_qs = pd.Series(df['query_prediction_key_jieba_words'].tolist() + df['title_jieba_words'].tolist() + df['prefix_jieba_words'].tolist())\n",
    "    words = [x for y in train_qs for x in y]\n",
    "    counts = Counter(words)\n",
    "    weights = {word: get_weight(count) for word, count in counts.items()}\n",
    "    for col in col_list:\n",
    "        fea1 = col[0]\n",
    "        fea2 = col[1]\n",
    "        train_df, valid_df = deal_word_for_all(train_df, valid_df, fea1, fea2, word_match_share, fea1[0] + '_' + fea2[0] + '_word_match')\n",
    "        train_df, valid_df = deal_word_for_all(train_df, valid_df, fea1, fea2, jaccard, fea1[0] + '_' + fea2[0] + '_jaccard')\n",
    "        train_df, valid_df = deal_word_for_all(train_df, valid_df, fea1, fea2, common_words, fea1[0] + '_' + fea2[0] + '_common_words')\n",
    "        train_df, valid_df = deal_word_for_all(train_df, valid_df, fea1, fea2, total_unique_words, fea1[0] + '_' + fea2[0] + '_total_unique_words')\n",
    "        train_df, valid_df = deal_word_for_all(train_df, valid_df, fea1, fea2, wc_diff, fea1[0] + '_' + fea2[0] + '_wc_diff')\n",
    "        train_df, valid_df = deal_word_for_all(train_df, valid_df, fea1, fea2, wc_ratio, fea1[0] + '_' + fea2[0] + '_wc_ratio')\n",
    "        train_df, valid_df = deal_word_for_all(train_df, valid_df, fea1, fea2, wc_diff_unique, fea1[0] + '_' + fea2[0] + '_wc_diff_unique')\n",
    "        train_df, valid_df = deal_word_for_all(train_df, valid_df, fea1, fea2, wc_ratio_unique, fea1[0] + '_' + fea2[0] + '_wc_ratio_unique')\n",
    "        f = functools.partial(tfidf_word_match_share, weights=weights)\n",
    "        train_df, valid_df = deal_word_for_all(train_df, valid_df, fea1, fea2, f, fea1[0] + '_' + fea2[0] + '_tfidf_word_match_share')\n",
    "    return train_df, valid_df\n",
    "\n",
    "col_list = [['query_prediction_key_jieba_words', 'title_jieba_words'], ['prefix_jieba_words', 'title_jieba_words'], ['prefix_jieba_words', 'query_prediction_key_jieba_words']]\n",
    "train_df, valid_df = get_word_statistic_feature(train_df, valid_df, col_list)\n",
    "print(train_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set values for various parameters\n",
    "num_features = 500  # Word vector dimensionality                      \n",
    "min_word_count = 1  # Minimum word count                        \n",
    "num_workers = 20       # Number of threads to run in parallel\n",
    "context = 5          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "word2vec_df = pd.concat([train_df[['query_prediction_words', 'title_jieba_words', 'prefix_jieba_words', 'query_prediction_number']], valid_df[['query_prediction_words', 'title_jieba_words', 'prefix_jieba_words', 'query_prediction_number']]])\n",
    "word2vec_df.reset_index(inplace=True)\n",
    "word2vec_list = word2vec_df['title_jieba_words'].tolist() + word2vec_df['prefix_jieba_words'].tolist() + [y for x in word2vec_df['query_prediction_words'][word2vec_df.query_prediction_number > 0] for y in x]\n",
    "model = word2vec.Word2Vec(word2vec_list, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "word_wv = model.wv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_w2v_array(word_list, word_wv, num_features):\n",
    "    word_vectors = np.zeros((len(word_list), num_features))\n",
    "    for i in range(len(word_list)):\n",
    "        word_vectors[i][:] = word_wv[str(word_list[i])]\n",
    "    mean_array = np.mean(word_vectors, axis=0)\n",
    "    return mean_array\n",
    "\n",
    "train_df['title_jieba_array'] = train_df['title_jieba_words'].map(lambda x : get_w2v_array(x, word_wv, num_features))\n",
    "valid_df['title_jieba_array'] = valid_df['title_jieba_words'].map(lambda x : get_w2v_array(x, word_wv, num_features))\n",
    "\n",
    "train_df['prefix_jieba_array'] = train_df['prefix_jieba_words'].map(lambda x : get_w2v_array(x, word_wv, num_features))\n",
    "valid_df['prefix_jieba_array'] = valid_df['prefix_jieba_words'].map(lambda x : get_w2v_array(x, word_wv, num_features))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_title_prefix_similarity(df, f_similarity):\n",
    "    title_array = df['title_jieba_array']\n",
    "    prefix_array = df['prefix_jieba_array']\n",
    "    similarity = 0\n",
    "    if f_similarity == 'dot':\n",
    "        similarity = np.dot(title_array, prefix_array)\n",
    "    elif f_similarity == 'norm':\n",
    "        similarity = np.linalg.norm(title_array - prefix_array)\n",
    "    else:\n",
    "        similarity = np.dot(title_array,prefix_array) / (np.linalg.norm(title_array) * np.linalg.norm(prefix_array))\n",
    "    return similarity\n",
    "\n",
    "# def get_title_query_similarity(df, f_similarity, word_wv, num_features):\n",
    "#     title_array = df['title_jieba_array']\n",
    "#     query_prediction_words = df['query_prediction_words']\n",
    "#     query_prediction_keys = df['query_prediction_keys']\n",
    "#     query_prediction_dict = df['query_prediction_dict']\n",
    "#     if len(query_prediction_keys) <= 0:\n",
    "#         return np.nan\n",
    "#     similarity = 0\n",
    "#     if f_similarity == 'dot':\n",
    "#         i = 0\n",
    "#         for key in query_prediction_keys:\n",
    "#             key_array = get_w2v_array(query_prediction_words[i], word_wv, num_features)\n",
    "#             similarity = similarity + np.dot(title_array, key_array) * float(query_prediction_dict[key])\n",
    "#             i = i + 1\n",
    "#     elif f_similarity == 'norm':\n",
    "#         i = 0\n",
    "#         for key in query_prediction_keys:\n",
    "#             key_array = get_w2v_array(query_prediction_words[i], word_wv, num_features)\n",
    "#             similarity = similarity + np.linalg.norm(title_array - key_array) * float(query_prediction_dict[key])\n",
    "#             i = i + 1\n",
    "#     else:\n",
    "#         i = 0\n",
    "#         for key in query_prediction_keys:\n",
    "#             key_array = get_w2v_array(query_prediction_words[i], word_wv, num_features)\n",
    "#             similarity = similarity + (np.dot(title_array, key_array) / (np.linalg.norm(title_array) * np.linalg.norm(key_array))) * float(query_prediction_dict[key])\n",
    "#             i = i + 1\n",
    "#     return similarity\n",
    "\n",
    "def get_title_query_similarity_list(df, f_similarity, word_wv, num_features):\n",
    "    title_array = df['title_jieba_array']\n",
    "    query_prediction_words = df['query_prediction_words']\n",
    "    query_prediction_keys = df['query_prediction_keys']\n",
    "    query_prediction_dict = df['query_prediction_dict']\n",
    "    similarity_list = list()\n",
    "    if len(query_prediction_keys) <= 0:\n",
    "        return similarity_list\n",
    "    if f_similarity == 'dot':\n",
    "        i = 0\n",
    "        for key in query_prediction_keys:\n",
    "            key_array = get_w2v_array(query_prediction_words[i], word_wv, num_features)\n",
    "            similarity = np.dot(title_array, key_array) * float(query_prediction_dict[key])\n",
    "            similarity_list.append(similarity)\n",
    "            i = i + 1\n",
    "    elif f_similarity == 'norm':\n",
    "        i = 0\n",
    "        for key in query_prediction_keys:\n",
    "            key_array = get_w2v_array(query_prediction_words[i], word_wv, num_features)\n",
    "            similarity = np.linalg.norm(title_array - key_array) * float(query_prediction_dict[key])\n",
    "            similarity_list.append(similarity)\n",
    "            i = i + 1\n",
    "    else:\n",
    "        i = 0\n",
    "        for key in query_prediction_keys:\n",
    "            key_array = get_w2v_array(query_prediction_words[i], word_wv, num_features)\n",
    "            similarity = (np.dot(title_array, key_array) / (np.linalg.norm(title_array) * np.linalg.norm(key_array))) * float(query_prediction_dict[key])\n",
    "            similarity_list.append(similarity)\n",
    "            i = i + 1\n",
    "    return similarity_list\n",
    "\n",
    "def get_similarity_feature(train_df, valid_df):\n",
    "    f_list = ['dot', 'norm', 'cosine']\n",
    "    for fun in f_list:\n",
    "        f_prefix_similarity = functools.partial(get_title_prefix_similarity, f_similarity=fun)\n",
    "        train_df['title_prefix_' + fun + '_similarity'] = train_df[['title_jieba_array', 'prefix_jieba_array']].apply(f_prefix_similarity, axis=1)\n",
    "        valid_df['title_prefix_' + fun + '_similarity'] = valid_df[['title_jieba_array', 'prefix_jieba_array']].apply(f_prefix_similarity, axis=1)\n",
    "#         f_query_similarity = functools.partial(get_title_query_similarity, f_similarity=fun, word_wv=word_wv, num_features=num_features)\n",
    "#         train_df['title_query_' + fun + '_similarity'] = train_df[['title_jieba_array', 'query_prediction_words', 'query_prediction_keys', 'query_prediction_dict']].apply(f_query_similarity, axis=1)\n",
    "#         valid_df['title_query_' + fun + '_similarity'] = valid_df[['title_jieba_array', 'query_prediction_words', 'query_prediction_keys', 'query_prediction_dict']].apply(f_query_similarity, axis=1)\n",
    "        f_query_similarity_list = functools.partial(get_title_query_similarity_list, f_similarity=fun, word_wv=word_wv, num_features=num_features)\n",
    "        train_df['title_query_' + fun + '_similarity_list'] = train_df[['title_jieba_array', 'query_prediction_words', 'query_prediction_keys', 'query_prediction_dict']].apply(f_query_similarity_list, axis=1)\n",
    "        valid_df['title_query_' + fun + '_similarity_list'] = valid_df[['title_jieba_array', 'query_prediction_words', 'query_prediction_keys', 'query_prediction_dict']].apply(f_query_similarity_list, axis=1)\n",
    "        train_df['title_query_' + fun + '_similarity'] = train_df['title_query_' + fun + '_similarity_list'].map(lambda x : np.nan if len(x)==0 else np.sum(x))\n",
    "        train_df['title_query_' + fun + '_similarity_max'] = train_df['title_query_' + fun + '_similarity_list'].map(lambda x : np.nan if len(x)==0 else np.max(x))\n",
    "        train_df['title_query_' + fun + '_similarity_min'] = train_df['title_query_' + fun + '_similarity_list'].map(lambda x : np.nan if len(x)==0 else np.min(x))\n",
    "        train_df['title_query_' + fun + '_similarity_mean'] = train_df['title_query_' + fun + '_similarity_list'].map(lambda x : np.nan if len(x)==0 else np.mean(x))\n",
    "        train_df['title_query_' + fun + '_similarity_std'] = train_df['title_query_' + fun + '_similarity_list'].map(lambda x : np.nan if len(x)==0 else np.std(x))\n",
    "        valid_df['title_query_' + fun + '_similarity'] = valid_df['title_query_' + fun + '_similarity_list'].map(lambda x : np.nan if len(x)==0 else np.sum(x))\n",
    "        valid_df['title_query_' + fun + '_similarity_max'] = valid_df['title_query_' + fun + '_similarity_list'].map(lambda x : np.nan if len(x)==0 else np.max(x))\n",
    "        valid_df['title_query_' + fun + '_similarity_min'] = valid_df['title_query_' + fun + '_similarity_list'].map(lambda x : np.nan if len(x)==0 else np.min(x))\n",
    "        valid_df['title_query_' + fun + '_similarity_mean'] = valid_df['title_query_' + fun + '_similarity_list'].map(lambda x : np.nan if len(x)==0 else np.mean(x))\n",
    "        valid_df['title_query_' + fun + '_similarity_std'] = valid_df['title_query_' + fun + '_similarity_list'].map(lambda x : np.nan if len(x)==0 else np.std(x))\n",
    "        print(fun + ' : finish!!!')\n",
    "    return train_df, valid_df\n",
    "\n",
    "train_df, valid_df = get_similarity_feature(train_df, valid_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fea = [\n",
    "    'query_prediction_number', 'query_prediction_max', 'query_prediction_min', 'query_prediction_mean', 'query_prediction_std',\n",
    "       'prefix_count', 'prefix_rate',\n",
    " 'title_count', 'title_rate', 'tag_count', 'tag_rate',\n",
    " 'query_prediction_count', 'query_prediction_rate', 'prefix_title_count',\n",
    " 'prefix_title_rate',  'prefix_tag_count', 'prefix_tag_rate',\n",
    " 'title_tag_count', 'title_tag_rate',\n",
    "    'prefix_click_number', 'title_click_number', 'query_prediction_click_number', 'prefix_tag_click_number', \n",
    "    'prefix_title_click_number', 'title_tag_click_number',\n",
    "    'is_title_in_query', 'is_prefix_in_title', \n",
    "    'title_tag_types', 'prefix_tag_types', 'tag_title_types', 'tag_prefix_types',\n",
    " 'title_prefix_types', 'prefix_title_types', 'tag_query_prediction_types', 'title_query_prediction_types',\n",
    "      'prefix_len', 'title_len',\n",
    " 'query_prediction_key_len_max', 'query_prediction_key_len_min',\n",
    " 'query_prediction_key_len_mean', 'query_prediction_key_len_std',\n",
    " 'len_title-prefix', 'len_prefix/title', 'len_mean-title', 'len_mean/title',\n",
    "    'q_t_word_match', 'q_t_jaccard', 'q_t_common_words',\n",
    " 'q_t_total_unique_words', 'q_t_wc_diff', 'q_t_wc_ratio',\n",
    " 'q_t_wc_diff_unique', 'q_t_wc_ratio_unique', 'q_t_tfidf_word_match_share',\n",
    " 'p_t_word_match', 'p_t_jaccard', 'p_t_common_words',\n",
    " 'p_t_total_unique_words', 'p_t_wc_diff', 'p_t_wc_ratio',\n",
    " 'p_t_wc_diff_unique', 'p_t_wc_ratio_unique', 'p_t_tfidf_word_match_share',\n",
    " 'p_q_word_match', 'p_q_jaccard', 'p_q_common_words',\n",
    " 'p_q_total_unique_words', 'p_q_wc_diff', 'p_q_wc_ratio',\n",
    " 'p_q_wc_diff_unique', 'p_q_wc_ratio_unique', 'p_q_tfidf_word_match_share',\n",
    "    'title_prefix_dot_similarity',\n",
    " 'title_query_dot_similarity', 'title_prefix_norm_similarity',\n",
    " 'title_query_norm_similarity', 'title_prefix_cosine_similarity',\n",
    " 'title_query_cosine_similarity',\n",
    "    'title_query_dot_similarity_max', 'title_query_dot_similarity_min',\n",
    " 'title_query_dot_similarity_mean', 'title_query_dot_similarity_std',\n",
    "    'title_query_norm_similarity_min', 'title_query_norm_similarity_mean',\n",
    " 'title_query_norm_similarity_std', 'title_prefix_cosine_similarity',\n",
    "    'title_query_cosine_similarity_max', 'title_query_cosine_similarity_min',\n",
    " 'title_query_cosine_similarity_mean', 'title_query_cosine_similarity_std',\n",
    "    'title_prefix_leven', 'title_prefix_leven_rate',\n",
    " 'title_query_leven_sum', 'title_query_leven_max', 'title_query_leven_min',\n",
    " 'title_query_leven_mean', 'title_query_leven_std',\n",
    "    'prefix', 'query_prediction', 'title', 'tag', 'label',\n",
    "      ]\n",
    "\n",
    "train_fea = fea + ['index']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 导出特征工程文件\n",
    "def exportDf(df, fileName):\n",
    "    df.to_csv('../temp/%s.csv' % fileName, header=True, index=True)\n",
    "\n",
    "exportDf(train_df[train_fea], 'train_offline_df')\n",
    "exportDf(valid_df[fea], 'valid_offline_df')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
